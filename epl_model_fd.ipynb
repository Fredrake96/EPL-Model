{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98ec254",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "909688be",
   "metadata": {},
   "source": [
    "# EPL XGBoost + Bayesian DixonColes + Transformer\n",
    "> ****:  Elo DC OOF  H2H//  \n",
    "> 2025-11-25\n",
    "\n",
    "****\n",
    "1. Elo baseline\n",
    "2. Dixon-Coles OOF fold posterior\n",
    "3. H2H features\n",
    "4. Time decay: lambda=0.00325/day (tau~308d)\n",
    "5. Draw handling: balanced weights + draw boost x3 (no SMOTE)\n",
    "6. Calibration: Isotonic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511d355",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080960a",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0699ab4a-e807-44eb-a1bf-37abc5e4d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTensor cxx: /usr/bin/g++\n",
      "PyTensor cxx: /usr/bin/g++\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Using device: cuda\n",
      "imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pytensor\n",
    "print(\"PyTensor cxx:\", pytensor.config.cxx)\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "from math import exp, factorial\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import dill\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    log_loss, \n",
    "    classification_report, \n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Imbalanced Learning \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# XGBoost \n",
    "import xgboost as xgb\n",
    "\n",
    "import optuna\n",
    "# PyTorch & Deep Learning \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Bayesian Modeling (PyMC) \n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as at\n",
    "\n",
    "# Check PyTensor compiler\n",
    "print(\"PyTensor cxx:\", pytensor.config.cxx)\n",
    "\n",
    "# Device Setup \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(\"imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fbfe0",
   "metadata": {},
   "source": [
    "### 0. Leakage protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4da485-fbe8-498f-b6fb-c610f930ff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 of leaked features has been defined\n"
     ]
    }
   ],
   "source": [
    "DIRECT_LEAK_COLS = [\n",
    "    'FTHG', 'FTAG', 'HTHG', 'HTAG', 'FTR', 'HTR',\n",
    "    'HS', 'AS'\n",
    "]\n",
    "\n",
    "DERIVED_LEAK_COLS = [\n",
    "    'shots_for', 'shots_against'\n",
    "]\n",
    "\n",
    "# Some of the features should not be removed as \"leak features\"\n",
    "# but they're imported within training after. So we still remove them here.\n",
    "\n",
    "FEATURES_ADD_AFTER = [\n",
    "    'HST', 'AST', 'HC', 'AC', \n",
    "    'HF', 'AF', 'HY', 'AY', 'HR', 'AR',\n",
    "    'sot_for', 'sot_against',\n",
    "    'corners_for', 'corners_against',\n",
    "    'shot_diff', 'sot_diff', 'corner_diff',\n",
    "    'foul_diff', 'ycard_diff', 'rcard_diff',\n",
    "    'shot_accuracy', 'opp_shot_accuracy',\n",
    "    'shot_acc_home', 'shot_acc_away',\n",
    "]\n",
    "\n",
    "ALL_LEAK_COLS = list(set(DIRECT_LEAK_COLS + DERIVED_LEAK_COLS + FEATURES_ADD_AFTER))\n",
    "len_leak_revise = list(set(DIRECT_LEAK_COLS + DERIVED_LEAK_COLS))\n",
    "\n",
    "def remove_leak_features(feature_list, verbose=True):\n",
    "    \"\"\"Remove leakage features\"\"\"\n",
    "    leaked = [f for f in feature_list if f in ALL_LEAK_COLS]\n",
    "    if verbose and leaked:\n",
    "        print(f\" remove {len(leaked)} leakage features: {leaked}\")\n",
    "    clean = [f for f in feature_list if f not in ALL_LEAK_COLS]\n",
    "    if verbose:\n",
    "        print(f\"✅ Features after cleaning: {len(feature_list)} → {len(clean)}\")\n",
    "    return clean\n",
    "\n",
    "print(f\"{len(len_leak_revise)} of leaked features has been defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d5c21",
   "metadata": {},
   "source": [
    "## 2. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922fcd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_df shape: (9500, 32)\n",
      "        Date  HomeTeam       AwayTeam  FTHG  FTAG FTR  y     Season\n",
      "0 2000-08-19  Charlton       Man City   4.0   0.0   H  0  2000/2001\n",
      "1 2000-08-19   Chelsea       West Ham   4.0   2.0   H  0  2000/2001\n",
      "2 2000-08-19  Coventry  Middlesbrough   1.0   3.0   A  2  2000/2001\n",
      "3 2000-08-19     Derby    Southampton   2.0   2.0   D  1  2000/2001\n",
      "4 2000-08-19     Leeds        Everton   2.0   0.0   H  0  2000/2001\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"epl-training.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"Date\", \"HomeTeam\", \"AwayTeam\"]).copy()\n",
    "\n",
    "match_key = [\"Date\", \"HomeTeam\", \"AwayTeam\"]\n",
    "df = df.sort_values(match_key).drop_duplicates(subset=match_key, keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "def clean_value(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def safe_ratio(a, b):\n",
    "    return a / b if (b not in [0, None] and not pd.isna(b)) else 0.0\n",
    "\n",
    "optional_map = {\n",
    "    \"HS\": \"shots_for\",\n",
    "    \"AS\": \"shots_against\",\n",
    "    \"HST\": \"sot_for\",\n",
    "    \"AST\": \"sot_against\",\n",
    "    \"HC\": \"corners_for\",\n",
    "    \"AC\": \"corners_against\",\n",
    "}\n",
    "\n",
    "for raw, newc in optional_map.items():\n",
    "    if raw in df.columns:\n",
    "        df[newc] = df[raw].apply(clean_value)\n",
    "\n",
    "if \"sot_for\" in df.columns and \"shots_for\" in df.columns:\n",
    "    df[\"shot_accuracy\"] = [safe_ratio(hst, hs) for hst, hs in zip(df[\"sot_for\"], df[\"shots_for\"])]\n",
    "if \"sot_against\" in df.columns and \"shots_against\" in df.columns:\n",
    "    df[\"opp_shot_accuracy\"] = [safe_ratio(ast, ss) for ast, ss in zip(df[\"sot_against\"], df[\"shots_against\"])]\n",
    "\n",
    "label_map = {\"H\": 0, \"D\": 1, \"A\": 2}\n",
    "df[\"y\"] = df[\"FTR\"].map(label_map).astype(int)\n",
    "\n",
    "def season_label(d, season_start_month=8):\n",
    "    y = d.year\n",
    "    m = d.month\n",
    "    start_year = y if m >= season_start_month else y - 1\n",
    "    return f\"{start_year}/{start_year+1}\"\n",
    "\n",
    "df[\"Season\"] = df[\"Date\"].apply(season_label)\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "cleaned_df = df.copy()\n",
    "\n",
    "# remove leak features\n",
    "print(\"cleaned_df shape:\", cleaned_df.shape)\n",
    "print(cleaned_df[[\"Date\",\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"FTR\",\"y\",\"Season\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04357335-6c7a-4c83-937a-f1dbea1b1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self._init_state()\n",
    "    \n",
    "    def _init_state(self):\n",
    "        self.state = {\n",
    "            'team_results': defaultdict(lambda: deque(maxlen=5)),\n",
    "            'home_results_5': defaultdict(lambda: deque(maxlen=5)),\n",
    "            'away_results_5': defaultdict(lambda: deque(maxlen=5)),\n",
    "            'team_goals_for': defaultdict(lambda: deque(maxlen=5)),\n",
    "            'team_goals_against': defaultdict(lambda: deque(maxlen=5)),\n",
    "            'gd_history': defaultdict(lambda: deque(maxlen=5)),\n",
    "            'season_points': defaultdict(lambda: defaultdict(int)),\n",
    "            'season_gd': defaultdict(lambda: defaultdict(int)),\n",
    "            'h2h_history': defaultdict(list),\n",
    "            'last_match_date': {},\n",
    "            'referee_stats': defaultdict(lambda: {'H': 0, 'D': 0, 'A': 0, 'yellow': deque(maxlen=20)}),\n",
    "            'elo': defaultdict(lambda: 1500.0),\n",
    "            'elo_att': defaultdict(lambda: 1500.0),\n",
    "            'elo_def': defaultdict(lambda: 1500.0),\n",
    "            'Current_season': None,\n",
    "            'team_draw_overall': defaultdict(lambda: deque(maxlen=10)),\n",
    "        }\n",
    "    \n",
    "    def get_state(self):\n",
    "        return deepcopy(self.state)\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        self.state = deepcopy(state)\n",
    "    \n",
    "    def compute_features(self, df, is_train=True):\n",
    "        df = df.sort_values(\"Date\").copy()\n",
    "        features = defaultdict(list)\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "            season = row[\"Season\"]\n",
    "            date = row[\"Date\"]\n",
    "            \n",
    "            # record\n",
    "            h_results = list(self.state['team_results'][h])\n",
    "            a_results = list(self.state['team_results'][a])\n",
    "            form_h = sum(h_results) / (len(h_results) * 3) if h_results else 0.5\n",
    "            form_a = sum(a_results) / (len(a_results) * 3) if a_results else 0.5\n",
    "            features['form_home'].append(form_h)\n",
    "            features['form_away'].append(form_a)\n",
    "            features['form_diff'].append(form_h - form_a)\n",
    "            \n",
    "            # home/away\n",
    "            h5_home = list(self.state['home_results_5'][h])\n",
    "            a5_away = list(self.state['away_results_5'][a])\n",
    "            features['L5HWR'].append(sum(1 for r in h5_home if r == 'H')/len(h5_home) if h5_home else 0.45)\n",
    "            features['L5HDR'].append(sum(1 for r in h5_home if r == 'D')/len(h5_home) if h5_home else 0.25)\n",
    "            features['L5AWR'].append(sum(1 for r in a5_away if r == 'A')/len(a5_away) if a5_away else 0.30)\n",
    "            features['L5ADR'].append(sum(1 for r in a5_away if r == 'D')/len(a5_away) if a5_away else 0.25)\n",
    "            \n",
    "            # goal\n",
    "            gf_h = list(self.state['team_goals_for'][h])\n",
    "            ga_h = list(self.state['team_goals_against'][h])\n",
    "            gf_a = list(self.state['team_goals_for'][a])\n",
    "            ga_a = list(self.state['team_goals_against'][a])\n",
    "            features['goals_pm_home'].append(np.mean(gf_h) if gf_h else 1.3)\n",
    "            features['goals_pm_away'].append(np.mean(gf_a) if gf_a else 1.3)\n",
    "            features['conceded_pm_home'].append(np.mean(ga_h) if ga_h else 1.3)\n",
    "            features['conceded_pm_away'].append(np.mean(ga_a) if ga_a else 1.3)\n",
    "            \n",
    "            gd_h = list(self.state['gd_history'][h])\n",
    "            gd_a = list(self.state['gd_history'][a])\n",
    "            features['gd_pm_home'].append(np.mean(gd_h) if gd_h else 0.0)\n",
    "            features['gd_pm_away'].append(np.mean(gd_a) if gd_a else 0.0)\n",
    "            features['gd_pm_diff'].append(features['gd_pm_home'][-1] - features['gd_pm_away'][-1])\n",
    "            \n",
    "            # standings\n",
    "            pts_h = self.state['season_points'][season][h]\n",
    "            pts_a = self.state['season_points'][season][a]\n",
    "            gd_season_h = self.state['season_gd'][season][h]\n",
    "            gd_season_a = self.state['season_gd'][season][a]\n",
    "            features['points_home'].append(pts_h)\n",
    "            features['points_away'].append(pts_a)\n",
    "            features['points_diff'].append(pts_h - pts_a)\n",
    "            features['season_gd_diff'].append(gd_season_h - gd_season_a)\n",
    "            \n",
    "            # position\n",
    "            all_teams = list(self.state['season_points'][season].keys())\n",
    "            if all_teams:\n",
    "                sorted_teams = sorted(all_teams,\n",
    "                    key=lambda t: (self.state['season_points'][season][t], self.state['season_gd'][season][t]),\n",
    "                    reverse=True)\n",
    "                pos_h = sorted_teams.index(h) + 1 if h in sorted_teams else 10\n",
    "                pos_a = sorted_teams.index(a) + 1 if a in sorted_teams else 10\n",
    "            else:\n",
    "                pos_h, pos_a = 10, 10\n",
    "            features['position_home'].append(pos_h)\n",
    "            features['position_away'].append(pos_a)\n",
    "            features['position_diff'].append(pos_a - pos_h)\n",
    "            \n",
    "            # H2H\n",
    "            key = tuple(sorted([h, a]))\n",
    "            past_h2h = self.state['h2h_history'][key][-10:]\n",
    "            if past_h2h:\n",
    "                h_wins = sum(1 for r in past_h2h if (r['home'] == h and r['result'] == 'H') or (r['home'] == a and r['result'] == 'A'))\n",
    "                draws = sum(1 for r in past_h2h if r['result'] == 'D')\n",
    "                total = len(past_h2h)\n",
    "                features['h2h_home_rate'].append(h_wins / total)\n",
    "                features['h2h_draw_rate'].append(draws / total)\n",
    "                features['h2h_away_rate'].append((total - h_wins - draws) / total)\n",
    "            else:\n",
    "                features['h2h_home_rate'].append(0.45)\n",
    "                features['h2h_draw_rate'].append(0.25)\n",
    "                features['h2h_away_rate'].append(0.30)\n",
    "            \n",
    "            # rest\n",
    "            last_h = self.state['last_match_date'].get(h)\n",
    "            last_a = self.state['last_match_date'].get(a)\n",
    "            rest_h = (date - last_h).days if last_h else 7\n",
    "            rest_a = (date - last_a).days if last_a else 7\n",
    "            features['rest_days_home'].append(np.clip(rest_h, 1, 21))\n",
    "            features['rest_days_away'].append(np.clip(rest_a, 1, 21))\n",
    "            features['rest_diff'].append(rest_h - rest_a)\n",
    "            \n",
    "            # referee\n",
    "            ref = row.get(\"Referee\", \"Unknown\")\n",
    "            ref_stats = self.state['referee_stats'][ref]\n",
    "            total_ref = ref_stats['H'] + ref_stats['D'] + ref_stats['A']\n",
    "            if total_ref > 0:\n",
    "                features['ref_home_rate'].append(ref_stats['H'] / total_ref)\n",
    "                features['ref_draw_rate'].append(ref_stats['D'] / total_ref)\n",
    "                features['ref_away_rate'].append(ref_stats['A'] / total_ref)\n",
    "            else:\n",
    "                features['ref_home_rate'].append(0.45)\n",
    "                features['ref_draw_rate'].append(0.25)\n",
    "                features['ref_away_rate'].append(0.30)\n",
    "            features['ref_matches'].append(total_ref)\n",
    "            features['ref_home_bias'].append(features['ref_home_rate'][-1] - features['ref_away_rate'][-1])\n",
    "            \n",
    "            # Elo\n",
    "            if self.state['Current_season'] is None:\n",
    "                self.state['Current_season'] = season\n",
    "            if season != self.state['Current_season'] and is_train:\n",
    "                for team in list(self.state['elo'].keys()):\n",
    "                    self.state['elo'][team] = 0.7 * self.state['elo'][team] + 0.3 * 1500.0\n",
    "                    self.state['elo_att'][team] = 0.7 * self.state['elo_att'][team] + 0.3 * 1500.0\n",
    "                    self.state['elo_def'][team] = 0.7 * self.state['elo_def'][team] + 0.3 * 1500.0\n",
    "                self.state['Current_season'] = season\n",
    "            \n",
    "            features['elo_home'].append(self.state['elo'][h])\n",
    "            features['elo_away'].append(self.state['elo'][a])\n",
    "            features['elo_diff'].append(self.state['elo'][h] - self.state['elo'][a] + 75)\n",
    "            features['elo_att_home'].append(self.state['elo_att'][h])\n",
    "            features['elo_def_home'].append(self.state['elo_def'][h])\n",
    "            features['elo_att_away'].append(self.state['elo_att'][a])\n",
    "            features['elo_def_away'].append(self.state['elo_def'][a])\n",
    "            \n",
    "            # draw tendency\n",
    "            draw_hist_h = list(self.state['team_draw_overall'][h])\n",
    "            draw_hist_a = list(self.state['team_draw_overall'][a])\n",
    "            features['draw_prop_home'].append(np.mean(draw_hist_h) if draw_hist_h else 0.25)\n",
    "            features['draw_prop_away'].append(np.mean(draw_hist_a) if draw_hist_a else 0.25)\n",
    "            features['draw_prop_sum'].append(features['draw_prop_home'][-1] + features['draw_prop_away'][-1])\n",
    "            \n",
    "            # update state\n",
    "            if is_train and 'FTR' in row and pd.notna(row['FTR']):\n",
    "                ftr = row['FTR']\n",
    "                gh = row.get('FTHG', 0) or 0\n",
    "                ga = row.get('FTAG', 0) or 0\n",
    "                \n",
    "                if ftr == 'H':\n",
    "                    self.state['team_results'][h].append(3)\n",
    "                    self.state['team_results'][a].append(0)\n",
    "                    self.state['season_points'][season][h] += 3\n",
    "                elif ftr == 'D':\n",
    "                    self.state['team_results'][h].append(1)\n",
    "                    self.state['team_results'][a].append(1)\n",
    "                    self.state['season_points'][season][h] += 1\n",
    "                    self.state['season_points'][season][a] += 1\n",
    "                else:\n",
    "                    self.state['team_results'][h].append(0)\n",
    "                    self.state['team_results'][a].append(3)\n",
    "                    self.state['season_points'][season][a] += 3\n",
    "                \n",
    "                self.state['home_results_5'][h].append(ftr)\n",
    "                self.state['away_results_5'][a].append(ftr)\n",
    "                self.state['team_goals_for'][h].append(gh)\n",
    "                self.state['team_goals_against'][h].append(ga)\n",
    "                self.state['team_goals_for'][a].append(ga)\n",
    "                self.state['team_goals_against'][a].append(gh)\n",
    "                self.state['gd_history'][h].append(gh - ga)\n",
    "                self.state['gd_history'][a].append(ga - gh)\n",
    "                self.state['season_gd'][season][h] += (gh - ga)\n",
    "                self.state['season_gd'][season][a] += (ga - gh)\n",
    "                self.state['h2h_history'][key].append({'home': h, 'result': ftr})\n",
    "                \n",
    "                ref = row.get(\"Referee\", \"Unknown\")\n",
    "                self.state['referee_stats'][ref][ftr] += 1\n",
    "                \n",
    "                exp_h = 1 / (1 + 10 ** ((self.state['elo'][a] - self.state['elo'][h] - 75) / 400))\n",
    "                act_h = 1 if ftr == 'H' else (0.5 if ftr == 'D' else 0)\n",
    "                K = 20\n",
    "                self.state['elo'][h] += K * (act_h - exp_h)\n",
    "                self.state['elo'][a] += K * ((1 - act_h) - (1 - exp_h))\n",
    "                \n",
    "                K_att = 20 * (1 + np.log1p(abs(gh - ga)))\n",
    "                lam_h = np.exp((self.state['elo_att'][h] - self.state['elo_def'][a] + 75) / 400)\n",
    "                lam_a = np.exp((self.state['elo_att'][a] - self.state['elo_def'][h]) / 400)\n",
    "                self.state['elo_att'][h] += K_att * (gh - lam_h)\n",
    "                self.state['elo_def'][a] += K_att * (lam_h - gh)\n",
    "                self.state['elo_att'][a] += K_att * (ga - lam_a)\n",
    "                self.state['elo_def'][h] += K_att * (lam_a - ga)\n",
    "                \n",
    "                is_draw = 1 if ftr == 'D' else 0\n",
    "                self.state['team_draw_overall'][h].append(is_draw)\n",
    "                self.state['team_draw_overall'][a].append(is_draw)\n",
    "            \n",
    "            self.state['last_match_date'][h] = date\n",
    "            self.state['last_match_date'][a] = date\n",
    "        for col, values in features.items():\n",
    "            df[col] = values\n",
    "            \n",
    "        return df\n",
    "\n",
    "#  basic form + gd_pm_*\n",
    "def add_basic_form_features(df):\n",
    "    df = df.sort_values(\"Date\").copy()\n",
    "    team_results = defaultdict(lambda: deque(maxlen=5))\n",
    "    form_home, form_away = [], []\n",
    "    win_streak_home, win_streak_away = [], []\n",
    "    unbeaten_home, unbeaten_away = [], []\n",
    "    gd_history = defaultdict(lambda: deque(maxlen=5))\n",
    "    gd_pm_home, gd_pm_away = [], []\n",
    "    gd_pm_diff = []\n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        # form\n",
    "        h_results = list(team_results[h])\n",
    "        if h_results:\n",
    "            form_home.append(sum(h_results) / (len(h_results) * 3))\n",
    "            win_streak_home.append(sum(1 for r in reversed(h_results) if r == 3))\n",
    "            unbeaten_home.append(sum(1 for r in reversed(h_results) if r >= 1))\n",
    "        else:\n",
    "            form_home.append(0.0)\n",
    "            win_streak_home.append(0)\n",
    "            unbeaten_home.append(0)\n",
    "        a_results = list(team_results[a])\n",
    "        if a_results:\n",
    "            form_away.append(sum(a_results) / (len(a_results) * 3))\n",
    "            win_streak_away.append(sum(1 for r in reversed(a_results) if r == 3))\n",
    "            unbeaten_away.append(sum(1 for r in reversed(a_results) if r >= 1))\n",
    "        else:\n",
    "            form_away.append(0.0)\n",
    "            win_streak_away.append(0)\n",
    "            unbeaten_away.append(0)\n",
    "        # result(home 3, 1, 0)\n",
    "        res_h = 3 if row[\"FTR\"] == 'H' else (1 if row[\"FTR\"] == 'D' else 0)\n",
    "        res_a = 3 if row[\"FTR\"] == 'A' else (1 if row[\"FTR\"] == 'D' else 0)\n",
    "        team_results[h].append(res_h)\n",
    "        team_results[a].append(res_a)\n",
    "        if gd_history[h]:\n",
    "            gd_pm_home.append(np.mean(gd_history[h]))\n",
    "        else:\n",
    "            gd_pm_home.append(0.0)\n",
    "        if gd_history[a]:\n",
    "            gd_pm_away.append(np.mean(gd_history[a]))\n",
    "        else:\n",
    "            gd_pm_away.append(0.0)\n",
    "        gd_pm_diff.append(gd_pm_home[-1] - gd_pm_away[-1])\n",
    "        gd_history[h].append(row[\"FTHG\"] - row[\"FTAG\"])\n",
    "        gd_history[a].append(row[\"FTAG\"] - row[\"FTHG\"])\n",
    "    df[\"form_home\"] = form_home\n",
    "    df[\"form_away\"] = form_away\n",
    "    df[\"win_streak_home\"] = win_streak_home\n",
    "    df[\"win_streak_away\"] = win_streak_away\n",
    "    df[\"unbeaten_home\"] = unbeaten_home\n",
    "    df[\"unbeaten_away\"] = unbeaten_away\n",
    "    df[\"gd_pm_home\"] = gd_pm_home\n",
    "    df[\"gd_pm_away\"] = gd_pm_away\n",
    "    df[\"gd_pm_diff\"] = gd_pm_diff\n",
    "    return df\n",
    "\n",
    "#  L5/L10 Feature\n",
    "def add_venue_specific_form(df):\n",
    "    df = df.sort_values(\"Date\").copy()\n",
    "    home_results_5 = defaultdict(lambda: deque(maxlen=5))\n",
    "    home_results_10 = defaultdict(lambda: deque(maxlen=10))\n",
    "    away_results_5 = defaultdict(lambda: deque(maxlen=5))\n",
    "    away_results_10 = defaultdict(lambda: deque(maxlen=10))\n",
    "    L5HWR, L5HDR, L5HLR = [], [], []\n",
    "    L5AWR, L5ADR, L5ALR = [], [], []\n",
    "    L10HWR, L10HDR, L10HLR = [], [], []\n",
    "    L10AWR, L10ADR, L10ALR = [], [], []\n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        res = row[\"FTR\"]\n",
    "        # home5/10\n",
    "        h5 = list(home_results_5[h])\n",
    "        L5HWR.append(sum(1 for r in h5 if r == 'H')/len(h5) if h5 else 0.0)\n",
    "        L5HDR.append(sum(1 for r in h5 if r == 'D')/len(h5) if h5 else 0.0)\n",
    "        L5HLR.append(sum(1 for r in h5 if r == 'A')/len(h5) if h5 else 0.0)\n",
    "        h10 = list(home_results_10[h])\n",
    "        L10HWR.append(sum(1 for r in h10 if r == 'H')/len(h10) if h10 else 0.0)\n",
    "        L10HDR.append(sum(1 for r in h10 if r == 'D')/len(h10) if h10 else 0.0)\n",
    "        L10HLR.append(sum(1 for r in h10 if r == 'A')/len(h10) if h10 else 0.0)\n",
    "        # away5/10\n",
    "        a5 = list(away_results_5[a])\n",
    "        L5AWR.append(sum(1 for r in a5 if r == 'H')/len(a5) if a5 else 0.0)\n",
    "        L5ADR.append(sum(1 for r in a5 if r == 'D')/len(a5) if a5 else 0.0)\n",
    "        L5ALR.append(sum(1 for r in a5 if r == 'A')/len(a5) if a5 else 0.0)\n",
    "        a10 = list(away_results_10[a])\n",
    "        L10AWR.append(sum(1 for r in a10 if r == 'H')/len(a10) if a10 else 0.0)\n",
    "        L10ADR.append(sum(1 for r in a10 if r == 'D')/len(a10) if a10 else 0.0)\n",
    "        L10ALR.append(sum(1 for r in a10 if r == 'A')/len(a10) if a10 else 0.0)\n",
    "        # resultrecord\n",
    "        home_results_5[h].append(res)\n",
    "        away_results_5[a].append(res)\n",
    "        home_results_10[h].append(res)\n",
    "        away_results_10[a].append(res)\n",
    "    df[\"L5HWR\"] = L5HWR; df[\"L5HDR\"] = L5HDR; df[\"L5HLR\"] = L5HLR\n",
    "    df[\"L10HWR\"] = L10HWR; df[\"L10HDR\"] = L10HDR; df[\"L10HLR\"] = L10HLR\n",
    "    df[\"L5AWR\"] = L5AWR; df[\"L5ADR\"] = L5ADR; df[\"L5ALR\"] = L5ALR\n",
    "    df[\"L10AWR\"] = L10AWR; df[\"L10ADR\"] = L10ADR; df[\"L10ALR\"] = L10ALR\n",
    "    df[\"L5_home_adv\"] = np.array(L5HWR) - np.array(L5AWR)\n",
    "    df[\"L10_home_adv\"] = np.array(L10HWR) - np.array(L10AWR)\n",
    "    return df\n",
    "\n",
    "def add_prematch_shot_form_v2(df, window=6):\n",
    "    df = df.sort_values(\"Date\").copy()\n",
    "    \n",
    "    if not all(c in df.columns for c in [\"HS\", \"AS\", \"HST\", \"AST\"]):\n",
    "        print(\"skip\")\n",
    "        return df\n",
    "    \n",
    "    team_history = defaultdict(lambda: deque(maxlen=window))\n",
    "    \n",
    "    # Result\n",
    "    results = {\n",
    "        'shots_for_pm_home': [], 'sot_for_pm_home': [],\n",
    "        'shots_against_pm_home': [], 'sot_against_pm_home': [],\n",
    "        'shots_for_pm_away': [], 'sot_for_pm_away': [],\n",
    "        'shots_against_pm_away': [], 'sot_against_pm_away': [],\n",
    "    }\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        home_team, away_team = row['HomeTeam'], row['AwayTeam']\n",
    "        \n",
    "        h_hist = list(team_history[home_team])\n",
    "        if h_hist:\n",
    "            results['shots_for_pm_home'].append(np.mean([h['shots'] for h in h_hist]))\n",
    "            results['sot_for_pm_home'].append(np.mean([h['sot'] for h in h_hist]))\n",
    "            results['shots_against_pm_home'].append(np.mean([h['shots_ag'] for h in h_hist]))\n",
    "            results['sot_against_pm_home'].append(np.mean([h['sot_ag'] for h in h_hist]))\n",
    "        else:\n",
    "            for k in ['shots_for_pm_home', 'sot_for_pm_home', 'shots_against_pm_home', 'sot_against_pm_home']:\n",
    "                results[k].append(np.nan)\n",
    "        \n",
    "        a_hist = list(team_history[away_team])\n",
    "        if a_hist:\n",
    "            results['shots_for_pm_away'].append(np.mean([h['shots'] for h in a_hist]))\n",
    "            results['sot_for_pm_away'].append(np.mean([h['sot'] for h in a_hist]))\n",
    "            results['shots_against_pm_away'].append(np.mean([h['shots_ag'] for h in a_hist]))\n",
    "            results['sot_against_pm_away'].append(np.mean([h['sot_ag'] for h in a_hist]))\n",
    "        else:\n",
    "            for k in ['shots_for_pm_away', 'sot_for_pm_away', 'shots_against_pm_away', 'sot_against_pm_away']:\n",
    "                results[k].append(np.nan)\n",
    "        \n",
    "        team_history[home_team].append({\n",
    "            'shots': row['HS'], 'sot': row['HST'],\n",
    "            'shots_ag': row['AS'], 'sot_ag': row['AST']\n",
    "        })\n",
    "        team_history[away_team].append({\n",
    "            'shots': row['AS'], 'sot': row['AST'],\n",
    "            'shots_ag': row['HS'], 'sot_ag': row['HST']\n",
    "        })\n",
    "    \n",
    "    #  df\n",
    "    for col, values in results.items():\n",
    "        df[col] = values\n",
    "    \n",
    "    df['shots_pm_diff'] = df['shots_for_pm_home'] - df['shots_for_pm_away']\n",
    "    df['sot_pm_diff'] = df['sot_for_pm_home'] - df['sot_for_pm_away']\n",
    "    df['shot_accuracy_pm_home'] = df['sot_for_pm_home'] / df['shots_for_pm_home'].replace(0, np.nan)\n",
    "    df['shot_accuracy_pm_away'] = df['sot_for_pm_away'] / df['shots_for_pm_away'].replace(0, np.nan)\n",
    "    df['shot_accuracy_pm_diff'] = df['shot_accuracy_pm_home'] - df['shot_accuracy_pm_away']\n",
    "    \n",
    "    # padding NaN\n",
    "    pm_cols = [c for c in df.columns if '_pm' in c and 'shot' in c.lower()]\n",
    "    for col in pm_cols:\n",
    "        df[col] = df[col].fillna(df[col].median() if df[col].notna().any() else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_prematch_corners_fouls(df, window=6):\n",
    "    df = df.sort_values(\"Date\").copy()\n",
    "    \n",
    "    has_corners = all(c in df.columns for c in [\"HC\", \"AC\"])\n",
    "    has_fouls = all(c in df.columns for c in [\"HF\", \"AF\"])\n",
    "    has_cards = all(c in df.columns for c in [\"HY\", \"AY\"])\n",
    "    \n",
    "    if not (has_corners or has_fouls or has_cards):\n",
    "        print(f\"cornar/yellow/red cards data is missing, skip\")\n",
    "        return df\n",
    "    \n",
    "    # record\n",
    "    team_corners = defaultdict(lambda: deque(maxlen=window))\n",
    "    team_fouls = defaultdict(lambda: deque(maxlen=window))\n",
    "    team_yellows = defaultdict(lambda: deque(maxlen=window))\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        home_team, away_team = row['HomeTeam'], row['AwayTeam']\n",
    "        \n",
    "        # corner\n",
    "        if has_corners:\n",
    "            h_corners = list(team_corners[home_team])\n",
    "            a_corners = list(team_corners[away_team])\n",
    "            \n",
    "            results['corners_for_pm_home'].append(np.mean([c[0] for c in h_corners]) if h_corners else np.nan)\n",
    "            results['corners_against_pm_home'].append(np.mean([c[1] for c in h_corners]) if h_corners else np.nan)\n",
    "            results['corners_for_pm_away'].append(np.mean([c[0] for c in a_corners]) if a_corners else np.nan)\n",
    "            results['corners_against_pm_away'].append(np.mean([c[1] for c in a_corners]) if a_corners else np.nan)\n",
    "        \n",
    "        # foul\n",
    "        if has_fouls:\n",
    "            h_fouls = list(team_fouls[home_team])\n",
    "            a_fouls = list(team_fouls[away_team])\n",
    "            \n",
    "            results['fouls_pm_home'].append(np.mean([f[0] for f in h_fouls]) if h_fouls else np.nan)\n",
    "            results['fouls_against_pm_home'].append(np.mean([f[1] for f in h_fouls]) if h_fouls else np.nan)\n",
    "            results['fouls_pm_away'].append(np.mean([f[0] for f in a_fouls]) if a_fouls else np.nan)\n",
    "            results['fouls_against_pm_away'].append(np.mean([f[1] for f in a_fouls]) if a_fouls else np.nan)\n",
    "        \n",
    "        # yellow card\n",
    "        if has_cards:\n",
    "            h_yellows = list(team_yellows[home_team])\n",
    "            a_yellows = list(team_yellows[away_team])\n",
    "            \n",
    "            results['yellows_pm_home'].append(np.mean(h_yellows) if h_yellows else np.nan)\n",
    "            results['yellows_pm_away'].append(np.mean(a_yellows) if a_yellows else np.nan)\n",
    "        \n",
    "        if has_corners:\n",
    "            team_corners[home_team].append((row['HC'], row['AC']))\n",
    "            team_corners[away_team].append((row['AC'], row['HC']))\n",
    "        if has_fouls:\n",
    "            team_fouls[home_team].append((row['HF'], row['AF']))\n",
    "            team_fouls[away_team].append((row['AF'], row['HF']))\n",
    "        if has_cards:\n",
    "            team_yellows[home_team].append(row['HY'])\n",
    "            team_yellows[away_team].append(row['AY'])\n",
    "    \n",
    "    #  df\n",
    "    for col, values in results.items():\n",
    "        df[col] = values\n",
    "    \n",
    "    if has_corners:\n",
    "        df['corners_pm_diff'] = df['corners_for_pm_home'] - df['corners_for_pm_away']\n",
    "    if has_fouls:\n",
    "        df['fouls_pm_diff'] = df['fouls_pm_home'] - df['fouls_pm_away']\n",
    "    if has_cards:\n",
    "        df['yellows_pm_diff'] = df['yellows_pm_home'] - df['yellows_pm_away']\n",
    "    \n",
    "    # padding NaN\n",
    "    new_cols = list(results.keys())\n",
    "    for col in new_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(df[col].median() if df[col].notna().any() else 0)\n",
    "    \n",
    "    print(f\"corner, foul, yellow/red cards history features are added\")\n",
    "    return df\n",
    "\n",
    "def add_advanced_h2h_features(df, max_matches=10, half_life_seasons=2.0):\n",
    "    df = df.sort_values(\"Date\").copy()\n",
    "    lambda_decay = np.log(2) / half_life_seasons\n",
    "\n",
    "    h2h_history = defaultdict(list)\n",
    "    h2h_home_rate_td = []\n",
    "    h2h_draw_rate_td = []\n",
    "    h2h_away_rate_td = []\n",
    "    h2h_matches_td = []\n",
    "    h2h_goal_diff_avg = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        d = row[\"Date\"]\n",
    "        # key(,homeaway)\n",
    "        key = tuple(sorted([h, a]))\n",
    "\n",
    "        past = h2h_history[key]\n",
    "        # max_matches \n",
    "        past = past[-max_matches:]\n",
    "\n",
    "        if past:\n",
    "            weights = []\n",
    "            home_win_w, draw_w, away_win_w = 0.0, 0.0, 0.0\n",
    "            gd_weighted_sum = 0.0\n",
    "            total_w = 0.0\n",
    "\n",
    "            for rec in past:\n",
    "                days_diff = (d - rec[\"date\"]).days\n",
    "                year_diff = max(days_diff / 365.25, 0.0)\n",
    "                w = np.exp(-lambda_decay * year_diff)\n",
    "                weights.append(w)\n",
    "                total_w += w\n",
    "\n",
    "                past_home = rec[\"home\"]\n",
    "                past_result = rec[\"result\"]\n",
    "                past_gd = rec[\"goal_diff\"]  # past_home \n",
    "\n",
    "                if past_home == h:\n",
    "                    gd_view_h = past_gd\n",
    "                    if past_result == \"H\":\n",
    "                        home_win_w += w\n",
    "                    elif past_result == \"D\":\n",
    "                        draw_w += w\n",
    "                    else:  # 'A'\n",
    "                        away_win_w += w\n",
    "                else:\n",
    "                    # home yes a,home h away\n",
    "                    gd_view_h = -past_gd\n",
    "                    if past_result == \"H\":\n",
    "                    # home a, a → home h lose\n",
    "                        away_win_w += w\n",
    "                    elif past_result == \"D\":\n",
    "                        draw_w += w\n",
    "                    else:  # 'A',away h, h\n",
    "                        home_win_w += w\n",
    "\n",
    "                gd_weighted_sum += gd_view_h * w\n",
    "\n",
    "            if total_w > 0:\n",
    "                h2h_home_rate_td.append(home_win_w / total_w)\n",
    "                h2h_draw_rate_td.append(draw_w / total_w)\n",
    "                h2h_away_rate_td.append(away_win_w / total_w)\n",
    "                h2h_matches_td.append(total_w)\n",
    "                h2h_goal_diff_avg.append(gd_weighted_sum / total_w)\n",
    "            else:\n",
    "                h2h_home_rate_td.append(0.45)\n",
    "                h2h_draw_rate_td.append(0.25)\n",
    "                h2h_away_rate_td.append(0.30)\n",
    "                h2h_matches_td.append(0.0)\n",
    "                h2h_goal_diff_avg.append(0.0)\n",
    "        else:\n",
    "            h2h_home_rate_td.append(0.45)\n",
    "            h2h_draw_rate_td.append(0.25)\n",
    "            h2h_away_rate_td.append(0.30)\n",
    "            h2h_matches_td.append(0.0)\n",
    "            h2h_goal_diff_avg.append(0.0)\n",
    "\n",
    "        gh, ga = row[\"FTHG\"], row[\"FTAG\"]\n",
    "        if gh > ga:\n",
    "            res = \"H\"\n",
    "        elif gh == ga:\n",
    "            res = \"D\"\n",
    "        else:\n",
    "            res = \"A\"\n",
    "        h2h_history[key].append({\n",
    "            \"date\": d,\n",
    "            \"home\": h,\n",
    "            \"result\": res,\n",
    "            \"goal_diff\": gh - ga\n",
    "        })\n",
    "\n",
    "    df[\"h2h_home_rate_td\"] = h2h_home_rate_td\n",
    "    df[\"h2h_draw_rate_td\"] = h2h_draw_rate_td\n",
    "    df[\"h2h_away_rate_td\"] = h2h_away_rate_td\n",
    "    df[\"h2h_matches_td\"] = h2h_matches_td\n",
    "    df[\"h2h_goal_diff_avg\"] = h2h_goal_diff_avg\n",
    "\n",
    "    print(\"Added advanced H2H features (time-decayed & directional)\")\n",
    "    return df\n",
    "\n",
    "def add_referee_features(df):\n",
    "    df = df.sort_values(\"Date\").copy()\n",
    "    if \"Referee\" not in df.columns:\n",
    "        return df\n",
    "    referee_stats = defaultdict(lambda: {'H':0, 'D':0, 'A':0, 'yellow':[], 'red':[], 'fouls':[]})\n",
    "    ref_home_rate, ref_draw_rate, ref_away_rate = [], [], []\n",
    "    ref_avg_yellow, ref_avg_red, ref_avg_fouls = [], [], []\n",
    "    ref_matches = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ref = row[\"Referee\"]\n",
    "        stats = referee_stats[ref]\n",
    "        total = stats['H'] + stats['D'] + stats['A']\n",
    "        if total > 0:\n",
    "            ref_home_rate.append(stats['H']/total)\n",
    "            ref_draw_rate.append(stats['D']/total)\n",
    "            ref_away_rate.append(stats['A']/total)\n",
    "        else:\n",
    "            ref_home_rate.append(0.0); ref_draw_rate.append(0.0); ref_away_rate.append(0.0)\n",
    "        ref_avg_yellow.append(np.mean(stats['yellow']) if stats['yellow'] else 0.0)\n",
    "        ref_avg_red.append(np.mean(stats['red']) if stats['red'] else 0.0)\n",
    "        ref_avg_fouls.append(np.mean(stats['fouls']) if stats['fouls'] else 0.0)\n",
    "        ref_matches.append(total)\n",
    "        stats['yellow'].append(row[\"HY\"] + row[\"AY\"])\n",
    "        stats['red'].append(row[\"HR\"] + row[\"AR\"])\n",
    "        stats['fouls'].append(row[\"HF\"] + row[\"AF\"])\n",
    "        stats[row[\"FTR\"]] += 1\n",
    "    df[\"ref_home_rate_v2\"] = ref_home_rate\n",
    "    df[\"ref_draw_rate_v2\"] = ref_draw_rate\n",
    "    df[\"ref_away_rate_v2\"] = ref_away_rate\n",
    "    df[\"ref_avg_yellow\"] = ref_avg_yellow\n",
    "    df[\"ref_avg_red\"] = ref_avg_red\n",
    "    df[\"ref_avg_fouls\"] = ref_avg_fouls\n",
    "    df[\"ref_matches_v2\"] = ref_matches\n",
    "    df[\"ref_home_bias_v2\"] = np.array(ref_home_rate) - np.array(ref_away_rate)\n",
    "    return df\n",
    "\n",
    "def add_all_advanced_features(df):\n",
    "    df = df.sort_values(\"Date\").copy()\n",
    "    \n",
    "    team_results = defaultdict(lambda: deque(maxlen=5))\n",
    "    \n",
    "    form_home, form_away = [], []\n",
    "    win_streak_home, win_streak_away = [], []\n",
    "    unbeaten_home, unbeaten_away = [], []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        \n",
    "        # home team\n",
    "        h_results = list(team_results[h])\n",
    "        if h_results:\n",
    "            form_home.append(sum(h_results) / (len(h_results) * 3))\n",
    "            streak = sum(1 for r in reversed(h_results) if r == 3)\n",
    "            win_streak_home.append(min(streak, len([r for r in reversed(h_results) if r == 3])))\n",
    "            unbeaten = sum(1 for i, r in enumerate(reversed(h_results)) if r >= 1 and all(x >= 1 for x in list(reversed(h_results))[:i+1]))\n",
    "            unbeaten_home.append(unbeaten)\n",
    "        else:\n",
    "            form_home.append(0.5)\n",
    "            win_streak_home.append(0)\n",
    "            unbeaten_home.append(0)\n",
    "        \n",
    "        # away team\n",
    "        a_results = list(team_results[a])\n",
    "        if a_results:\n",
    "            form_away.append(sum(a_results) / (len(a_results) * 3))\n",
    "            streak = sum(1 for r in reversed(a_results) if r == 3)\n",
    "            win_streak_away.append(min(streak, len([r for r in reversed(a_results) if r == 3])))\n",
    "            unbeaten = sum(1 for i, r in enumerate(reversed(a_results)) if r >= 1 and all(x >= 1 for x in list(reversed(a_results))[:i+1]))\n",
    "            unbeaten_away.append(unbeaten)\n",
    "        else:\n",
    "            form_away.append(0.5)\n",
    "            win_streak_away.append(0)\n",
    "            unbeaten_away.append(0)\n",
    "        \n",
    "        # result\n",
    "        ftr = row[\"FTR\"]\n",
    "        if ftr == \"H\":\n",
    "            team_results[h].append(3)\n",
    "            team_results[a].append(0)\n",
    "        elif ftr == \"D\":\n",
    "            team_results[h].append(1)\n",
    "            team_results[a].append(1)\n",
    "        else:\n",
    "            team_results[h].append(0)\n",
    "            team_results[a].append(3)\n",
    "    \n",
    "    df[\"form_home_v2\"] = form_home\n",
    "    df[\"form_away_v2\"] = form_away\n",
    "    df[\"form_diff_v2\"] = np.array(form_home) - np.array(form_away)\n",
    "    df[\"win_streak_home\"] = win_streak_home\n",
    "    df[\"win_streak_away\"] = win_streak_away\n",
    "    df[\"unbeaten_home\"] = unbeaten_home\n",
    "    df[\"unbeaten_away\"] = unbeaten_away\n",
    "    \n",
    "    home_home_results = defaultdict(lambda: deque(maxlen=5))\n",
    "    away_away_results = defaultdict(lambda: deque(maxlen=5))\n",
    "    \n",
    "    home_home_form, away_away_form = [], []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        \n",
    "        hh = list(home_home_results[h])\n",
    "        home_home_form.append(sum(hh) / (len(hh) * 3) if hh else 0.5)\n",
    "        \n",
    "        aa = list(away_away_results[a])\n",
    "        away_away_form.append(sum(aa) / (len(aa) * 3) if aa else 0.5)\n",
    "        \n",
    "        ftr = row[\"FTR\"]\n",
    "        if ftr == \"H\":\n",
    "            home_home_results[h].append(3)\n",
    "            away_away_results[a].append(0)\n",
    "        elif ftr == \"D\":\n",
    "            home_home_results[h].append(1)\n",
    "            away_away_results[a].append(1)\n",
    "        else:\n",
    "            home_home_results[h].append(0)\n",
    "            away_away_results[a].append(3)\n",
    "    \n",
    "    df[\"home_home_form\"] = home_home_form\n",
    "    df[\"away_away_form\"] = away_away_form\n",
    "    \n",
    "    team_goals_for = defaultdict(lambda: deque(maxlen=5))\n",
    "    team_goals_against = defaultdict(lambda: deque(maxlen=5))\n",
    "    \n",
    "    attack_mom_home, attack_mom_away = [], []\n",
    "    defense_mom_home, defense_mom_away = [], []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        \n",
    "        gf_h = list(team_goals_for[h])\n",
    "        ga_h = list(team_goals_against[h])\n",
    "        attack_mom_home.append(np.mean(gf_h) if gf_h else 1.3)\n",
    "        defense_mom_home.append(np.mean(ga_h) if ga_h else 1.3)\n",
    "        \n",
    "        gf_a = list(team_goals_for[a])\n",
    "        ga_a = list(team_goals_against[a])\n",
    "        attack_mom_away.append(np.mean(gf_a) if gf_a else 1.3)\n",
    "        defense_mom_away.append(np.mean(ga_a) if ga_a else 1.3)\n",
    "        \n",
    "        team_goals_for[h].append(row[\"FTHG\"])\n",
    "        team_goals_against[h].append(row[\"FTAG\"])\n",
    "        team_goals_for[a].append(row[\"FTAG\"])\n",
    "        team_goals_against[a].append(row[\"FTHG\"])\n",
    "    \n",
    "    df[\"attack_momentum_home\"] = attack_mom_home\n",
    "    df[\"attack_momentum_away\"] = attack_mom_away\n",
    "    df[\"defense_momentum_home\"] = defense_mom_home\n",
    "    df[\"defense_momentum_away\"] = defense_mom_away\n",
    "    df[\"attack_vs_defense_home\"] = np.array(attack_mom_home) - np.array(defense_mom_away)\n",
    "    df[\"attack_vs_defense_away\"] = np.array(attack_mom_away) - np.array(defense_mom_home)\n",
    "    \n",
    "    season_points = defaultdict(lambda: defaultdict(int))\n",
    "    season_gd = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    position_home, position_away = [], []\n",
    "    points_home, points_away = [], []\n",
    "    gd_home, gd_away = [], []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        season = row[\"Season\"]\n",
    "        \n",
    "        pts_h = season_points[season][h]\n",
    "        pts_a = season_points[season][a]\n",
    "        gd_h_val = season_gd[season][h]\n",
    "        gd_a_val = season_gd[season][a]\n",
    "        \n",
    "        points_home.append(pts_h)\n",
    "        points_away.append(pts_a)\n",
    "        gd_home.append(gd_h_val)\n",
    "        gd_away.append(gd_a_val)\n",
    "        \n",
    "        all_teams = list(season_points[season].keys())\n",
    "        if all_teams:\n",
    "            sorted_teams = sorted(all_teams, \n",
    "                                  key=lambda t: (season_points[season][t], season_gd[season][t]), \n",
    "                                  reverse=True)\n",
    "            pos_h = sorted_teams.index(h) + 1 if h in sorted_teams else 10\n",
    "            pos_a = sorted_teams.index(a) + 1 if a in sorted_teams else 10\n",
    "        else:\n",
    "            pos_h, pos_a = 10, 10\n",
    "        \n",
    "        position_home.append(pos_h)\n",
    "        position_away.append(pos_a)\n",
    "        \n",
    "        ftr = row[\"FTR\"]\n",
    "        gh, ga = row[\"FTHG\"], row[\"FTAG\"]\n",
    "        if ftr == \"H\":\n",
    "            season_points[season][h] += 3\n",
    "        elif ftr == \"D\":\n",
    "            season_points[season][h] += 1\n",
    "            season_points[season][a] += 1\n",
    "        else:\n",
    "            season_points[season][a] += 3\n",
    "        \n",
    "        season_gd[season][h] += (gh - ga)\n",
    "        season_gd[season][a] += (ga - gh)\n",
    "    \n",
    "    df[\"position_home_v2\"] = position_home\n",
    "    df[\"position_away_v2\"] = position_away\n",
    "    df[\"position_diff_v2\"] = np.array(position_away) - np.array(position_home)\n",
    "    df[\"points_home_v2\"] = points_home\n",
    "    df[\"points_away_v2\"] = points_away\n",
    "    df[\"points_diff_v2\"] = np.array(points_home) - np.array(points_away)\n",
    "    df[\"gd_home\"] = gd_home\n",
    "    df[\"gd_away\"] = gd_away\n",
    "    df[\"gd_diff\"] = np.array(gd_home) - np.array(gd_away)\n",
    "    \n",
    "    h2h_history = defaultdict(list)\n",
    "    \n",
    "    h2h_home_rate, h2h_draw_rate, h2h_away_rate = [], [], []\n",
    "    h2h_matches = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        key = tuple(sorted([h, a]))\n",
    "        \n",
    "        past = h2h_history[key][-10:]\n",
    "        \n",
    "        if past:\n",
    "            h_wins = sum(1 for r in past if \n",
    "                        (r[\"home\"] == h and r[\"result\"] == \"H\") or \n",
    "                        (r[\"home\"] == a and r[\"result\"] == \"A\"))\n",
    "            draws = sum(1 for r in past if r[\"result\"] == \"D\")\n",
    "            total = len(past)\n",
    "            \n",
    "            h2h_home_rate.append(h_wins / total)\n",
    "            h2h_draw_rate.append(draws / total)\n",
    "            h2h_away_rate.append((total - h_wins - draws) / total)\n",
    "            h2h_matches.append(total)\n",
    "        else:\n",
    "            h2h_home_rate.append(0.45)\n",
    "            h2h_draw_rate.append(0.25)\n",
    "            h2h_away_rate.append(0.30)\n",
    "            h2h_matches.append(0)\n",
    "        \n",
    "        h2h_history[key].append({\"home\": h, \"result\": row[\"FTR\"]})\n",
    "    \n",
    "    df[\"h2h_home_rate_v2\"] = h2h_home_rate\n",
    "    df[\"h2h_draw_rate_v2\"] = h2h_draw_rate\n",
    "    df[\"h2h_away_rate_v2\"] = h2h_away_rate\n",
    "    df[\"h2h_matches_v2\"] = h2h_matches\n",
    "    \n",
    "    last_match = {}\n",
    "    rest_home, rest_away = [], []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        d = row[\"Date\"]\n",
    "        \n",
    "        rest_h = (d - last_match[h]).days if h in last_match else 7\n",
    "        rest_a = (d - last_match[a]).days if a in last_match else 7\n",
    "        \n",
    "        rest_home.append(np.clip(rest_h, 1, 21))\n",
    "        rest_away.append(np.clip(rest_a, 1, 21))\n",
    "        \n",
    "        last_match[h] = d\n",
    "        last_match[a] = d\n",
    "    \n",
    "    df[\"rest_days_home_v2\"] = rest_home\n",
    "    df[\"rest_days_away_v2\"] = rest_away\n",
    "    df[\"rest_diff_v2\"] = np.array(rest_home) - np.array(rest_away)\n",
    "    \n",
    "    goals_history = defaultdict(lambda: deque(maxlen=5))\n",
    "    \n",
    "    gs_pm_home, gc_pm_home = [], []\n",
    "    gs_pm_away, gc_pm_away = [], []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        \n",
    "        h_hist = list(goals_history[h])\n",
    "        a_hist = list(goals_history[a])\n",
    "        \n",
    "        if h_hist:\n",
    "            gs_pm_home.append(np.mean([x[0] for x in h_hist]))\n",
    "            gc_pm_home.append(np.mean([x[1] for x in h_hist]))\n",
    "        else:\n",
    "            gs_pm_home.append(1.3)\n",
    "            gc_pm_home.append(1.3)\n",
    "        \n",
    "        if a_hist:\n",
    "            gs_pm_away.append(np.mean([x[0] for x in a_hist]))\n",
    "            gc_pm_away.append(np.mean([x[1] for x in a_hist]))\n",
    "        else:\n",
    "            gs_pm_away.append(1.3)\n",
    "            gc_pm_away.append(1.3)\n",
    "        \n",
    "        goals_history[h].append((row[\"FTHG\"], row[\"FTAG\"]))\n",
    "        goals_history[a].append((row[\"FTAG\"], row[\"FTHG\"]))\n",
    "    \n",
    "    df[\"goals_scored_pm_home\"] = gs_pm_home\n",
    "    df[\"goals_conceded_pm_home\"] = gc_pm_home\n",
    "    df[\"goals_scored_pm_away\"] = gs_pm_away\n",
    "    df[\"goals_conceded_pm_away\"] = gc_pm_away\n",
    "    \n",
    "    league_avg_home = df[\"FTHG\"].mean()\n",
    "    league_avg_away = df[\"FTAG\"].mean()\n",
    "    \n",
    "    team_attack = defaultdict(lambda: deque(maxlen=10))\n",
    "    team_defense = defaultdict(lambda: deque(maxlen=10))\n",
    "    \n",
    "    xg_home, xg_away = [], []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        \n",
    "        h_att = list(team_attack[h])\n",
    "        a_def = list(team_defense[a])\n",
    "        \n",
    "        if h_att and a_def:\n",
    "            h_attack_str = np.mean(h_att) / league_avg_home if league_avg_home > 0 else 1\n",
    "            a_defense_str = np.mean(a_def) / league_avg_away if league_avg_away > 0 else 1\n",
    "            xg_h = league_avg_home * h_attack_str * a_defense_str\n",
    "        else:\n",
    "            xg_h = league_avg_home\n",
    "        \n",
    "        a_att = list(team_attack[a])\n",
    "        h_def = list(team_defense[h])\n",
    "        \n",
    "        if a_att and h_def:\n",
    "            a_attack_str = np.mean(a_att) / league_avg_away if league_avg_away > 0 else 1\n",
    "            h_defense_str = np.mean(h_def) / league_avg_home if league_avg_home > 0 else 1\n",
    "            xg_a = league_avg_away * a_attack_str * h_defense_str\n",
    "        else:\n",
    "            xg_a = league_avg_away\n",
    "        \n",
    "        xg_home.append(xg_h)\n",
    "        xg_away.append(xg_a)\n",
    "        \n",
    "        team_attack[h].append(row[\"FTHG\"])\n",
    "        team_defense[h].append(row[\"FTAG\"])\n",
    "        team_attack[a].append(row[\"FTAG\"])\n",
    "        team_defense[a].append(row[\"FTHG\"])\n",
    "    \n",
    "    df[\"xg_home\"] = xg_home\n",
    "    df[\"xg_away\"] = xg_away\n",
    "    df[\"xg_diff\"] = np.array(xg_home) - np.array(xg_away)\n",
    "    df[\"xg_total\"] = np.array(xg_home) + np.array(xg_away)\n",
    "    \n",
    "    season_match_count = defaultdict(int)\n",
    "    match_week = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        season = row[\"Season\"]\n",
    "        season_match_count[season] += 1\n",
    "        week = (season_match_count[season] - 1) // 10 + 1\n",
    "        match_week.append(min(week, 38))\n",
    "    \n",
    "    df[\"match_week\"] = match_week\n",
    "    df[\"is_late_season\"] = (np.array(match_week) >= 30).astype(int)\n",
    "    \n",
    "    print(f\"Added {len([c for c in df.columns if c not in ['Date','HomeTeam','AwayTeam','FTHG','FTAG','FTR','y','Season']])} features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_draw_specific_features(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df = df.sort_values(\"Date\").copy()\n",
    "\n",
    "    if \"form_diff\" in df.columns:\n",
    "        df[\"abs_form_diff\"] = df[\"form_diff\"].abs()\n",
    "    if \"points_diff\" in df.columns:\n",
    "        df[\"abs_points_diff\"] = df[\"points_diff\"].abs()\n",
    "    if \"gd_diff\" in df.columns:\n",
    "        df[\"abs_gd_diff\"] = df[\"gd_diff\"].abs()\n",
    "    if \"elo_sum_diff\" in df.columns:\n",
    "        df[\"abs_elo_sum_diff\"] = df[\"elo_sum_diff\"].abs()\n",
    "    if \"attack_vs_defense_home\" in df.columns and \"attack_vs_defense_away\" in df.columns:\n",
    "        df[\"att_vs_def_diff_abs\"] = (\n",
    "            (df[\"attack_vs_defense_home\"] - df[\"attack_vs_defense_away\"]).abs()\n",
    "        )\n",
    "    if \"position_diff\" in df.columns:\n",
    "        df[\"abs_position_diff\"] = df[\"position_diff\"].abs()\n",
    "    if \"h2h_goal_diff_avg\" in df.columns:\n",
    "        df[\"abs_h2h_gd_avg\"] = df[\"h2h_goal_diff_avg\"].abs()\n",
    "\n",
    " # xG & xG\n",
    "    if \"xg_total\" in df.columns:\n",
    "        df[\"low_xg_flag\"] = (df[\"xg_total\"] <= df[\"xg_total\"].median()).astype(int)\n",
    "\n",
    "    if \"attack_momentum_home\" in df.columns and \"attack_momentum_away\" in df.columns:\n",
    "        df[\"attack_mom_sum\"] = df[\"attack_momentum_home\"] + df[\"attack_momentum_away\"]\n",
    "    if \"defense_momentum_home\" in df.columns and \"defense_momentum_away\" in df.columns:\n",
    "        df[\"defense_mom_sum\"] = df[\"defense_momentum_home\"] + df[\"defense_momentum_away\"]\n",
    "\n",
    "    if \"shots_pm_diff\" in df.columns:\n",
    "        df[\"abs_shots_pm_diff\"] = df[\"shots_pm_diff\"].abs()\n",
    "    if \"sot_pm_diff\" in df.columns:\n",
    "        df[\"abs_sot_pm_diff\"] = df[\"sot_pm_diff\"].abs()\n",
    "    if \"corners_pm_diff\" in df.columns:\n",
    "        df[\"abs_corners_pm_diff\"] = df[\"corners_pm_diff\"].abs()\n",
    "    if \"fouls_pm_diff\" in df.columns:\n",
    "        df[\"abs_fouls_pm_diff\"] = df[\"fouls_pm_diff\"].abs()\n",
    "    if \"yellows_pm_diff\" in df.columns:\n",
    "        df[\"abs_yellows_pm_diff\"] = df[\"yellows_pm_diff\"].abs()\n",
    "\n",
    "    # last N draw rate + home/away draw rate\n",
    "    N = 8\n",
    "    team_draw_overall = defaultdict(lambda: deque(maxlen=N))\n",
    "    team_draw_home    = defaultdict(lambda: deque(maxlen=N))\n",
    "    team_draw_away    = defaultdict(lambda: deque(maxlen=N))\n",
    "\n",
    "    draw_prop_home_list = []\n",
    "    draw_prop_away_list = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        h, a = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "\n",
    "        hist_overall_h = list(team_draw_overall[h])\n",
    "        hist_overall_a = list(team_draw_overall[a])\n",
    "        hist_home_h    = list(team_draw_home[h])\n",
    "        hist_away_a    = list(team_draw_away[a])\n",
    "\n",
    "        # league baseline ~ 0.25\n",
    "        def rate_or_baseline(hist, baseline=0.25):\n",
    "            return (sum(hist) / len(hist)) if hist else baseline\n",
    "\n",
    "        draw_prop_home = (\n",
    "            0.5 * rate_or_baseline(hist_overall_h) +\n",
    "            0.5 * rate_or_baseline(hist_home_h)\n",
    "        )\n",
    "        draw_prop_away = (\n",
    "            0.5 * rate_or_baseline(hist_overall_a) +\n",
    "            0.5 * rate_or_baseline(hist_away_a)\n",
    "        )\n",
    "\n",
    "        draw_prop_home_list.append(draw_prop_home)\n",
    "        draw_prop_away_list.append(draw_prop_away)\n",
    "\n",
    "        # result\n",
    "        is_draw = 1 if row[\"FTR\"] == \"D\" else 0\n",
    "        team_draw_overall[h].append(is_draw)\n",
    "        team_draw_overall[a].append(is_draw)\n",
    "\n",
    "        team_draw_home[h].append(is_draw)\n",
    "        team_draw_away[a].append(is_draw)\n",
    "\n",
    "    df[\"draw_prop_home_v2\"] = draw_prop_home_list\n",
    "    df[\"draw_prop_away_v2\"] = draw_prop_away_list\n",
    "    df[\"draw_prop_sum_v2\"]  = df[\"draw_prop_home_v2\"] + df[\"draw_prop_away_v2\"]\n",
    "    df[\"draw_prop_diff_v2\"] = df[\"draw_prop_home_v2\"] - df[\"draw_prop_away_v2\"]\n",
    "\n",
    "    if \"match_week\" in df.columns:\n",
    "        df[\"early_season\"] = (df[\"match_week\"] <= 5).astype(int)\n",
    "        df[\"mid_season\"]   = ((df[\"match_week\"] >= 6) & (df[\"match_week\"] <= 28)).astype(int)\n",
    "        df[\"late_season\"]  = (df[\"match_week\"] >= 29).astype(int)\n",
    "\n",
    "    if \"position_home\" in df.columns and \"position_away\" in df.columns:\n",
    "        mid_home = ((df[\"position_home\"] >= 7) & (df[\"position_home\"] <= 14)).astype(int)\n",
    "        mid_away = ((df[\"position_away\"] >= 7) & (df[\"position_away\"] <= 14)).astype(int)\n",
    "        df[\"both_mid_table\"] = (mid_home & mid_away).astype(int)\n",
    "\n",
    "    if \"rest_diff\" in df.columns:\n",
    "        df[\"abs_rest_diff\"] = df[\"rest_diff\"].abs()\n",
    "\n",
    "    if \"h2h_draw_rate\" in df.columns and \"h2h_draw_rate_td\" in df.columns:\n",
    "        df[\"h2h_draw_rate_mean\"] = 0.5 * (df[\"h2h_draw_rate\"] + df[\"h2h_draw_rate_td\"])\n",
    "\n",
    "    if \"ref_draw_rate\" in df.columns:\n",
    "        df[\"high_draw_ref_flag\"] = (df[\"ref_draw_rate\"] >= df[\"ref_draw_rate\"].median()).astype(int)\n",
    "\n",
    "    print(\" Added extended draw-specific features\")\n",
    "    return df\n",
    "\n",
    "def compute_all_features(df, fe=None, is_train=True,\n",
    "                         use_state_features=True,\n",
    "                         use_all_adv_block=True,\n",
    "                         use_shot_corners=True,\n",
    "                         use_td_h2h=True,\n",
    "                         use_draw_block=True):\n",
    "    \"\"\"\n",
    "\n",
    "        fe = FeatureEngineering()\n",
    "        cleaned_df = cleaned_df.sort_values(\"Date\").copy()\n",
    "        full_df = compute_all_features(cleaned_df, fe=fe, is_train=True)[0]\n",
    "\n",
    "    return:(df_with_features, fe)\n",
    "    \"\"\"\n",
    "    df = df.sort_values(\"Date\").copy()\n",
    "\n",
    "    if fe is None:\n",
    "        fe = FeatureEngineering()\n",
    "\n",
    "    # based on state ( class)\n",
    "    if use_state_features:\n",
    "        df = fe.compute_features(df, is_train=is_train)\n",
    "\n",
    "    #  (win_streak / xg / match_week etc)\n",
    "    if use_all_adv_block:\n",
    "        df = add_all_advanced_features(df)\n",
    "\n",
    "    # shot / corner / foul / yellow card\n",
    "    if use_shot_corners:\n",
    "        df = add_prematch_shot_form_v2(df)\n",
    "        df = add_prematch_corners_fouls(df)\n",
    "\n",
    "    # time decay H2H\n",
    "    if use_td_h2h:\n",
    "        df = add_advanced_h2h_features(df)\n",
    "\n",
    "    # draw \n",
    "    if use_draw_block:\n",
    "        df = add_draw_specific_features(df)\n",
    "\n",
    "    \n",
    "    # df = add_referee_features(df)\n",
    "\n",
    "    return df, fe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b22d037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering on cleaned_df(rolling by time state)...\n",
      "Added 107 features\n",
      "corner, foul, yellow/red cards history features are added\n",
      "Added advanced H2H features (time-decayed & directional)\n",
      " Added extended draw-specific features\n",
      "invalid seasons:\n",
      "Series([], dtype: int64)\n",
      "recent_seasons: ['2013/2014', '2014/2015', '2015/2016', '2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021']\n",
      "sizes: 3040 380 1140\n",
      "class_weights (draw boosted): {0: 0.7423687423687424, 1: 1.5766148043375767, 2: 1.046831955922865}\n",
      "time_decay_tau_days: 307.6923076923077\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineering()\n",
    "\n",
    "print(\"Feature engineering on cleaned_df(rolling by time state)...\")\n",
    "cleaned_df = cleaned_df.sort_values(\"Date\").copy()\n",
    "cleaned_df, fe = compute_all_features(\n",
    "    cleaned_df,\n",
    "    fe=fe,\n",
    "    is_train=True,           # rolling through entire timeline\n",
    "    use_state_features=True,\n",
    "    use_all_adv_block=True,\n",
    "    use_shot_corners=True,\n",
    "    use_td_h2h=True,\n",
    "    use_draw_block=True,\n",
    ")\n",
    "\n",
    "season_counts = cleaned_df.groupby(\"Season\").size()\n",
    "valid_seasons = season_counts[season_counts == 380].index.tolist()\n",
    "print(\"invalid seasons:\")\n",
    "print(season_counts[season_counts != 380])\n",
    "\n",
    "def season_start_year(s): \n",
    "    return int(str(s).split(\"/\")[0])\n",
    "\n",
    "train_seasons = [f\"{y}/{y+1}\" for y in range(2000, 2021)]\n",
    "val_seasons   = [\"2021/2022\"]\n",
    "test_seasons  = [\"2022/2023\", \"2023/2024\", \"2024/2025\"]\n",
    "\n",
    "train_df = cleaned_df[\n",
    "    cleaned_df[\"Season\"].isin(train_seasons) & \n",
    "    cleaned_df[\"Season\"].isin(valid_seasons)\n",
    "].copy()\n",
    "\n",
    "val_df   = cleaned_df[\n",
    "    cleaned_df[\"Season\"].isin(val_seasons) & \n",
    "    cleaned_df[\"Season\"].isin(valid_seasons)\n",
    "].copy()\n",
    "\n",
    "test_df  = cleaned_df[\n",
    "    cleaned_df[\"Season\"].isin(test_seasons) & \n",
    "    cleaned_df[\"Season\"].isin(valid_seasons)\n",
    "].copy()\n",
    "\n",
    "all_train_seasons = sorted(train_df[\"Season\"].unique(), key=season_start_year)\n",
    "recent_seasons = all_train_seasons[-8:]\n",
    "train_recent = train_df[train_df[\"Season\"].isin(recent_seasons)].copy()\n",
    "\n",
    "print(\"recent_seasons:\", recent_seasons)\n",
    "print(\"sizes:\", len(train_recent), len(val_df), len(test_df))\n",
    "\n",
    "# This section can be kept,for DC / other models\n",
    "DECAY_LAMBDA = 0.00325\n",
    "TIME_DECAY_TAU = 1.0 / DECAY_LAMBDA\n",
    "DRAW_CLASS_MULT = 1.1\n",
    "\n",
    "y_tr = train_recent[\"y\"].astype(int).values\n",
    "base_class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=y_tr\n",
    ")\n",
    "class_weights = {\n",
    "    i: float(w) * (DRAW_CLASS_MULT if i == 1 else 1.0)\n",
    "    for i, w in enumerate(base_class_weights)\n",
    "}\n",
    "\n",
    "def time_decay_weights(df, tau=TIME_DECAY_TAU):\n",
    "    max_date = df[\"Date\"].max()\n",
    "    dt = (max_date - df[\"Date\"]).dt.days.values\n",
    "    return np.exp(-dt / tau)\n",
    "\n",
    "def make_sample_weight(df, tau=TIME_DECAY_TAU):\n",
    "    w_time = time_decay_weights(df, tau=tau)\n",
    "    w_class = np.array([class_weights[int(y)] for y in df[\"y\"].values])\n",
    "    return w_time * w_class\n",
    "\n",
    "print(\"class_weights (draw boosted):\", class_weights)\n",
    "print(\"time_decay_tau_days:\", TIME_DECAY_TAU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1508248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Dataset size verification\n",
      "==================================================\n",
      "train_df: 7980 row\n",
      "val_df: 380 row\n",
      "test_df: 1140 row\n",
      "\n",
      "Date range verification(ensure correct time order):\n",
      "train: 2000-08-19 00:00:00 ~ 2021-05-23 00:00:00\n",
      "val:   2021-08-13 00:00:00 ~ 2022-05-22 00:00:00\n",
      "test:  2022-08-05 00:00:00 ~ 2025-05-25 00:00:00\n",
      "features statistical validation, make sure the distribution make sense\n",
      "form_home            train=  0.450  val=  0.459  test=  0.456\n",
      "elo_diff             train= 75.285  val= 75.693  test= 75.922\n",
      "h2h_draw_rate        train=  0.256  val=  0.232  test=  0.234\n",
      "position_diff        train= -0.196  val= -0.161  test= -0.164\n",
      "NaN features ratio validation\n",
      "train: average NaN ratio = 0.0003\n",
      "val: average NaN ratio = 0.0002\n",
      "test: average NaN ratio = 0.0001\n",
      "feature engineering has been completed. data has been splited correctly\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verify data split correctness\n",
    "print(\"=\" * 50)\n",
    "print(\"Dataset size verification\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"train_df: {len(train_df)} row\")\n",
    "print(f\"val_df: {len(val_df)} row\")\n",
    "print(f\"test_df: {len(test_df)} row\")\n",
    "\n",
    "# Verify no data leakage:check date ranges\n",
    "print(\"\\nDate range verification(ensure correct time order):\")\n",
    "print(f\"train: {train_df['Date'].min()} ~ {train_df['Date'].max()}\")\n",
    "print(f\"val:   {val_df['Date'].min()} ~ {val_df['Date'].max()}\")\n",
    "print(f\"test:  {test_df['Date'].min()} ~ {test_df['Date'].max()}\")\n",
    "\n",
    "# Verify feature statistics(key checkpoints)\n",
    "print(f\"features statistical validation, make sure the distribution make sense\")\n",
    "key_features = ['form_home', 'elo_diff', 'h2h_draw_rate', 'position_diff']\n",
    "for feat in key_features:\n",
    "    if feat in train_df.columns:\n",
    "        train_mean = train_df[feat].mean()\n",
    "        val_mean = val_df[feat].mean()\n",
    "        test_mean = test_df[feat].mean()\n",
    "        print(f\"{feat:20s} train={train_mean:7.3f}  val={val_mean:7.3f}  test={test_mean:7.3f}\")\n",
    "\n",
    "print(f\"NaN features ratio validation\")\n",
    "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    nan_ratio = df[numeric_cols].isna().mean().mean()\n",
    "    print(f\"{name}: average NaN ratio = {nan_ratio:.4f}\")\n",
    "\n",
    "print(f\"feature engineering has been completed. data has been splited correctly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18677bd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ad8a5f9",
   "metadata": {},
   "source": [
    "## 3. Data Transformation & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136017ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current base_df columns: 171\n",
      "check the column number of base_df, len(numeric_cols)\n",
      "total number of column, len(candidate_features)\n",
      " remove 24 leakage features: ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR', 'shots_for', 'shots_against', 'sot_for', 'sot_against', 'corners_for', 'corners_against', 'shot_accuracy', 'opp_shot_accuracy']\n",
      "✅ Features after cleaning: 163 → 139\n",
      "features after removed leak features, len(candidate_features)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if DC model uses only recent seasons,e.g. dc_train_df,can replace base_df with that one;\n",
    "# otherwise,just use train_df as base.\n",
    "base_df = train_df   # or dc_train_df,depending on what you feed to DC \n",
    "\n",
    "print(\"Current base_df columns:\", len(base_df.columns))\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_cols = base_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"check the column number of base_df, len(numeric_cols)\")\n",
    "\n",
    "# y( id/meta column)\n",
    "meta_cols = [\"y\"]  # if match_id,\n",
    "candidate_features = [c for c in numeric_cols if c not in meta_cols]\n",
    "\n",
    "print(f\"total number of column, len(candidate_features)\")\n",
    "\n",
    "# Remove leakage features\n",
    "candidate_features = remove_leak_features(candidate_features)\n",
    "\n",
    "print(f\"features after removed leak features, len(candidate_features)\")\n",
    "\n",
    "# safety check\n",
    "if len(candidate_features) == 0:\n",
    "    print(f\"candidate_features is empty, the first 50th column：\")\n",
    "    print(numeric_cols[:50])\n",
    "    raise ValueError(\n",
    "        \"candidate_features has been deleted as empty by leak features\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80057b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_scores_once_full(\n",
    "    df,\n",
    "    candidate_features,\n",
    "    target_col=\"y\",\n",
    "    methods=('permutation', 'mutual_info', 'rf_importance'),\n",
    "    n_estimators_rf=200,\n",
    "    n_repeats_perm=10,\n",
    "    random_state_base=42,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "      - RandomForest + permutation_importance\n",
    "      - mutual_info_classif\n",
    "      - RandomForest feature_importances_\n",
    "    return:\n",
    "      - combined_scores\n",
    "      - method_scores\n",
    "    \"\"\"\n",
    "    available_features = [f for f in candidate_features if f in df.columns]\n",
    "    if verbose:\n",
    "        print(f\"  number of avaliable features: {len(available_features)}\")\n",
    "\n",
    "    if not available_features:\n",
    "        if verbose:\n",
    "            print(f\"no feature avaliable, return empty\")\n",
    "        return pd.Series(dtype=float), {}\n",
    "\n",
    "    X = df[available_features].copy().fillna(0.0)\n",
    "    y = df[target_col].astype(int)\n",
    "\n",
    "    method_scores = {}\n",
    "\n",
    "    # ---- 1) Permutation Importance ----\n",
    "    if 'permutation' in methods:\n",
    "        if verbose:\n",
    "            print(\"  [1/3] calculate Permutation...\")\n",
    "        rf_perm = RandomForestClassifier(\n",
    "            n_estimators=n_estimators_rf,\n",
    "            random_state=random_state_base,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_perm.fit(X, y)\n",
    "        perm = permutation_importance(\n",
    "            rf_perm, X, y,\n",
    "            n_repeats=n_repeats_perm,\n",
    "            random_state=random_state_base\n",
    "        )\n",
    "        perm_scores = pd.Series(perm.importances_mean, index=X.columns)\n",
    "        method_scores['permutation'] = perm_scores\n",
    "\n",
    "    # ---- 2) Mutual Information ----\n",
    "    if 'mutual_info' in methods:\n",
    "        if verbose:\n",
    "            print(\"  [2/3] calculate Mutual Information...\")\n",
    "        mi_vals = mutual_info_classif(X, y, random_state=random_state_base)\n",
    "        mi_scores = pd.Series(mi_vals, index=X.columns)\n",
    "        method_scores['mutual_info'] = mi_scores\n",
    "\n",
    "    # ---- 3) RandomForest feature_importances_ ----\n",
    "    if 'rf_importance' in methods:\n",
    "        if verbose:\n",
    "            print(\"  [3/3] calculate RandomForest...\")\n",
    "        rf_imp = RandomForestClassifier(\n",
    "            n_estimators=n_estimators_rf,\n",
    "            random_state=random_state_base + 1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_imp.fit(X, y)\n",
    "        rf_scores = pd.Series(rf_imp.feature_importances_, index=X.columns)\n",
    "        method_scores['rf_importance'] = rf_scores\n",
    "\n",
    "    if not method_scores:\n",
    "        if verbose:\n",
    "            print(f\"None of the rating mathoed is applied.\")\n",
    "        return pd.Series(dtype=float), {}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Combine multiple methods' score, find average.\")\n",
    "\n",
    "    # Construct DataFrame: line=Feature, column=method\n",
    "    score_df = pd.DataFrame(method_scores)\n",
    "\n",
    "    # z-score standardization, dominate\n",
    "    score_df_z = (score_df - score_df.mean()) / (score_df.std(ddof=0) + 1e-9)\n",
    "\n",
    "    # compute average as combined score\n",
    "    combined_scores = score_df_z.mean(axis=1)\n",
    "    combined_scores = combined_scores.sort_values(ascending=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"rating process has been complete once, below are top 10 combined features:\")\n",
    "        print(combined_scores.head(10).to_dict())\n",
    "\n",
    "    return combined_scores, method_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6aef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tf_features(df, cols):\n",
    "    good = []\n",
    "    for c in cols:\n",
    "        if not isinstance(c, str) or c not in df.columns:\n",
    "            continue\n",
    "        s = df[c]\n",
    "        if isinstance(s, pd.DataFrame):\n",
    "            if all(pd.api.types.is_numeric_dtype(s[col]) for col in s.columns):\n",
    "                if (s.nunique(axis=1) == 1).all():\n",
    "                    s = s.iloc[:, 0]\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        if not pd.api.types.is_numeric_dtype(s.dtype):\n",
    "            continue\n",
    "        good.append(c)\n",
    "    return good\n",
    "\n",
    "def select_unified_features(df, verbose=True):\n",
    "    \"\"\"\n",
    "    return:(all_features, xgb_features, dc_features, tf_features)\n",
    "    \"\"\"\n",
    "    # leaked features\n",
    "    LEAK_COLS = {\n",
    "        'FTHG', 'FTAG', 'HTHG', 'HTAG', 'FTR', 'HTR',\n",
    "        'HS', 'AS', 'HST', 'AST', 'HC', 'AC', \n",
    "        'HF', 'AF', 'HY', 'AY', 'HR', 'AR',\n",
    "        'shots_for', 'shots_against', 'sot_for', 'sot_against',\n",
    "        'corners_for', 'corners_against', 'shot_accuracy', 'opp_shot_accuracy',\n",
    "    }\n",
    "    \n",
    "    # core features\n",
    "    manual_core = [\n",
    "        # form/form\n",
    "        \"form_home\", \"form_away\", \"form_diff\",\n",
    "        \"form_home_v2\", \"form_away_v2\", \"form_diff_v2\",\n",
    "        \"home_home_form\", \"away_away_form\",\n",
    "        \"win_streak_home\", \"win_streak_away\", \"unbeaten_home\", \"unbeaten_away\",\n",
    "        # L5/L10\n",
    "        \"L5HWR\", \"L5HDR\", \"L5AWR\", \"L5ADR\",\n",
    "        \"L10HWR\", \"L10HDR\", \"L10AWR\", \"L10ADR\",\n",
    "        \"L5_home_adv\", \"L10_home_adv\",\n",
    "        # integration/position\n",
    "        \"points_home\", \"points_away\", \"points_diff\",\n",
    "        \"points_home_v2\", \"points_away_v2\", \"points_diff_v2\",\n",
    "        \"position_home\", \"position_away\", \"position_diff\",\n",
    "        \"position_home_v2\", \"position_away_v2\", \"position_diff_v2\",\n",
    "        \"gd_home\", \"gd_away\", \"gd_diff\", \"season_gd_diff\",\n",
    "        # Elo\n",
    "        \"elo_home\", \"elo_away\", \"elo_diff\",\n",
    "        \"elo_att_home\", \"elo_def_home\", \"elo_att_away\", \"elo_def_away\",\n",
    "        # momentum\n",
    "        \"attack_momentum_home\", \"attack_momentum_away\",\n",
    "        \"defense_momentum_home\", \"defense_momentum_away\",\n",
    "        \"attack_vs_defense_home\", \"attack_vs_defense_away\",\n",
    "        # goal\n",
    "        \"goals_pm_home\", \"goals_pm_away\", \"conceded_pm_home\", \"conceded_pm_away\",\n",
    "        \"goals_scored_pm_home\", \"goals_scored_pm_away\",\n",
    "        \"goals_conceded_pm_home\", \"goals_conceded_pm_away\",\n",
    "        \"gd_pm_home\", \"gd_pm_away\", \"gd_pm_diff\",\n",
    "        # xG\n",
    "        \"xg_home\", \"xg_away\", \"xg_diff\", \"xg_total\",\n",
    "        # H2H\n",
    "        \"h2h_home_rate\", \"h2h_draw_rate\", \"h2h_away_rate\",\n",
    "        \"h2h_home_rate_v2\", \"h2h_draw_rate_v2\", \"h2h_away_rate_v2\",\n",
    "        \"h2h_home_rate_td\", \"h2h_draw_rate_td\", \"h2h_away_rate_td\",\n",
    "        \"h2h_goal_diff_avg\", \"h2h_matches\", \"h2h_matches_v2\", \"h2h_matches_td\",\n",
    "        # rest\n",
    "        \"rest_days_home\", \"rest_days_away\", \"rest_diff\",\n",
    "        \"rest_days_home_v2\", \"rest_days_away_v2\", \"rest_diff_v2\",\n",
    "        # draw\n",
    "        \"draw_prop_home\", \"draw_prop_away\", \"draw_prop_sum\",\n",
    "        \"draw_prop_home_v2\", \"draw_prop_away_v2\", \"draw_prop_sum_v2\", \"draw_prop_diff_v2\",\n",
    "        # \n",
    "        \"abs_form_diff\", \"abs_points_diff\", \"abs_gd_diff\", \"abs_position_diff\", \"abs_h2h_gd_avg\",\n",
    "        # pm\n",
    "        \"shots_for_pm_home\", \"shots_for_pm_away\", \"sot_for_pm_home\", \"sot_for_pm_away\",\n",
    "        \"shots_against_pm_home\", \"shots_against_pm_away\", \"sot_against_pm_home\", \"sot_against_pm_away\",\n",
    "        \"shots_pm_diff\", \"sot_pm_diff\",\n",
    "        # corners/fouls/pm\n",
    "        \"corners_for_pm_home\", \"corners_for_pm_away\", \"corners_pm_diff\",\n",
    "        \"fouls_pm_home\", \"fouls_pm_away\", \"fouls_pm_diff\",\n",
    "        \"yellows_pm_home\", \"yellows_pm_away\", \"yellows_pm_diff\",\n",
    "        # referee\n",
    "        \"ref_home_rate\", \"ref_draw_rate\", \"ref_away_rate\",\n",
    "        \"ref_home_rate_v2\", \"ref_draw_rate_v2\", \"ref_away_rate_v2\",\n",
    "        \"ref_matches\", \"ref_matches_v2\", \"ref_home_bias\", \"ref_home_bias_v2\",\n",
    "        \"ref_avg_yellow\", \"ref_avg_red\", \"ref_avg_fouls\",\n",
    "        # \n",
    "        \"match_week\", \"is_late_season\", \"early_season\", \"mid_season\", \"late_season\", \"both_mid_table\",\n",
    "    ]\n",
    "    \n",
    "    manual_core = [c for c in manual_core if c in df.columns]\n",
    "    \n",
    "    # \n",
    "    extra_cols = []\n",
    "    meta_cols = {\"date\", \"season\", \"hometeam\", \"awayteam\", \"ftr\", \"y\", \"referee\"}\n",
    "    for c in df.columns:\n",
    "        name = str(c).lower()\n",
    "        if c in manual_core or name in meta_cols or c in LEAK_COLS:\n",
    "            continue\n",
    "        if any(kw in name for kw in [\"momentum\", \"adv\", \"streak\", \"prop\"]):\n",
    "            extra_cols.append(c)\n",
    "    \n",
    "    candidate = list(dict.fromkeys(manual_core + extra_cols))\n",
    "    candidate = [c for c in candidate if c not in LEAK_COLS]\n",
    "    all_features = clean_tf_features(df, candidate)\n",
    "    \n",
    "    # \n",
    "    xgb_features = all_features.copy()\n",
    "    \n",
    "    dc_priority = [\n",
    "        \"elo_diff\", \"form_diff\", \"form_diff_v2\", \"points_diff\", \"position_diff\", \"gd_diff\",\n",
    "        \"h2h_home_rate\", \"h2h_draw_rate\", \"rest_diff\", \"xg_diff\", \"L5_home_adv\", \"draw_prop_sum_v2\",\n",
    "    ]\n",
    "    dc_features = [f for f in dc_priority if f in all_features]\n",
    "    for f in all_features:\n",
    "        if f not in dc_features and len(dc_features) < 12:\n",
    "            dc_features.append(f)\n",
    "    dc_features = dc_features[:12]\n",
    "    \n",
    "    tf_features = all_features.copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"total features are choosed: all = {len(all_features)}, XGB={len(xgb_features)}, DC={len(dc_features)}, TF={len(tf_features)}\")\n",
    "        print(f\"DC features: {dc_features}\")\n",
    "    \n",
    "    return all_features, xgb_features, dc_features, tf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbccfef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have load features from unified_features.pkl\n",
      "\n",
      "final number of features: XGB=120, DC=12, TF=120\n",
      "DC features: ['elo_diff', 'form_diff', 'form_diff_v2', 'points_diff', 'position_diff', 'gd_diff', 'h2h_home_rate', 'h2h_draw_rate', 'rest_diff', 'xg_diff', 'draw_prop_sum_v2', 'form_home']\n"
     ]
    }
   ],
   "source": [
    "RESELECT_FEATURES = False  #  True \n",
    "\n",
    "if (not RESELECT_FEATURES) and os.path.exists(\"unified_features.pkl\"):\n",
    "    # \n",
    "    with open(\"unified_features.pkl\", \"rb\") as f:\n",
    "        saved = pickle.load(f)\n",
    "    feature_cols_xgb = saved[\"feature_cols_xgb\"]\n",
    "    dc_feature_cols = saved[\"dc_feature_cols\"]\n",
    "    tf_token_features = saved[\"tf_token_features\"]\n",
    "    \n",
    "    LEAK_CHECK = {'FTHG','FTAG','HS','AS','HST','AST','HC','AC','HF','AF','HY','AY','HR','AR',\n",
    "                  'shots_for','shots_against','sot_for','sot_against','corners_for','corners_against'}\n",
    "    feature_cols_xgb = [f for f in feature_cols_xgb if f not in LEAK_CHECK]\n",
    "    dc_feature_cols = [f for f in dc_feature_cols if f not in LEAK_CHECK]\n",
    "    tf_token_features = [f for f in tf_token_features if f not in LEAK_CHECK]\n",
    "    \n",
    "    print(f\"we have load features from unified_features.pkl\")\n",
    "\n",
    "else:\n",
    " # ( train_df)\n",
    "    _, feature_cols_xgb, dc_feature_cols, tf_token_features = select_unified_features(train_df, verbose=True)\n",
    "    \n",
    "    # Save\n",
    "    with open(\"unified_features.json\", \"w\") as f:\n",
    "        json.dump({\"xgb\": feature_cols_xgb, \"dc\": dc_feature_cols, \"tf\": tf_token_features}, f, indent=2)\n",
    "    with open(\"unified_features.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"feature_cols_xgb\": feature_cols_xgb, \"dc_feature_cols\": dc_feature_cols, \n",
    "                     \"tf_token_features\": tf_token_features}, f)\n",
    "    print(\"✅ unified_features.pkl\")\n",
    "\n",
    "print(f\"\\nfinal number of features: XGB={len(feature_cols_xgb)}, DC={len(dc_feature_cols)}, TF={len(tf_token_features)}\")\n",
    "print(f\"DC features: {dc_feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318427c2",
   "metadata": {},
   "source": [
    "## 4. Methodology Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a98a9b0",
   "metadata": {},
   "source": [
    "## 5. Model Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4b52f",
   "metadata": {},
   "source": [
    "### Bayesian Dixon–Coles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f5f818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of DC features matrix: (3040, 12) train: (7980, 12) val: (380, 12) test: (1140, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# DC ( train_recent DC)\n",
    "dc_train_df = train_recent.copy()\n",
    "\n",
    "# scaler( dc_train_df)\n",
    "dc_scaler = StandardScaler()\n",
    "X_dc_train = dc_scaler.fit_transform(\n",
    "    dc_train_df[dc_feature_cols].copy().fillna(0.0)\n",
    ")\n",
    "\n",
    "# val/test ( scaler)\n",
    "X_dc_train_full = dc_scaler.transform(\n",
    "    train_df[dc_feature_cols].copy().fillna(0.0)\n",
    ")\n",
    "X_dc_val = dc_scaler.transform(\n",
    "    val_df[dc_feature_cols].copy().fillna(0.0)\n",
    ")\n",
    "X_dc_test = dc_scaler.transform(\n",
    "    test_df[dc_feature_cols].copy().fillna(0.0)\n",
    ")\n",
    "\n",
    "print(\"shape of DC features matrix:\" , X_dc_train.shape,\n",
    "      \"train:\", X_dc_train_full.shape,\n",
    "      \"val:\", X_dc_val.shape,\n",
    "      \"test:\", X_dc_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b3ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw, beta_h, beta_a]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57359b48ca5f487ea2b4f3acea576a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 139 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 31 n_features_dc: 12\n",
      "Bayes DC posterior checkpoint has been save as bayes_dc_global.ckpt\n"
     ]
    }
   ],
   "source": [
    "dc_train_df = train_recent.copy()\n",
    "\n",
    "teams = sorted(set(dc_train_df[\"HomeTeam\"]).union(dc_train_df[\"AwayTeam\"]))\n",
    "tmap = {t: i for i, t in enumerate(teams)}\n",
    "n_teams = len(teams)\n",
    "\n",
    "home_idx = dc_train_df[\"HomeTeam\"].map(tmap).values.astype(\"int64\")\n",
    "away_idx = dc_train_df[\"AwayTeam\"].map(tmap).values.astype(\"int64\")\n",
    "gh = dc_train_df[\"FTHG\"].astype(int).values\n",
    "ga = dc_train_df[\"FTAG\"].astype(int).values\n",
    "\n",
    "xi = DECAY_LAMBDA\n",
    "last_date = dc_train_df[\"Date\"].max()\n",
    "delta_days = (last_date - dc_train_df[\"Date\"]).dt.days.values\n",
    "w = np.exp(-xi * delta_days)\n",
    "\n",
    "X_dc_train = dc_scaler.transform(\n",
    "    dc_train_df[dc_feature_cols].copy().fillna(0.0)\n",
    ")\n",
    "n_features_dc = X_dc_train.shape[1]\n",
    "\n",
    "X_dc_shared = at.as_tensor_variable(X_dc_train, dtype=\"float64\")\n",
    "hi = at.as_tensor_variable(home_idx, dtype=\"int64\")\n",
    "ai = at.as_tensor_variable(away_idx, dtype=\"int64\")\n",
    "gH = at.as_tensor_variable(gh, dtype=\"int64\")\n",
    "gA = at.as_tensor_variable(ga, dtype=\"int64\")\n",
    "weights = at.as_tensor_variable(w, dtype=\"float64\")\n",
    "\n",
    "with pm.Model() as dc_model:\n",
    "    sigma_att = pm.HalfNormal(\"sigma_att\", sigma=0.7)\n",
    "    sigma_def = pm.HalfNormal(\"sigma_def\", sigma=0.7)\n",
    "\n",
    "    att_offset = pm.Normal(\"att_offset\", 0.0, 1.0, shape=n_teams)\n",
    "    def_offset = pm.Normal(\"def_offset\", 0.0, 1.0, shape=n_teams)\n",
    "\n",
    "    attack_raw  = att_offset * sigma_att\n",
    "    defense_raw = def_offset * sigma_def\n",
    "    attack  = pm.Deterministic(\"attack\",  attack_raw  - attack_raw[-1])\n",
    "    defense = pm.Deterministic(\"defense\", defense_raw - defense_raw[-1])\n",
    "\n",
    "    home_adv = pm.Normal(\"home_adv\", 0.0, 0.5)\n",
    "    rho_raw  = pm.Normal(\"rho_raw\", 0.0, 0.7)\n",
    "    rho = pm.Deterministic(\"rho\", 0.6 * pm.math.tanh(rho_raw))\n",
    "\n",
    "    beta_h = pm.Normal(\"beta_h\", 0.0, 0.3, shape=n_features_dc)\n",
    "    beta_a = pm.Normal(\"beta_a\", 0.0, 0.3, shape=n_features_dc)\n",
    "\n",
    "    lin_h = home_adv + attack[hi] - defense[ai] + at.dot(X_dc_shared, beta_h)\n",
    "    lin_a = attack[ai] - defense[hi] + at.dot(X_dc_shared, beta_a)\n",
    "\n",
    "    lam_h = pm.math.exp(lin_h)\n",
    "    lam_a = pm.math.exp(lin_a)\n",
    "\n",
    "    corr = at.ones_like(gH, dtype=\"float64\")\n",
    "\n",
    "    m00 = at.and_(at.eq(gH, 0), at.eq(gA, 0))\n",
    "    m01 = at.and_(at.eq(gH, 0), at.eq(gA, 1))\n",
    "    m10 = at.and_(at.eq(gH, 1), at.eq(gA, 0))\n",
    "    m11 = at.and_(at.eq(gH, 1), at.eq(gA, 1))\n",
    "\n",
    "    corr = at.switch(m00, 1 - lam_h * lam_a * rho, corr)\n",
    "    corr = at.switch(m01, 1 + lam_h * rho, corr)\n",
    "    corr = at.switch(m10, 1 + lam_a * rho, corr)\n",
    "    corr = at.switch(m11, 1 - rho, corr)\n",
    "    corr = at.clip(corr, 1e-6, np.inf)\n",
    "\n",
    "    logp_home = pm.logp(pm.Poisson.dist(mu=lam_h), gH)\n",
    "    logp_away = pm.logp(pm.Poisson.dist(mu=lam_a), gA)\n",
    "    logp_corr = at.log(corr)\n",
    "\n",
    "    pm.Potential(\"weighted_like\",\n",
    "                 at.sum(weights * (logp_home + logp_away + logp_corr)))\n",
    "\n",
    "    # sampling\n",
    "    trace_dc = pm.sample(\n",
    "        draws=1000,\n",
    "        tune=500,\n",
    "        chains=4,\n",
    "        cores=2,             \n",
    "        target_accept=0.95,    # Less conservative → faster\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "post = trace_dc.posterior.stack(sample=(\"chain\", \"draw\"))\n",
    "\n",
    "attack_s   = post[\"attack\"].values               # (n_samples, n_teams)\n",
    "defense_s  = post[\"defense\"].values             # (n_samples, n_teams)\n",
    "home_adv_s = post[\"home_adv\"].values            # (n_samples,)\n",
    "rho_s      = post[\"rho\"].values                 # (n_samples,)\n",
    "beta_h_s   = post[\"beta_h\"].transpose(\"sample\", \"beta_h_dim_0\").values\n",
    "beta_a_s   = post[\"beta_a\"].transpose(\"sample\", \"beta_a_dim_0\").values\n",
    "\n",
    "n_samples = attack_s.shape[0]\n",
    "print(\"n_samples:\", n_samples, \"n_features_dc:\", n_features_dc)\n",
    "\n",
    "import pickle\n",
    "\n",
    "dc_bayes_ckpt = {\n",
    "    \"tmap\": tmap,\n",
    "    \"attack_s\": attack_s,\n",
    "    \"defense_s\": defense_s,\n",
    "    \"home_adv_s\": home_adv_s,\n",
    "    \"rho_s\": rho_s,\n",
    "    \"beta_h_s\": beta_h_s,\n",
    "    \"beta_a_s\": beta_a_s,\n",
    "    \"dc_feature_cols\": dc_feature_cols,\n",
    "    \"DECAY_LAMBDA\": DECAY_LAMBDA,\n",
    "    \"dc_scaler\": dc_scaler,\n",
    "}\n",
    "\n",
    "with open(\"bayes_dc_global.ckpt\", \"wb\") as f:\n",
    "    pickle.dump(dc_bayes_ckpt, f)\n",
    "\n",
    "print(f\"Bayes DC posterior checkpoint has been save as bayes_dc_global.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5801bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, factorial\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def poisson_pmf(lam, k):\n",
    "    return exp(-lam) * lam**k / factorial(k)\n",
    "\n",
    "def dc_joint_matrix(lam_h, lam_a, rho, max_goals=5):\n",
    "    goals = np.arange(max_goals + 1)\n",
    "    p_h = np.array([poisson_pmf(lam_h, g) for g in goals])\n",
    "    p_a = np.array([poisson_pmf(lam_a, g) for g in goals])\n",
    "    M = np.outer(p_h, p_a)\n",
    "    if abs(rho) > 1e-9:\n",
    "        M = M.copy()\n",
    "        M[0,0] *= max(1e-12, 1 - lam_h*lam_a*rho)\n",
    "        if max_goals >= 1:\n",
    "            M[0,1] *= max(1e-12, 1 + lam_h*rho)\n",
    "            M[1,0] *= max(1e-12, 1 + lam_a*rho)\n",
    "            M[1,1] *= max(1e-12, 1 - rho)\n",
    "    return M / M.sum()\n",
    "\n",
    "def bayes_dc_predict_full(df_subset,\n",
    "                          X_dc_subset,\n",
    "                          attack_s, defense_s,\n",
    "                          home_adv_s, rho_s,\n",
    "                          beta_h_s, beta_a_s,\n",
    "                          tmap,\n",
    "                          max_goals=5):\n",
    "    \n",
    "    n_matches = len(df_subset)\n",
    "    n_samples = attack_s.shape[0]\n",
    "    n_features_dc = X_dc_subset.shape[1]\n",
    "\n",
    "    out_probs = np.zeros((n_matches, 3))\n",
    "    out_scorelines = []\n",
    "\n",
    "    for i, (_, row) in enumerate(df_subset.iterrows()):\n",
    "        home, away = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "        ih = tmap.get(home)\n",
    "        ia = tmap.get(away)\n",
    "\n",
    "        # \n",
    "        if ih is None or ia is None:\n",
    "            out_probs[i] = [1/3, 1/3, 1/3]\n",
    "            out_scorelines.append(\n",
    "                np.full((max_goals+1, max_goals+1),\n",
    "                        1/((max_goals+1)**2))\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        x_vec = X_dc_subset[i]  # shape: (n_features_dc,)\n",
    "\n",
    "        M_avg = np.zeros((max_goals+1, max_goals+1))\n",
    "\n",
    "        for k in range(n_samples):\n",
    "            base_h = home_adv_s[k] + attack_s[k, ih] - defense_s[k, ia]\n",
    "            base_a = attack_s[k, ia] - defense_s[k, ih]\n",
    "\n",
    "            adj_h = np.dot(beta_h_s[k], x_vec)\n",
    "            adj_a = np.dot(beta_a_s[k], x_vec)\n",
    "\n",
    "            lam_h_k = np.exp(base_h + adj_h)\n",
    "            lam_a_k = np.exp(base_a + adj_a)\n",
    "            rho_k = rho_s[k]\n",
    "\n",
    "            M_avg += dc_joint_matrix(lam_h_k, lam_a_k, rho_k, max_goals=max_goals)\n",
    "\n",
    "        M_avg /= n_samples\n",
    "        out_scorelines.append(M_avg)\n",
    "\n",
    "        p_home = np.tril(M_avg, k=-1).sum()   # gH > gA\n",
    "        p_away = np.triu(M_avg, k=1).sum()    # gH < gA\n",
    "        p_draw = np.trace(M_avg)\n",
    "\n",
    "        tot = p_home + p_draw + p_away\n",
    "        out_probs[i] = [p_home/tot, p_draw/tot, p_away/tot]  #  = [H, D, A]\n",
    "\n",
    "    return out_probs, out_scorelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3984f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayes DC val logloss: 0.9729\n",
      "Bayes DC test logloss: 1.001\n"
     ]
    }
   ],
   "source": [
    "# X_dc_*( scaler standardization)\n",
    "\n",
    "beta_h_s   = post[\"beta_h\"].transpose(\"sample\", \"beta_h_dim_0\").values\n",
    "beta_a_s   = post[\"beta_a\"].transpose(\"sample\", \"beta_a_dim_0\").values\n",
    "\n",
    "proba_train_dc, scorelines_train = bayes_dc_predict_full(\n",
    "    train_df,\n",
    "    X_dc_train_full,\n",
    "    attack_s, defense_s,\n",
    "    home_adv_s, rho_s,\n",
    "    beta_h_s, beta_a_s,\n",
    "    tmap\n",
    ")\n",
    "proba_val_dc, scorelines_val = bayes_dc_predict_full(\n",
    "    val_df,\n",
    "    X_dc_val,\n",
    "    attack_s, defense_s,\n",
    "    home_adv_s, rho_s,\n",
    "    beta_h_s, beta_a_s,\n",
    "    tmap\n",
    ")\n",
    "proba_test_dc, scorelines_test = bayes_dc_predict_full(\n",
    "    test_df,\n",
    "    X_dc_test,\n",
    "    attack_s, defense_s,\n",
    "    home_adv_s, rho_s,\n",
    "    beta_h_s, beta_a_s,\n",
    "    tmap\n",
    ")\n",
    "\n",
    "print(\"Bayes DC val logloss:\",\n",
    "      round(log_loss(val_df[\"y\"], proba_val_dc, labels=[0,1,2]), 4))\n",
    "print(\"Bayes DC test logloss:\",\n",
    "      round(log_loss(test_df[\"y\"], proba_test_dc, labels=[0,1,2]), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690de5a7",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "494f5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features after cleaning: 120 → 120\n",
      "XGB feature count: 120\n",
      "features has been load from unified_features.pkl\n",
      "number of XGB features 120\n",
      "number of DC features 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "if (not RESELECT_FEATURES) and os.path.exists(\"unified_features.pkl\"):\n",
    "    with open(\"unified_features.pkl\", \"rb\") as f:\n",
    "        saved = pickle.load(f)\n",
    "\n",
    "    feature_cols_xgb = saved[\"feature_cols_xgb\"]\n",
    "    dc_feature_cols  = saved.get(\"dc_feature_cols\", [])\n",
    "    stability_score_dict = saved.get(\"stability_score\", {})\n",
    "    feature_cols_xgb = remove_leak_features(feature_cols_xgb)\n",
    "    \n",
    "    print(f\"XGB feature count: {len(feature_cols_xgb)}\")\n",
    "    print(f\"features has been load from unified_features.pkl\")\n",
    "    print(f\"number of XGB features\", len(feature_cols_xgb))\n",
    "    print(f\"number of DC features\", len(dc_feature_cols))\n",
    "\n",
    "else:\n",
    "    # \n",
    "    feature_cols_xgb = list(selected_features_60)\n",
    "    feature_cols_xgb = [c for c in feature_cols_xgb if c in train_df.columns]\n",
    "    \n",
    "    # Remove leakage features\n",
    "    feature_cols_xgb = remove_leak_features(feature_cols_xgb)\n",
    "    \n",
    "    print(f\"XGB feature count: {len(feature_cols_xgb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6bcde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"train_df\", \"val_df\", \"test_df\"]:\n",
    "    df = locals()[name]\n",
    "    df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "    locals()[name] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fb0bf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB features (unique): 120\n"
     ]
    }
   ],
   "source": [
    "feature_cols_xgb = list(dict.fromkeys(feature_cols_xgb))\n",
    "print(\"XGB features (unique):\", len(feature_cols_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f9908dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_means = train_df[feature_cols_xgb].mean()\n",
    "\n",
    "for dfx in [train_df, val_df, test_df]:\n",
    " # fillna, train_means\n",
    "    dfx[feature_cols_xgb] = dfx[feature_cols_xgb].fillna(train_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce13461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeated feature name []\n",
      "train_means index duplicated?: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"repeated feature name\", [c for c in feature_cols_xgb if feature_cols_xgb.count(c) > 1])\n",
    "train_means = train_df[feature_cols_xgb].mean()\n",
    "print(\"train_means index duplicated?:\", train_means.index.duplicated().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61bf89c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-15 15:13:25,568] A new study created in memory with name: no-name-2adeb226-3d84-4b89-83a7-8efb7e50ab55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB feature count: 120\n",
      "All train seasons: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021']\n",
      "CV fold seasons (target seasons): ['2015/2016', '2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021']\n",
      "XGB device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-15 15:13:36,955] Trial 0 finished with value: 1.0182854349688006 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 0.8395497738645505, 'reg_lambda': 2.223055163432705, 'gamma': 0.040125418880110675, 'subsample': 0.721755258343871, 'colsample_bytree': 0.8818579903579938}. Best is trial 0 with value: 1.0182854349688006.\n",
      "[I 2025-12-15 15:13:45,327] Trial 1 finished with value: 1.020187063563738 and parameters: {'max_depth': 3, 'min_child_weight': 5, 'reg_alpha': 1.5778157199926826, 'reg_lambda': 2.5188666949365617, 'gamma': 0.2070593747444298, 'subsample': 0.7837454461059394, 'colsample_bytree': 0.7116894452295639}. Best is trial 0 with value: 1.0182854349688006.\n",
      "[I 2025-12-15 15:13:54,331] Trial 2 finished with value: 1.0221782721126897 and parameters: {'max_depth': 4, 'min_child_weight': 5, 'reg_alpha': 0.957883410569316, 'reg_lambda': 2.47039659382137, 'gamma': 0.40739884221360456, 'subsample': 0.8657310377457114, 'colsample_bytree': 0.8844913955239555}. Best is trial 0 with value: 1.0182854349688006.\n",
      "[I 2025-12-15 15:14:05,441] Trial 3 finished with value: 1.0198352982596122 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 1.082038862459887, 'reg_lambda': 2.810589645747937, 'gamma': 0.31581626216125774, 'subsample': 0.8512029818158248, 'colsample_bytree': 0.7337703301768623}. Best is trial 0 with value: 1.0182854349688006.\n",
      "[I 2025-12-15 15:14:14,394] Trial 4 finished with value: 1.016986437074465 and parameters: {'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 0.9171164596785282, 'reg_lambda': 3.9593101843582716, 'gamma': 0.2537808233885571, 'subsample': 0.7349785646680412, 'colsample_bytree': 0.7784179577242174}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:14:24,271] Trial 5 finished with value: 1.0195297615765642 and parameters: {'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 1.6791170736756031, 'reg_lambda': 2.655677496828308, 'gamma': 0.06458904271633048, 'subsample': 0.8215150277497292, 'colsample_bytree': 0.7054848051298035}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:14:33,588] Trial 6 finished with value: 1.0216297375540084 and parameters: {'max_depth': 4, 'min_child_weight': 5, 'reg_alpha': 1.919790388072175, 'reg_lambda': 2.9888711887028006, 'gamma': 0.1963530505474323, 'subsample': 0.8735179186022051, 'colsample_bytree': 0.716684604872765}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:14:42,888] Trial 7 finished with value: 1.018271032224016 and parameters: {'max_depth': 3, 'min_child_weight': 5, 'reg_alpha': 1.5531044586352944, 'reg_lambda': 2.445837376767956, 'gamma': 0.3973292660743456, 'subsample': 0.7579549285088469, 'colsample_bytree': 0.876541262906579}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:14:50,548] Trial 8 finished with value: 1.0189614210959272 and parameters: {'max_depth': 3, 'min_child_weight': 7, 'reg_alpha': 1.7544754565183904, 'reg_lambda': 2.242728310948194, 'gamma': 0.47649666720895145, 'subsample': 0.767509492834652, 'colsample_bytree': 0.7177863848245983}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:14:58,801] Trial 9 finished with value: 1.0196405456115565 and parameters: {'max_depth': 3, 'min_child_weight': 7, 'reg_alpha': 1.489009047445414, 'reg_lambda': 2.2240037001138253, 'gamma': 0.3028675870593441, 'subsample': 0.829030594954516, 'colsample_bytree': 0.7034069531208799}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:15:08,925] Trial 10 finished with value: 1.0187490710771951 and parameters: {'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 1.2428971265599977, 'reg_lambda': 3.8992557427521826, 'gamma': 0.13049344328107632, 'subsample': 0.7056899318317009, 'colsample_bytree': 0.7832119201050283}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:15:19,177] Trial 11 finished with value: 1.018434069749671 and parameters: {'max_depth': 3, 'min_child_weight': 6, 'reg_alpha': 1.3252628532061286, 'reg_lambda': 3.5236851590020217, 'gamma': 0.36404181893134463, 'subsample': 0.7443375207641596, 'colsample_bytree': 0.8323738178019457}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:15:27,506] Trial 12 finished with value: 1.0182290508045497 and parameters: {'max_depth': 3, 'min_child_weight': 5, 'reg_alpha': 1.1968332511855415, 'reg_lambda': 3.3349925810891277, 'gamma': 0.4874170623208779, 'subsample': 0.7503355974055729, 'colsample_bytree': 0.7970656605487322}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:15:35,468] Trial 13 finished with value: 1.017961640861501 and parameters: {'max_depth': 3, 'min_child_weight': 6, 'reg_alpha': 1.1273736309000038, 'reg_lambda': 3.458309436355421, 'gamma': 0.47112241150957185, 'subsample': 0.7321216729852894, 'colsample_bytree': 0.7767893669259218}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:15:46,743] Trial 14 finished with value: 1.0177858341749761 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 1.0767876759448822, 'reg_lambda': 3.9971684078038106, 'gamma': 0.250714308929803, 'subsample': 0.7285680201020028, 'colsample_bytree': 0.7618829964120472}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:15:55,632] Trial 15 finished with value: 1.0187310985982723 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 0.8173681389437799, 'reg_lambda': 3.938102618825531, 'gamma': 0.24935656839403406, 'subsample': 0.7961128871716002, 'colsample_bytree': 0.7601432180593356}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:16:05,133] Trial 16 finished with value: 1.0192053115669897 and parameters: {'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 0.9907038016133999, 'reg_lambda': 3.6963488979480643, 'gamma': 0.15361440430486129, 'subsample': 0.7053188934155786, 'colsample_bytree': 0.8285709054374054}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:16:14,319] Trial 17 finished with value: 1.0208184873197073 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 0.989377436444162, 'reg_lambda': 3.224053530269475, 'gamma': 0.27036883643090104, 'subsample': 0.8981962015319583, 'colsample_bytree': 0.7481371302381475}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:16:25,417] Trial 18 finished with value: 1.0186624277202743 and parameters: {'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 1.283390910229783, 'reg_lambda': 3.7204478619383865, 'gamma': 0.10659659733062027, 'subsample': 0.78304662753748, 'colsample_bytree': 0.8221705898879879}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:16:35,133] Trial 19 finished with value: 1.0178847553328414 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 0.8970674631979654, 'reg_lambda': 3.995860820375772, 'gamma': 0.21352236381056877, 'subsample': 0.7265401290963001, 'colsample_bytree': 0.7627241501691852}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:16:44,408] Trial 20 finished with value: 1.0184457275444403 and parameters: {'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 1.079491840088494, 'reg_lambda': 3.7072835535741393, 'gamma': 0.34259942909830365, 'subsample': 0.773514759439321, 'colsample_bytree': 0.8514133333131352}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:16:55,562] Trial 21 finished with value: 1.0183650935688853 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 0.8867079854970729, 'reg_lambda': 3.9312472378229417, 'gamma': 0.19749079733464117, 'subsample': 0.7323722372985652, 'colsample_bytree': 0.7594928254513365}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:17:05,826] Trial 22 finished with value: 1.0205551644053132 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 0.9118550669816486, 'reg_lambda': 3.9753828449230917, 'gamma': 0.2450860946277969, 'subsample': 0.719332730117124, 'colsample_bytree': 0.7794690877578925}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:17:15,630] Trial 23 finished with value: 1.0183546470227502 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 0.8002659975301863, 'reg_lambda': 3.7441029670486405, 'gamma': 0.28079203126847396, 'subsample': 0.7007675421390616, 'colsample_bytree': 0.8016088230786564}. Best is trial 4 with value: 1.016986437074465.\n",
      "[I 2025-12-15 15:17:24,508] Trial 24 finished with value: 1.0188532261408436 and parameters: {'max_depth': 4, 'min_child_weight': 6, 'reg_alpha': 1.0364309328893833, 'reg_lambda': 3.590416364362892, 'gamma': 0.16789532524785566, 'subsample': 0.7410568287064658, 'colsample_bytree': 0.7367511496205814}. Best is trial 4 with value: 1.016986437074465.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: 4\n",
      "Best value (cv mean logloss): 1.016986437074465\n",
      "Best params: {'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 0.9171164596785282, 'reg_lambda': 3.9593101843582716, 'gamma': 0.2537808233885571, 'subsample': 0.7349785646680412, 'colsample_bytree': 0.7784179577242174}\n",
      "Median best_iter of best trial: 62.5\n",
      "\n",
      "== Optuna result ==\n",
      "best_overall_params: {'max_depth': 4, 'min_child_weight': 7, 'reg_alpha': 0.9171164596785282, 'reg_lambda': 3.9593101843582716, 'gamma': 0.2537808233885571, 'subsample': 0.7349785646680412, 'colsample_bytree': 0.7784179577242174}\n",
      "best_overall (cv mean logloss): 1.016986437074465\n",
      "best_overall_iters: 62\n",
      "==> best_n = 67\n"
     ]
    }
   ],
   "source": [
    "feature_cols_xgb = [c for c in feature_cols_xgb if c in train_df.columns]\n",
    "print(f\"XGB feature count: {len(feature_cols_xgb)}\")\n",
    "\n",
    "# filter for features not in train_df (prevent column name mismatch)\n",
    "train_means = train_df[feature_cols_xgb].mean()\n",
    "for dfx in [train_df, val_df, test_df]:\n",
    "    for col in feature_cols_xgb:\n",
    "        dfx[col] = dfx[col].fillna(train_means[col])\n",
    "\n",
    "all_train_seasons = sorted(train_df[\"Season\"].unique(), key=season_start_year)\n",
    "recent_train_seasons = all_train_seasons[-10:]      #  10 \n",
    "fold_seasons = recent_train_seasons[-6:]            #  6 \"CV\"\n",
    "\n",
    "print(\"All train seasons:\", all_train_seasons)\n",
    "print(\"CV fold seasons (target seasons):\", fold_seasons)\n",
    "\n",
    "def make_time_class_weight(df, tau=TIME_DECAY_TAU, draw_boost=DRAW_CLASS_MULT):\n",
    "    y = df[\"y\"].astype(int).values\n",
    "\n",
    "    base_w = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.array([0, 1, 2]),\n",
    "        y=y\n",
    "    )\n",
    "    class_w = {\n",
    "        i: float(w) * (draw_boost if i == 1 else 1.0)\n",
    "        for i, w in enumerate(base_w)\n",
    "    }\n",
    "\n",
    "    max_date = df[\"Date\"].max()\n",
    "    dt = (max_date - df[\"Date\"]).dt.days.values\n",
    "    w_time = np.exp(-dt / tau)\n",
    "    w_class = np.array([class_w[int(v)] for v in y])\n",
    "\n",
    "    return w_time * w_class\n",
    "\n",
    "#  GPU\n",
    "try:\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "except Exception:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"XGB device:\", device)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 4)\n",
    "    min_child_weight = trial.suggest_int(\"min_child_weight\", 5, 7)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0.8, 2.0)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 2.0, 4.0)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.0, 0.5)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.7, 0.9)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.7, 0.9)\n",
    "\n",
    "    fold_losses = []\n",
    "    fold_best_iters = []\n",
    "\n",
    "    for fs in fold_seasons:\n",
    "        fs_idx = all_train_seasons.index(fs)\n",
    "\n",
    "        hist_seasons = all_train_seasons[:fs_idx][-8:]\n",
    "        hist_df = train_df[train_df[\"Season\"].isin(hist_seasons)].copy()\n",
    "        fold_df = train_df[train_df[\"Season\"] == fs].copy()\n",
    "\n",
    "        if len(hist_df) == 0 or len(fold_df) == 0:\n",
    "            continue\n",
    "\n",
    "        w_hist = make_time_class_weight(\n",
    "            hist_df,\n",
    "            tau=TIME_DECAY_TAU,\n",
    "            draw_boost=DRAW_CLASS_MULT\n",
    "        )\n",
    "\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=max_depth,\n",
    "            min_child_weight=min_child_weight,\n",
    "            gamma=gamma,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            objective=\"multi:softprob\",\n",
    "            num_class=3,\n",
    "            eval_metric=\"mlogloss\",\n",
    "            tree_method=\"hist\",\n",
    "            device=device,          # if xgboost ,\n",
    "            early_stopping_rounds=15,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            hist_df[feature_cols_xgb],\n",
    "            hist_df[\"y\"].astype(int),\n",
    "            sample_weight=w_hist,\n",
    "            eval_set=[(fold_df[feature_cols_xgb], fold_df[\"y\"].astype(int))],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        best_iter = (\n",
    "            int(model.best_iteration)\n",
    "            if hasattr(model, \"best_iteration\") and model.best_iteration is not None\n",
    "            else model.get_params().get(\"n_estimators\", 800)\n",
    "        )\n",
    "\n",
    "        proba_fold = model.predict_proba(fold_df[feature_cols_xgb])\n",
    "        ll = log_loss(fold_df[\"y\"], proba_fold, labels=[0, 1, 2])\n",
    "\n",
    "        fold_losses.append(ll)\n",
    "        fold_best_iters.append(best_iter)\n",
    "\n",
    " # fold()\n",
    "    if not fold_losses:\n",
    "        return 10.0\n",
    "\n",
    "    mean_ll = float(np.mean(fold_losses))\n",
    "    median_iter = float(np.median(fold_best_iters))\n",
    "\n",
    "    trial.set_user_attr(\"median_best_iter\", median_iter)\n",
    "    return mean_ll\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best value (cv mean logloss):\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Median best_iter of best trial:\", study.best_trial.user_attrs[\"median_best_iter\"])\n",
    "\n",
    "best_overall_params = study.best_params\n",
    "best_overall = study.best_value\n",
    "best_overall_iters = int(study.best_trial.user_attrs[\"median_best_iter\"])\n",
    "best_n = int(best_overall_iters + 5)\n",
    "\n",
    "print(\"\\n== Optuna result ==\")\n",
    "print(\"best_overall_params:\", best_overall_params)\n",
    "print(\"best_overall (cv mean logloss):\", best_overall)\n",
    "print(\"best_overall_iters:\", best_overall_iters)\n",
    "print(\"==> best_n =\", best_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1a85203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train_recent seasons: ['2013/2014', '2014/2015', '2015/2016', '2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021']\n",
      "train_recent size: 3040\n",
      "val size: 380 test size: 1140\n",
      "\n",
      "=== XGB Performance ===\n",
      "Train LL: 1.0321\n",
      "Val LL:   1.0261 (gap=-0.0059)\n",
      "Test LL:  1.0295 (gap=-0.0025)\n",
      "low or no overfitting\n"
     ]
    }
   ],
   "source": [
    "recent_seasons = all_train_seasons[-8:]\n",
    "train_recent = train_df[train_df[\"Season\"].isin(recent_seasons)].copy()\n",
    "\n",
    "print(\"Final train_recent seasons:\", recent_seasons)\n",
    "print(\"train_recent size:\", len(train_recent))\n",
    "print(\"val size:\", len(val_df), \"test size:\", len(test_df))\n",
    "\n",
    "# + sample_weight\n",
    "w_train_recent = make_time_class_weight(\n",
    "    train_recent,\n",
    "    tau=TIME_DECAY_TAU,\n",
    "    draw_boost=DRAW_CLASS_MULT\n",
    ")\n",
    "\n",
    "# from Optuna result\n",
    "md  = best_overall_params[\"max_depth\"]\n",
    "mcw = best_overall_params[\"min_child_weight\"]\n",
    "ra  = best_overall_params[\"reg_alpha\"]\n",
    "rl  = best_overall_params[\"reg_lambda\"]\n",
    "gm  = best_overall_params[\"gamma\"]\n",
    "sub = best_overall_params[\"subsample\"]\n",
    "col = best_overall_params[\"colsample_bytree\"]\n",
    "\n",
    "# final XGB Model\n",
    "xgb_final = xgb.XGBClassifier(\n",
    "    n_estimators=best_n,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=md,\n",
    "    min_child_weight=mcw,\n",
    "    gamma=gm,\n",
    "    reg_alpha=ra,\n",
    "    reg_lambda=rl,\n",
    "    subsample=sub,\n",
    "    colsample_bytree=col,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=3,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    tree_method=\"hist\",\n",
    "    device=device,      # if, device=...\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_final.fit(\n",
    "    train_recent[feature_cols_xgb],\n",
    "    train_recent[\"y\"].astype(int),\n",
    "    sample_weight=w_train_recent,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# full train/val/test \n",
    "proba_train_xgb = xgb_final.predict_proba(train_df[feature_cols_xgb])\n",
    "proba_val_xgb   = xgb_final.predict_proba(val_df[feature_cols_xgb])\n",
    "proba_test_xgb  = xgb_final.predict_proba(test_df[feature_cols_xgb])\n",
    "\n",
    "# Calculate logloss + \n",
    "train_ll = log_loss(train_df[\"y\"], proba_train_xgb, labels=[0, 1, 2])\n",
    "val_ll   = log_loss(val_df[\"y\"],   proba_val_xgb,   labels=[0, 1, 2])\n",
    "test_ll  = log_loss(test_df[\"y\"],  proba_test_xgb,  labels=[0, 1, 2])\n",
    "\n",
    "gap_val  = val_ll  - train_ll\n",
    "gap_test = test_ll - train_ll\n",
    "\n",
    "print(\"\\n=== XGB Performance ===\")\n",
    "print(f\"Train LL: {train_ll:.4f}\")\n",
    "print(f\"Val LL:   {val_ll:.4f} (gap={gap_val:+.4f})\")\n",
    "print(f\"Test LL:  {test_ll:.4f} (gap={gap_test:+.4f})\")\n",
    "\n",
    "if gap_val > 0.10 or gap_test > 0.10:\n",
    "    print(\"significant overfitting\")\n",
    "elif gap_val > 0.05 or gap_test > 0.05:\n",
    "    print(\"midium overfitting\")\n",
    "else:\n",
    "    print(\"low or no overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f321581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB val logloss: 1.027\n",
      "XGB test logloss: 1.0323\n",
      "\n",
      "XGB Final model trained on train_recent + val\n"
     ]
    }
   ],
   "source": [
    "# Final training on train_recent + val\n",
    "train_recent = train_df[train_df[\"Season\"].isin(recent_seasons)].copy()\n",
    "w_train_recent = make_sample_weight(train_recent)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "except:\n",
    "    device = \"cpu\"\n",
    "\n",
    "xgb_tmp = xgb.XGBClassifier(\n",
    "    n_estimators=best_n, learning_rate=0.03,\n",
    "    max_depth=4, min_child_weight=5, gamma=0.5,\n",
    "    reg_alpha=0.5, reg_lambda=2.0,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    objective=\"multi:softprob\", num_class=3,\n",
    "    eval_metric=\"mlogloss\", tree_method=\"hist\",\n",
    "    device=device,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_tmp.fit(train_recent[feature_cols_xgb], train_recent[\"y\"].astype(int),\n",
    "            sample_weight=w_train_recent, verbose=False)\n",
    "\n",
    "proba_val_xgb  = xgb_tmp.predict_proba(val_df[feature_cols_xgb])\n",
    "proba_test_xgb = xgb_tmp.predict_proba(test_df[feature_cols_xgb])\n",
    "\n",
    "print(\"XGB val logloss:\", round(log_loss(val_df[\"y\"], proba_val_xgb, labels=[0,1,2]), 4))\n",
    "print(\"XGB test logloss:\", round(log_loss(test_df[\"y\"], proba_test_xgb, labels=[0,1,2]), 4))\n",
    "\n",
    "# Final model on (train_recent + val)\n",
    "trainval_df = pd.concat([train_recent, val_df], axis=0, ignore_index=True)\n",
    "w_trainval = np.concatenate([w_train_recent, make_sample_weight(val_df)])\n",
    "\n",
    "xgb_final = xgb.XGBClassifier(\n",
    "    n_estimators=best_n, learning_rate=0.03,\n",
    "    max_depth=4, min_child_weight=5, gamma=0.5,\n",
    "    reg_alpha=0.5, reg_lambda=2.0,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    objective=\"multi:softprob\", num_class=3,\n",
    "    eval_metric=\"mlogloss\", tree_method=\"hist\",\n",
    "    device=device,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_final.fit(trainval_df[feature_cols_xgb], trainval_df[\"y\"].astype(int),\n",
    "              sample_weight=w_trainval, verbose=False)\n",
    "\n",
    "proba_train_xgb = xgb_final.predict_proba(train_df[feature_cols_xgb])\n",
    "proba_val_xgb  = xgb_final.predict_proba(val_df[feature_cols_xgb])\n",
    "proba_test_xgb = xgb_final.predict_proba(test_df[feature_cols_xgb])\n",
    "\n",
    "print(f\"\\nXGB Final model trained on train_recent + val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b15b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB OOF has been save as xgb_oof_config.pkl\n"
     ]
    }
   ],
   "source": [
    "# save xgb oof\n",
    "xgb_base_params = xgb_final.get_params()\n",
    "\n",
    "xgb_oof_cfg = {\n",
    "    \"xgb_base_params\": xgb_base_params,\n",
    "    \"best_n\": best_n,\n",
    "}\n",
    "\n",
    "with open(\"xgb_oof_config.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xgb_oof_cfg, f)\n",
    "\n",
    "print(f\"XGB OOF has been save as xgb_oof_config.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da92bef",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1dd8cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Transformer v3\n",
    "warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    TORCH_OK = True\n",
    "except Exception as e:\n",
    "    print(\" torch not available:\", e)\n",
    "    TORCH_OK = True\n",
    "\n",
    "if TORCH_OK:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Transformer device:\", DEVICE)\n",
    "\n",
    "class EnhancedMatchTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    simplified version\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len=5, feat_dim=20, match_feat_dim=3, \n",
    "                 d_model=64, nhead=4, num_layers=2, \n",
    "                 ff_dim=128, num_classes=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # input projection\n",
    "\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(feat_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),  # using ReLU, more stable\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # position c\n",
    "        self.pos_emb = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        # simple encoder\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff_dim, \n",
    "            dropout=dropout,\n",
    "            batch_first=True, \n",
    "            activation=\"relu\"  # ReLU\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        \n",
    "        # using attention pool instead of using last time step\n",
    "        self.attn_pool = nn.Sequential(\n",
    "            nn.Linear(d_model, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # match projection\n",
    "        self.match_proj = nn.Sequential(\n",
    "            nn.Linear(match_feat_dim, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # home_summary + away_summary + match_feat\n",
    "        final_dim = d_model * 2 + d_model // 2\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(final_dim),\n",
    "            nn.Linear(final_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.5)  # gain\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "    \n",
    "    def encode_team(self, seq):\n",
    "        \"\"\"\n",
    "        encoder for single team\n",
    "        \"\"\"\n",
    "        B, L, _ = seq.shape\n",
    "        \n",
    "        # projection + position code\n",
    "        x = self.input_proj(seq)\n",
    "        pos_idx = torch.arange(L, device=seq.device).unsqueeze(0).expand(B, -1)\n",
    "        x = x + self.pos_emb(pos_idx)\n",
    "        \n",
    "        # Transformer Encoding\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        attn_weights = self.attn_pool(z)  # [B, L, 1]\n",
    "        summary = (z * attn_weights).sum(dim=1)  # [B, d_model]\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def forward(self, home_seq, away_seq, match_feat):\n",
    "        # homeaway\n",
    "        home_summary = self.encode_team(home_seq)\n",
    "        away_summary = self.encode_team(away_seq)\n",
    "        \n",
    "        match_emb = self.match_proj(match_feat)\n",
    "        \n",
    "        # combine all features\n",
    "        combined = torch.cat([home_summary, away_summary, match_emb], dim=1)\n",
    "        \n",
    "        return self.classifier(combined)\n",
    "\n",
    "def build_team_sequences_fixed(df, feature_cols, seq_len=5):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.sort_values(\"Date\").copy().reset_index(drop=True)\n",
    "    \n",
    "    safe_features = [f for f in feature_cols if f in df.columns]\n",
    "    FEAT_DIM = len(safe_features)\n",
    "    \n",
    "    print(f\"Sequence feature number: {FEAT_DIM}\")\n",
    "    print(f\"Sequence feature : {safe_features[:10]}...\")  # only print first 10th\n",
    "    \n",
    "    zero_token = np.zeros(FEAT_DIM, dtype=np.float32)\n",
    "    \n",
    "    # maintain past sequence for each team\n",
    "    # structure: {team: deque([token1, token2, ...])}\n",
    "    team_history = defaultdict(lambda: deque([zero_token.copy() for _ in range(seq_len)], maxlen=seq_len))\n",
    "    \n",
    "    home_seqs = []\n",
    "    away_seqs = []\n",
    "    match_features = []\n",
    "    \n",
    "    for idx, match in df.iterrows():\n",
    "        h_team = match[\"HomeTeam\"]\n",
    "        a_team = match[\"AwayTeam\"]\n",
    "        \n",
    "        home_seq = np.array(list(team_history[h_team]), dtype=np.float32)\n",
    "        away_seq = np.array(list(team_history[a_team]), dtype=np.float32)\n",
    "        \n",
    "        home_seqs.append(home_seq)\n",
    "        away_seqs.append(away_seq)\n",
    "        \n",
    "        # \n",
    "        h2h_home = match.get(\"h2h_home_rate\", 0.45)\n",
    "        h2h_draw = match.get(\"h2h_draw_rate\", 0.25)\n",
    " # rest_diff → clip → [-2,2] around\n",
    "        rest_val = match.get(\"rest_diff\", 0.0)\n",
    "        rest_val = 0.0 if pd.isna(rest_val) else float(rest_val)\n",
    "        rest_val = np.clip(rest_val, -14.0, 14.0) / 7.0   # [-2,2] interval\n",
    "\n",
    "        match_feat = np.array(\n",
    "            [\n",
    "                float(h2h_home) if not pd.isna(h2h_home) else 0.45,\n",
    "                float(h2h_draw) if not pd.isna(h2h_draw) else 0.25,\n",
    "                rest_val,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        match_features.append(match_feat)\n",
    "\n",
    "        # \n",
    "        home_token = []\n",
    "        away_token = []\n",
    "        \n",
    "        for f in safe_features:\n",
    "            v = match.get(f, 0.0)\n",
    "            v = 0.0 if pd.isna(v) else float(v)\n",
    "        \n",
    "            #  xxx_home / xxx_away\n",
    "            if f.endswith(\"_home\"):\n",
    "                f_away = f.replace(\"_home\", \"_away\")\n",
    "                v_home = v\n",
    "                v_away = match.get(f_away, v)  #  v\n",
    "                v_away = 0.0 if pd.isna(v_away) else float(v_away)\n",
    "        \n",
    "            elif f.endswith(\"_away\"):\n",
    "                f_home = f.replace(\"_away\", \"_home\")\n",
    "                v_away = v\n",
    "                v_home = match.get(f_home, v)\n",
    "                v_home = 0.0 if pd.isna(v_home) else float(v_home)\n",
    "        \n",
    " # xxx_diff(home)\n",
    "            elif f.endswith(\"_diff\"):\n",
    "                v_home = v\n",
    "                v_away = -v  # away\n",
    "        \n",
    "            else:\n",
    "                v_home = v\n",
    "                v_away = v\n",
    "        \n",
    "            home_token.append(float(v_home))\n",
    "            away_token.append(float(v_away))\n",
    "\n",
    "        home_token = np.array(home_token, dtype=np.float32)\n",
    "        away_token = np.array(away_token, dtype=np.float32)\n",
    "        \n",
    "        team_history[h_team].append(home_token)\n",
    "        team_history[a_team].append(away_token)\n",
    "    \n",
    "    df[\"home_form_seq\"] = home_seqs\n",
    "    df[\"away_form_seq\"] = away_seqs\n",
    "    df[\"match_features\"] = match_features\n",
    "    \n",
    "    return df, FEAT_DIM\n",
    "\n",
    "# 3: Dataset -> EnhancedMatchDataset\n",
    "\n",
    "class EnhancedMatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, df, class_weights=None):\n",
    " # columns\n",
    "        home_np = np.stack(df[\"home_form_seq\"].values).astype(np.float32)\n",
    "        away_np = np.stack(df[\"away_form_seq\"].values).astype(np.float32)\n",
    "        match_np = np.stack(df[\"match_features\"].values).astype(np.float32)\n",
    "        \n",
    "        # NaN\n",
    "        home_np = np.nan_to_num(home_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        away_np = np.nan_to_num(away_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        match_np = np.nan_to_num(match_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # \n",
    "        print(f\"  home_seq range: [{home_np.min():.2f}, {home_np.max():.2f}]\")\n",
    "        print(f\"  away_seq range: [{away_np.min():.2f}, {away_np.max():.2f}]\")\n",
    "        print(f\"  match_feat range: [{match_np.min():.2f}, {match_np.max():.2f}]\")\n",
    "        \n",
    "        self.home_seq = torch.tensor(home_np, dtype=torch.float32)\n",
    "        self.away_seq = torch.tensor(away_np, dtype=torch.float32)\n",
    "        self.match_feat = torch.tensor(match_np, dtype=torch.float32)\n",
    "        self.y = torch.tensor(df[\"y\"].astype(int).values, dtype=torch.long)\n",
    "        \n",
    "        # \n",
    "        if class_weights is not None:\n",
    "            self.sample_weights = torch.tensor(\n",
    "                [class_weights[yi] for yi in self.y.numpy()], \n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        else:\n",
    "            self.sample_weights = torch.ones(len(self.y), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.home_seq[idx], self.away_seq[idx], \n",
    "                self.match_feat[idx], self.y[idx], self.sample_weights[idx])\n",
    "\n",
    "# 4: -> train_enhanced_tf\n",
    "\n",
    "def train_enhanced_tf(model, train_loader, val_loader, \n",
    "                      class_weights, epochs=50, patience=8):\n",
    "    \"\"\"\n",
    "\n",
    "    2. use FocalLoss\n",
    "    \"\"\"\n",
    "\n",
    " # , draw class(1)\n",
    "    weight_tensor = torch.tensor(\n",
    "        [class_weights.get(i, 1.0) for i in range(3)], \n",
    "        dtype=torch.float32\n",
    "    ).to(DEVICE)\n",
    "\n",
    " # use FocalLoss CrossEntropy\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "\n",
    "    #  & Learning rate\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "    # \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "    best_ll = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        tr_correct = 0\n",
    "        tr_total = 0\n",
    "        all_preds = []\n",
    "\n",
    "        for home_seq, away_seq, match_feat, yb, sw in train_loader:\n",
    "            home_seq = home_seq.to(DEVICE)\n",
    "            away_seq = away_seq.to(DEVICE)\n",
    "            match_feat = match_feat.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            sw = sw.to(DEVICE)\n",
    "\n",
    "            home_seq = torch.nan_to_num(home_seq, nan=0.0)\n",
    "            away_seq = torch.nan_to_num(away_seq, nan=0.0)\n",
    "            match_feat = torch.nan_to_num(match_feat, nan=0.0)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(enabled=(DEVICE.type == \"cuda\")):\n",
    "                logits = model(home_seq, away_seq, match_feat)\n",
    "\n",
    "                if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                    print(f\" Epoch {ep}: NaN/Inf in logits, skipping batch\")\n",
    "                    continue\n",
    "\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print(f\" Epoch {ep}: NaN loss, skipping batch\")\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            tr_loss += loss.item() * yb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            tr_correct += (preds == yb).sum().item()\n",
    "            tr_total += yb.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        tr_loss /= len(train_loader.dataset)\n",
    "        tr_acc = tr_correct / tr_total\n",
    "        train_pred_dist = np.bincount(all_preds, minlength=3) / len(all_preds)\n",
    "\n",
    "        model.eval()\n",
    "        probs, ys, val_preds = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for home_seq, away_seq, match_feat, yb, _ in val_loader:\n",
    "                home_seq = home_seq.to(DEVICE)\n",
    "                away_seq = away_seq.to(DEVICE)\n",
    "                match_feat = match_feat.to(DEVICE)\n",
    "                home_seq = torch.nan_to_num(home_seq, nan=0.0)\n",
    "                away_seq = torch.nan_to_num(away_seq, nan=0.0)\n",
    "                match_feat = torch.nan_to_num(match_feat, nan=0.0)\n",
    "\n",
    "                logits = model(home_seq, away_seq, match_feat)\n",
    "                prob = torch.softmax(logits, dim=1)\n",
    "                prob = torch.clamp(prob, 1e-7, 1 - 1e-7)\n",
    "                probs.append(prob.cpu().numpy())\n",
    "                ys.append(yb.numpy())\n",
    "                val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        proba_val = np.vstack(probs)\n",
    "        y_val = np.concatenate(ys)\n",
    "        proba_val = proba_val / proba_val.sum(axis=1, keepdims=True)\n",
    "        va_ll = log_loss(y_val, proba_val, labels=[0, 1, 2])\n",
    "        val_pred_dist = np.bincount(val_preds, minlength=3) / len(val_preds)\n",
    "\n",
    "        scheduler.step(va_ll)\n",
    "\n",
    "        if ep == 1 or ep % 3 == 0:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {ep:02d} | lr={lr:.6f} | train_loss={tr_loss:.4f} | train_acc={tr_acc:.3f}\")\n",
    "            print(f\"         | val_logloss={va_ll:.4f}\")\n",
    "            print(f\"         | train_pred_dist: H={train_pred_dist[0]:.2f} D={train_pred_dist[1]:.2f} A={train_pred_dist[2]:.2f}\")\n",
    "            print(f\"         | val_pred_dist:   H={val_pred_dist[0]:.2f} D={val_pred_dist[1]:.2f} A={val_pred_dist[2]:.2f}\")\n",
    "\n",
    "            if max(train_pred_dist) > 0.8:\n",
    "                print(f\"Warning: Training set predictions are too focused on categories {np.argmax(train_pred_dist)}\")\n",
    "\n",
    "        if va_ll < best_ll - 1e-4:\n",
    "            best_ll = va_ll\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    print(f\"\\n Best val logloss: {best_ll:.4f}\")\n",
    "    return model\n",
    "\n",
    "def predict_enhanced_tf(model, loader):\n",
    "    \"\"\"\n",
    "    prediction function\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for home_seq, away_seq, match_feat, _, _ in loader:\n",
    "            home_seq = home_seq.to(DEVICE)\n",
    "            away_seq = away_seq.to(DEVICE)\n",
    "            match_feat = match_feat.to(DEVICE)\n",
    "            \n",
    "            home_seq = torch.nan_to_num(home_seq, nan=0.0)\n",
    "            away_seq = torch.nan_to_num(away_seq, nan=0.0)\n",
    "            match_feat = torch.nan_to_num(match_feat, nan=0.0)\n",
    "            \n",
    "            logits = model(home_seq, away_seq, match_feat)\n",
    "            prob = torch.softmax(logits, dim=1)\n",
    "            prob = torch.clamp(prob, 1e-7, 1 - 1e-7)\n",
    "            probs.append(prob.cpu().numpy())\n",
    "    \n",
    "    proba = np.vstack(probs)\n",
    "    proba = proba / proba.sum(axis=1, keepdims=True)\n",
    "    return proba\n",
    "\n",
    "def standardize_sequences(train_df, val_df, test_df=None):\n",
    "    \"\"\"\n",
    "    only use statistics from training set\n",
    "    \"\"\"\n",
    "    # \n",
    "    flat_train_home = np.vstack(train_df[\"home_form_seq\"].values).astype(np.float32)\n",
    "    flat_train_home = np.nan_to_num(flat_train_home, nan=0.0)\n",
    "    \n",
    "    mu_seq = flat_train_home.mean(axis=0)\n",
    "    sd_seq = flat_train_home.std(axis=0)\n",
    "    \n",
    "    mask_seq = sd_seq > 1e-3\n",
    "    sd_seq_safe = sd_seq.copy()\n",
    "    sd_seq_safe[~mask_seq] = 1.0  # \n",
    "    \n",
    "    flat_match = np.vstack(train_df[\"match_features\"].values).astype(np.float32)\n",
    "    flat_match = np.nan_to_num(flat_match, nan=0.0)\n",
    "    mu_match = flat_match.mean(axis=0)\n",
    "    sd_match = flat_match.std(axis=0)\n",
    "    mask_match = sd_match > 1e-3\n",
    "    sd_match_safe = sd_match.copy()\n",
    "    sd_match_safe[~mask_match] = 1.0\n",
    "    \n",
    "    def norm_seq(seq):\n",
    "        seq = np.nan_to_num(seq.astype(np.float32), nan=0.0)\n",
    "        return (seq - mu_seq) / sd_seq_safe\n",
    "    \n",
    "    def norm_match(feat):\n",
    "        feat = np.nan_to_num(feat.astype(np.float32), nan=0.0)\n",
    "        return (feat - mu_match) / sd_match_safe\n",
    "\n",
    "    for dfx in [train_df, val_df] + ([test_df] if test_df is not None else []):\n",
    "        dfx[\"home_form_seq\"] = dfx[\"home_form_seq\"].apply(norm_seq)\n",
    "        dfx[\"away_form_seq\"] = dfx[\"away_form_seq\"].apply(norm_seq)\n",
    "        dfx[\"match_features\"] = dfx[\"match_features\"].apply(norm_match)\n",
    "    \n",
    "    return train_df, val_df, test_df, mu_seq, sd_seq, mu_match, sd_match\n",
    "\n",
    "# home: -> run_enhanced_tf\n",
    "\n",
    "# home: -> run_enhanced_tf\n",
    "\n",
    "def run_enhanced_tf(cleaned_df, train_df, val_df, test_df, \n",
    "                    tf_token_features, seq_len=5, epochs=50):\n",
    "    \n",
    "    print(f\" Running EnhancedMatchTransformer \")\n",
    "    \n",
    "    # filt from tf_token_features instead of using feature_cols directly\n",
    "    name_map = {f: str(f).lower() for f in tf_token_features}\n",
    "\n",
    "    safe_features = []\n",
    "    for f in tf_token_features:\n",
    "        name = name_map[f]\n",
    "\n",
    "        # skip useless features\n",
    "        if f not in cleaned_df.columns:\n",
    "            continue\n",
    "\n",
    "        # features prior to save :form / elo / position / points / streak / pm\n",
    "        if (\n",
    "            (\"pm\" in name) or\n",
    "            (\"form\" in name) or\n",
    "            (\"elo\" in name) or\n",
    "            (\"position\" in name) or\n",
    "            (\"points\" in name) or\n",
    "            (\"l10\" in name) or\n",
    "            (\"win_streak\" in name) or\n",
    "            (\"unbeaten\" in name) or\n",
    "            (\"draw\" in name) or\n",
    "            (\"xg\" in name) or\n",
    "            (\"h2h\" in name) or\n",
    "            (\"rest\" in name) or\n",
    "            (\"momentum\" in name) or\n",
    "            (\"attack\" in name) or\n",
    "            (\"defense\" in name)\n",
    "        ):\n",
    "            safe_features.append(f)\n",
    "\n",
    " # if the number is not enough, use tf_token_features\n",
    "    if len(safe_features) < 10:\n",
    "        print(f\"lack of secure features, all using tf_token_features\")\n",
    "        safe_features = [f for f in tf_token_features if f in cleaned_df.columns]\n",
    "\n",
    "    print(f\"using {len(safe_features)} of sequence features\")\n",
    "\n",
    "    # Constructing the sequence: \n",
    "    # using the subset of lines from train/val/test \n",
    "    # that correspond to cleaned_df\n",
    "    print(f\"build team past sequence\")\n",
    "    \n",
    "    # using rows from cleaned_df , keep same as train/val/test \n",
    "    all_idx = pd.Index(train_df.index).union(val_df.index).union(test_df.index)\n",
    "    all_df = cleaned_df.loc[all_idx].sort_values(\"Date\")\n",
    "\n",
    "    # build past sequence\n",
    "    all_df_with_seq, feat_dim = build_team_sequences_fixed(\n",
    "        all_df, safe_features, seq_len=seq_len\n",
    "    )\n",
    "    \n",
    "    # sequence data are feed back to their sets\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    seq_cols = [\"home_form_seq\", \"away_form_seq\", \"match_features\"]\n",
    "    for col in seq_cols:\n",
    "        if col in train_df.columns:\n",
    "            train_df.drop(columns=[col], inplace=True)\n",
    "        if col in val_df.columns:\n",
    "            val_df.drop(columns=[col], inplace=True)\n",
    "        if col in test_df.columns:\n",
    "            test_df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    seq_data = all_df_with_seq[seq_cols]\n",
    "    train_df = train_df.join(seq_data, how=\"left\")\n",
    "    val_df   = val_df.join(seq_data, how=\"left\")\n",
    "    test_df  = test_df.join(seq_data, how=\"left\")\n",
    "    \n",
    "    zero_seq = np.zeros((seq_len, feat_dim), dtype=np.float32)\n",
    "    zero_match = np.zeros(3, dtype=np.float32)\n",
    "    \n",
    "    for dfx in [train_df, val_df, test_df]:\n",
    "        dfx[\"home_form_seq\"] = dfx[\"home_form_seq\"].apply(\n",
    "            lambda x: x if isinstance(x, np.ndarray) else zero_seq.copy()\n",
    "        )\n",
    "        dfx[\"away_form_seq\"] = dfx[\"away_form_seq\"].apply(\n",
    "            lambda x: x if isinstance(x, np.ndarray) else zero_seq.copy()\n",
    "        )\n",
    "        dfx[\"match_features\"] = dfx[\"match_features\"].apply(\n",
    "            lambda x: x if isinstance(x, np.ndarray) else zero_match.copy()\n",
    "        )\n",
    "    \n",
    "    # standardization\n",
    "    print(f\"standardization...\")\n",
    "    train_df, val_df, test_df, mu_seq, sd_seq, mu_match, sd_match = \\\n",
    "        standardize_sequences(train_df, val_df, test_df)\n",
    "    \n",
    "    # \n",
    "    print(f\"calculated class weights\")\n",
    "    y_train = train_df[\"y\"].astype(int).values\n",
    "    class_counts = np.bincount(y_train, minlength=3)\n",
    "    total = class_counts.sum()\n",
    "    \n",
    "    class_weights = {\n",
    "        0: total / (3 * class_counts[0])*1,           # Home\n",
    "        1: total / (3 * class_counts[1]) * 1.1,     # Draw ()\n",
    "        2: total / (3 * class_counts[2]),           # Away\n",
    "    }\n",
    "    \n",
    "    print(f\"  class distribution: H={class_counts[0]}, D={class_counts[1]}, A={class_counts[2]}\")\n",
    "    print(f\"  class weight: H={class_weights[0]:.2f}, D={class_weights[1]:.2f}, A={class_weights[2]:.2f}\")\n",
    "\n",
    "    # 5 Oversampling Enhancement draw class(1)\n",
    "    try:\n",
    "        X_sample = np.vstack(train_df[\"home_form_seq\"].values)[:, :5]  # 5\n",
    "        y_sample = train_df[\"y\"].values\n",
    "\n",
    "        sm = SMOTE(sampling_strategy={1: int(class_counts[1] * 1.3)}, random_state=42)\n",
    "        X_res, y_res = sm.fit_resample(X_sample, y_sample)\n",
    "\n",
    "    # random sampling train_df row, SMOTE \n",
    "        selected_indices = np.random.choice(train_df.index, size=len(X_res), replace=True)\n",
    "        train_df = train_df.loc[selected_indices].copy()\n",
    "        train_df[\"y\"] = y_res\n",
    "        print(f\"  Oversampled train size: {len(train_df)}\")\n",
    "    except Exception as e:\n",
    "        print(\" SMOTE oversampling failure:\", str(e))\n",
    "    \n",
    "    # DataLoader\n",
    "    print(\"create dataloader\")\n",
    "    train_ds = EnhancedMatchDataset(train_df, class_weights)\n",
    "    val_ds   = EnhancedMatchDataset(val_df,   class_weights)\n",
    "    test_ds  = EnhancedMatchDataset(test_df,  class_weights)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False)\n",
    "    \n",
    "    # create model\n",
    "\n",
    "    model = EnhancedMatchTransformer(\n",
    "        seq_len=seq_len,\n",
    "        feat_dim=feat_dim,\n",
    "        match_feat_dim=3,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        ff_dim=128,\n",
    "        dropout=0.3\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\" number of model features {n_params:,}\")\n",
    "    \n",
    "    # Train\n",
    "    print(f\" Training start \")\n",
    "    model = train_enhanced_tf(\n",
    "        model, train_loader, val_loader, \n",
    "        class_weights, epochs=epochs, patience=8\n",
    "    )\n",
    "    \n",
    "    # forecast\n",
    "    proba_train_tf = predict_enhanced_tf(model, train_loader)\n",
    "    proba_val_tf   = predict_enhanced_tf(model, val_loader)\n",
    "    proba_test_tf  = predict_enhanced_tf(model, test_loader)\n",
    "    \n",
    "    # Evaluation\n",
    "    val_ll = log_loss(val_df[\"y\"].values, proba_val_tf, labels=[0, 1, 2])\n",
    "    test_ll = log_loss(test_df[\"y\"].values, proba_test_tf, labels=[0, 1, 2])\n",
    "    \n",
    "    val_pred  = proba_val_tf.argmax(axis=1)\n",
    "    test_pred = proba_test_tf.argmax(axis=1)\n",
    "    \n",
    "    val_dist  = np.bincount(val_pred,  minlength=3) / len(val_pred)\n",
    "    test_dist = np.bincount(test_pred, minlength=3) / len(test_pred)\n",
    "    \n",
    "    print(f\"Result of transformer\")\n",
    "    print(f\"Val  logloss: {val_ll:.4f}\")\n",
    "    print(f\"Test logloss: {test_ll:.4f}\")\n",
    "    print(f\"Val  prediction distribution: H={val_dist[0]:.3f} D={val_dist[1]:.3f} A={val_dist[2]:.3f}\")\n",
    "    print(f\"Test prediction distribution: H={test_dist[0]:.3f} D={test_dist[1]:.3f} A={test_dist[2]:.3f}\")\n",
    "    # Save Transformer v3 ckpt\n",
    "    TF_CKPT_PATH = \"enhanced_tf_v3.ckpt\"   #  v2,\n",
    "    \n",
    "    tf_ckpt = {\n",
    "        # model weights\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"seq_len\": seq_len,\n",
    "        \"feat_dim\": feat_dim,\n",
    "        \"match_feat_dim\": 3,\n",
    "        \"d_model\": 64,\n",
    "        \"nhead\": 4,\n",
    "        \"num_layers\": 2,\n",
    "        \"ff_dim\": 128,\n",
    "        \"dropout\": 0.3,\n",
    "        \"token_features\": safe_features,\n",
    "        \"mu_seq\":   mu_seq,\n",
    "        \"sd_seq\":   sd_seq,\n",
    "        \"mu_match\": mu_match,\n",
    "        \"sd_match\": sd_match,\n",
    "    }\n",
    "\n",
    "    torch.save(tf_ckpt, TF_CKPT_PATH)\n",
    "    print(f\"Transformer v3 has been save to {TF_CKPT_PATH}\")\n",
    "    return model, proba_train_tf, proba_val_tf, proba_test_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b6e5439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running EnhancedMatchTransformer \n",
      "using 101 of sequence features\n",
      "build team past sequence\n",
      "Sequence feature number: 101\n",
      "Sequence feature : ['form_home', 'form_away', 'form_diff', 'form_home_v2', 'form_away_v2', 'form_diff_v2', 'home_home_form', 'away_away_form', 'win_streak_home', 'win_streak_away']...\n",
      "standardization...\n",
      "calculated class weights\n",
      "  class distribution: H=3673, D=1997, A=2310\n",
      "  class weight: H=0.72, D=1.47, A=1.15\n",
      " SMOTE oversampling failure: Found input variables with inconsistent numbers of samples: [79800, 7980]\n",
      "create dataloader\n",
      "  home_seq range: [-45.94, 45.94]\n",
      "  away_seq range: [-45.94, 45.94]\n",
      "  match_feat range: [-5.73, 5.75]\n",
      "  home_seq range: [-7.65, 7.17]\n",
      "  away_seq range: [-7.65, 7.17]\n",
      "  match_feat range: [-5.73, 5.75]\n",
      "  home_seq range: [-62.47, 62.47]\n",
      "  away_seq range: [-62.47, 62.47]\n",
      "  match_feat range: [-5.73, 5.75]\n",
      " number of model features 95,748\n",
      " Training start \n",
      "Epoch 01 | lr=0.001000 | train_loss=1.0890 | train_acc=0.394\n",
      "         | val_logloss=1.0487\n",
      "         | train_pred_dist: H=0.33 D=0.37 A=0.30\n",
      "         | val_pred_dist:   H=0.43 D=0.39 A=0.19\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0755 | train_acc=0.415\n",
      "         | val_logloss=1.0472\n",
      "         | train_pred_dist: H=0.36 D=0.38 A=0.26\n",
      "         | val_pred_dist:   H=0.40 D=0.28 A=0.32\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0700 | train_acc=0.421\n",
      "         | val_logloss=1.0412\n",
      "         | train_pred_dist: H=0.33 D=0.41 A=0.26\n",
      "         | val_pred_dist:   H=0.43 D=0.27 A=0.30\n",
      "Epoch 09 | lr=0.001000 | train_loss=1.0658 | train_acc=0.427\n",
      "         | val_logloss=1.0474\n",
      "         | train_pred_dist: H=0.37 D=0.38 A=0.25\n",
      "         | val_pred_dist:   H=0.44 D=0.23 A=0.33\n",
      "Epoch 12 | lr=0.000500 | train_loss=1.0451 | train_acc=0.447\n",
      "         | val_logloss=1.0533\n",
      "         | train_pred_dist: H=0.32 D=0.42 A=0.26\n",
      "         | val_pred_dist:   H=0.39 D=0.19 A=0.41\n",
      "Early stopping at epoch 14\n",
      "\n",
      " Best val logloss: 1.0412\n",
      "Result of transformer\n",
      "Val  logloss: 1.0412\n",
      "Test logloss: 1.0573\n",
      "Val  prediction distribution: H=0.429 D=0.271 A=0.300\n",
      "Test prediction distribution: H=0.395 D=0.275 A=0.330\n",
      "Transformer v3 has been save to enhanced_tf_v3.ckpt\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        ce_loss = self.ce(logits, target)  # [B]\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal = (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal.mean() if self.reduction == 'mean' else focal\n",
    "    \n",
    "if TORCH_OK:\n",
    " # tf_token_features transformer feature columns\n",
    "    model_tf, proba_train_tf, proba_val_tf, proba_test_tf = run_enhanced_tf(\n",
    "        cleaned_df=cleaned_df,\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        test_df=test_df,\n",
    "        tf_token_features=tf_token_features,\n",
    "        seq_len=10,      # or 8 / 10 \n",
    "        epochs=100       #  50 \n",
    "    )\n",
    "else:\n",
    " # no torch fallback, meta \n",
    "    proba_train_tf = np.full((len(train_df), 3), 1/3, dtype=float)\n",
    "    proba_val_tf   = np.full((len(val_df),   3), 1/3, dtype=float)\n",
    "    proba_test_tf  = np.full((len(test_df),  3), 1/3, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b6e2d",
   "metadata": {},
   "source": [
    "### Collecting features from base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1115ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " load xgb oof config from xgb_oof_config.pkl\n",
      "   BEST_N = 67\n"
     ]
    }
   ],
   "source": [
    "# XGB (base_params + best_n)\n",
    "with open(\"xgb_oof_config.pkl\", \"rb\") as f:\n",
    "    xgb_cfg = pickle.load(f)\n",
    "\n",
    "XGB_BASE_PARAMS = xgb_cfg[\"xgb_base_params\"]\n",
    "BEST_N = xgb_cfg.get(\"best_n\", XGB_BASE_PARAMS.get(\"n_estimators\", 800))\n",
    "\n",
    "print(f\" load xgb oof config from xgb_oof_config.pkl\")\n",
    "print(\"   BEST_N =\", BEST_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca9db589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB proba shapes: train (7980, 3) val (380, 3) test (1140, 3)\n"
     ]
    }
   ],
   "source": [
    "params = XGB_BASE_PARAMS.copy()\n",
    "\n",
    "# \n",
    "params.pop(\"n_estimators\", None)\n",
    "lr = params.pop(\"learning_rate\", 0.03)\n",
    "\n",
    "xgb_final = xgb.XGBClassifier(\n",
    "    **params,\n",
    "    n_estimators=BEST_N,\n",
    "    learning_rate=lr,\n",
    ")\n",
    "\n",
    "# (if make_sample_weight)\n",
    "sw_train =make_sample_weight(train_df) if \"make_sample_weight\" in globals() else None\n",
    "\n",
    "X_tr = train_df[feature_cols_xgb].to_numpy(dtype=np.float32)\n",
    "y_tr = train_df[\"y\"].astype(int).to_numpy()\n",
    "\n",
    "xgb_final.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    sample_weight=sw_train,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "if \"proba_train_xgb\" not in globals():\n",
    "    proba_train_xgb = xgb_final.predict_proba(\n",
    "        train_df[feature_cols_xgb].to_numpy(dtype=np.float32)\n",
    "    )\n",
    "\n",
    "if \"proba_val_xgb\" not in globals():\n",
    "    proba_val_xgb = xgb_final.predict_proba(\n",
    "        val_df[feature_cols_xgb].to_numpy(dtype=np.float32)\n",
    "    )\n",
    "\n",
    "if \"proba_test_xgb\" not in globals():\n",
    "    proba_test_xgb = xgb_final.predict_proba(\n",
    "        test_df[feature_cols_xgb].to_numpy(dtype=np.float32)\n",
    "    )\n",
    "\n",
    "print(\"XGB proba shapes:\",\n",
    "      \"train\", proba_train_xgb.shape,\n",
    "      \"val\", proba_val_xgb.shape,\n",
    "      \"test\", proba_test_xgb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10178cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayes DC val logloss: 0.9729\n",
      "Bayes DC test logloss: 1.001\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Bayes-DC posterior \n",
    "if \"attack_s\" not in globals():\n",
    "    with open(\"bayes_dc_global.ckpt\", \"rb\") as f:\n",
    "        dc_bayes = pickle.load(f)\n",
    "\n",
    "    tmap        = dc_bayes[\"tmap\"]\n",
    "    attack_s    = dc_bayes[\"attack_s\"]\n",
    "    defense_s   = dc_bayes[\"defense_s\"]\n",
    "    home_adv_s  = dc_bayes[\"home_adv_s\"]\n",
    "    rho_s       = dc_bayes[\"rho_s\"]\n",
    "    beta_h_s    = dc_bayes[\"beta_h_s\"]\n",
    "    beta_a_s    = dc_bayes[\"beta_a_s\"]\n",
    "    dc_feature_cols = dc_bayes[\"dc_feature_cols\"]\n",
    "    DECAY_LAMBDA    = dc_bayes[\"DECAY_LAMBDA\"]\n",
    "    dc_scaler       = dc_bayes[\"dc_scaler\"]\n",
    "\n",
    "    print(\"✅ bayes_dc_global.ckpt recovery Bayes-DC posterior\")\n",
    "\n",
    "X_dc_train_full = dc_scaler.transform(\n",
    "    train_df[dc_feature_cols].copy().fillna(0.0)\n",
    ")\n",
    "X_dc_val = dc_scaler.transform(\n",
    "    val_df[dc_feature_cols].copy().fillna(0.0)\n",
    ")\n",
    "X_dc_test = dc_scaler.transform(\n",
    "    test_df[dc_feature_cols].copy().fillna(0.0)\n",
    ")\n",
    "\n",
    "# \n",
    "if \"proba_train_dc\" not in globals():\n",
    "    proba_train_dc, scorelines_train = bayes_dc_predict_full(\n",
    "        train_df,\n",
    "        X_dc_train_full,\n",
    "        attack_s, defense_s,\n",
    "        home_adv_s, rho_s,\n",
    "        beta_h_s, beta_a_s,\n",
    "        tmap\n",
    "    )\n",
    "\n",
    "if \"proba_val_dc\" not in globals():\n",
    "    proba_val_dc, scorelines_val = bayes_dc_predict_full(\n",
    "        val_df,\n",
    "        X_dc_val,\n",
    "        attack_s, defense_s,\n",
    "        home_adv_s, rho_s,\n",
    "        beta_h_s, beta_a_s,\n",
    "        tmap\n",
    "    )\n",
    "\n",
    "if \"proba_test_dc\" not in globals():\n",
    "    proba_test_dc, scorelines_test = bayes_dc_predict_full(\n",
    "        test_df,\n",
    "        X_dc_test,\n",
    "        attack_s, defense_s,\n",
    "        home_adv_s, rho_s,\n",
    "        beta_h_s, beta_a_s,\n",
    "        tmap\n",
    "    )\n",
    "\n",
    "print(\"Bayes DC val logloss:\",\n",
    "      round(log_loss(val_df[\"y\"], proba_val_dc, labels=[0,1,2]), 4))\n",
    "print(\"Bayes DC test logloss:\",\n",
    "      round(log_loss(test_df[\"y\"], proba_test_dc, labels=[0,1,2]), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcecc574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob shapes:\n",
      "  XGB:  train=(7980, 3), val=(380, 3), test=(1140, 3)\n",
      "  DC:   train=(7980, 3), val=(380, 3), test=(1140, 3)\n",
      "  TF:   train=(7980, 3), val=(380, 3), test=(1140, 3)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "if \"proba_train_xgb\" not in globals():\n",
    "    proba_train_xgb = xgb_final.predict_proba(train_df[feature_cols_xgb])\n",
    "\n",
    "if \"proba_train_dc\" not in globals():\n",
    "    proba_train_dc, _ = bayes_dc_predict_full(train_df, attack_s, defense_s, home_adv_s, rho_s, tmap)\n",
    "\n",
    "if TORCH_OK:\n",
    "    if \"proba_train_tf\" not in globals():\n",
    "        train_loader_full = DataLoader(EnhancedMatchDataset(train_df), batch_size=64, shuffle=False, pin_memory=True)\n",
    "        proba_train_tf = predict_tf(model_tf, train_loader_full)\n",
    "else:\n",
    "    proba_train_tf = np.full((len(train_df), 3), 1/3)\n",
    "    proba_val_tf = np.full((len(val_df), 3), 1/3)\n",
    "    proba_test_tf = np.full((len(test_df), 3), 1/3)\n",
    "\n",
    "print(\"Prob shapes:\")\n",
    "print(f\"  XGB:  train={proba_train_xgb.shape}, val={proba_val_xgb.shape}, test={proba_test_xgb.shape}\")\n",
    "print(f\"  DC:   train={proba_train_dc.shape}, val={proba_val_dc.shape}, test={proba_test_dc.shape}\")\n",
    "print(f\"  TF:   train={proba_train_tf.shape}, val={proba_val_tf.shape}, test={proba_test_tf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e310e1f",
   "metadata": {},
   "source": [
    "### OOF stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8986d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf(model, loader):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for hs, asq, mfeat, _ in loader:\n",
    "            hs = hs.to(DEVICE)\n",
    "            asq = asq.to(DEVICE)\n",
    "            mfeat = mfeat.to(DEVICE)\n",
    "            logits = model(hs, asq, mfeat)\n",
    "            prob = torch.softmax(logits, dim=1)\n",
    "            prob = torch.clamp(prob, 1e-7, 1 - 1e-7)\n",
    "            probs.append(prob.cpu().numpy())\n",
    "    proba = np.vstack(probs)\n",
    "    proba = proba / proba.sum(axis=1, keepdims=True)\n",
    "    return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "816b444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_enhanced_tf_checkpoint(path=\"enhanced_tf_v3.ckpt\", device=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "\n",
    "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
    "\n",
    " # from ckpt \n",
    "    seq_len         = ckpt.get(\"seq_len\", 5)\n",
    "    feat_dim        = ckpt.get(\"feat_dim\")\n",
    "    match_feat_dim  = ckpt.get(\"match_feat_dim\", 3)\n",
    "    d_model         = ckpt.get(\"d_model\", 64)\n",
    "    nhead           = ckpt.get(\"nhead\", 4)\n",
    "    num_layers      = ckpt.get(\"num_layers\", 2)\n",
    "    ff_dim          = ckpt.get(\"ff_dim\", 128)\n",
    "    dropout         = ckpt.get(\"dropout\", 0.3)\n",
    "\n",
    "    # \n",
    "    model = EnhancedMatchTransformer(\n",
    "        seq_len=seq_len,\n",
    "        feat_dim=feat_dim,\n",
    "        match_feat_dim=match_feat_dim,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_layers=num_layers,\n",
    "        ff_dim=ff_dim,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    " # ✅: token_features, all_token_features\n",
    "    token_features = ckpt.get(\"token_features\", ckpt.get(\"all_token_features\", []))\n",
    "\n",
    "    mu_seq   = ckpt[\"mu_seq\"]\n",
    "    sd_seq   = ckpt[\"sd_seq\"]\n",
    "    mu_match = ckpt[\"mu_match\"]\n",
    "    sd_match = ckpt[\"sd_match\"]\n",
    "\n",
    "    return model, token_features, mu_seq, sd_seq, mu_match, sd_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "916e29d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF safe features: 120\n",
      "Sequence feature number: 120\n",
      "Sequence feature : ['form_home', 'form_away', 'form_diff', 'form_home_v2', 'form_away_v2', 'form_diff_v2', 'home_home_form', 'away_away_form', 'win_streak_home', 'win_streak_away']...\n",
      "TF sequence construction complete: FORM_FEATURE_DIM_TF = 120\n",
      "Does train_recent have home_form_seq? True\n"
     ]
    }
   ],
   "source": [
    "# TF sequence( OOF + final TF )\n",
    "\n",
    "SEQ_LEN_TF = 5   # sum run_enhanced_tf  seq_len \n",
    "\n",
    "# TF token ,if tf_token_features ;\n",
    "safe_features_tf = [f for f in tf_token_features if f in cleaned_df.columns]\n",
    "\n",
    "print(\"TF safe features:\", len(safe_features_tf))\n",
    "\n",
    "# cleaned_df \n",
    "cleaned_with_seq, FORM_FEATURE_DIM_TF = build_team_sequences_fixed(\n",
    "    cleaned_df, safe_features_tf, seq_len=SEQ_LEN_TF\n",
    ")\n",
    "\n",
    "seq_cols_tf = [\"home_form_seq\", \"away_form_seq\", \"match_features\"]\n",
    "tf_seq_store = cleaned_with_seq[seq_cols_tf]\n",
    "\n",
    "#  join round train_df / val_df / test_df\n",
    "for name in [\"train_df\", \"val_df\", \"test_df\"]:\n",
    "    df = globals()[name]\n",
    "    df = df.drop(columns=seq_cols_tf, errors=\"ignore\")\n",
    " # by index connection( df cleaned_df)\n",
    "    df = df.join(tf_seq_store, how=\"left\")\n",
    "    globals()[name] = df\n",
    "\n",
    "# train_recent, OOF \n",
    "train_recent = train_df[train_df[\"Season\"].isin(recent_seasons)].copy()\n",
    "\n",
    "print(\"TF sequence construction complete: FORM_FEATURE_DIM_TF =\", FORM_FEATURE_DIM_TF)\n",
    "print(\"Does train_recent have home_form_seq?\", \"home_form_seq\" in train_recent.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6678a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw specialist\n",
    "\n",
    "def fit_draw_xgb_fold(train_fold, feature_cols_draw):\n",
    "    \"\"\"\n",
    "    - target: 1 = Draw, 0 = Not-Draw\n",
    "    \"\"\"\n",
    "    if \"XGB_BASE_PARAMS\" not in globals() or \"BEST_N\" not in globals():\n",
    "        raise ValueError(\" XGB_BASE_PARAMS and BEST_N(home XGB consistent) should be defined before.\")\n",
    "\n",
    "    params = XGB_BASE_PARAMS.copy()\n",
    "\n",
    "    params.pop(\"n_estimators\", None)\n",
    "    params.pop(\"num_class\",   None)\n",
    "\n",
    " # ;eval_metric logloss\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params[\"eval_metric\"] = \"logloss\"\n",
    "\n",
    " # learning_rate \n",
    "    lr = params.pop(\"learning_rate\", 0.03)\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        **params,\n",
    "        n_estimators=BEST_N,\n",
    "        learning_rate=lr,\n",
    "    )\n",
    "\n",
    "    # Feature & Label\n",
    "    X_train = train_fold[feature_cols_draw].to_numpy(dtype=np.float32)\n",
    "    y_train = (train_fold[\"y\"].astype(int).to_numpy() == 1).astype(int)  # 1 = Draw\n",
    "\n",
    " # sample_weight( TIME_DECAY_LAMBDA + DRAW_CLASS_MULT)\n",
    "    sw = make_sample_weight(train_fold) if \"make_sample_weight\" in globals() else None\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        sample_weight=sw,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_draw_specialist_oof(\n",
    "    meta_df,\n",
    "    seasons_sorted,\n",
    "    draw_feature_cols,\n",
    "    max_train_seasons=8,\n",
    "    target_last_k_seasons=6,\n",
    "):\n",
    "    \"\"\"\n",
    "    - input:\n",
    "                 (contain 'Season', 'Date', 'y' and draw_feature_cols)\n",
    "    - output:\n",
    "    \"\"\"\n",
    "    df = meta_df.sort_values(\"Date\").copy()\n",
    "\n",
    "    # \n",
    "    feature_cols_draw = [c for c in draw_feature_cols if c in df.columns]\n",
    "    if len(feature_cols_draw) == 0:\n",
    "        raise ValueError(\"draw_feature_cols  meta_df ,.\")\n",
    "\n",
    "    n = len(df)\n",
    "    oof_pD = np.full(n, np.nan, dtype=float)\n",
    "\n",
    " # K OOF target\n",
    "    if target_last_k_seasons is None or target_last_k_seasons <= 0:\n",
    "        target_seasons = seasons_sorted\n",
    "    else:\n",
    "        target_seasons = seasons_sorted[-min(target_last_k_seasons, len(seasons_sorted)):]\n",
    "\n",
    "    print(\"🎯 Draw-specialist OOF target seasons:\", target_seasons)\n",
    "\n",
    "    for s in target_seasons:\n",
    "        # current val s  = season s\n",
    "        val_mask = (df[\"Season\"] == s)\n",
    "        # training set = max number of season of max_train_seasons \n",
    "        train_seasons = [ss for ss in seasons_sorted if ss < s][-max_train_seasons:]\n",
    "        train_mask = df[\"Season\"].isin(train_seasons)\n",
    "\n",
    "        if train_mask.sum() == 0:\n",
    "            print(f\"no earlier season could beused for training, skip.\")\n",
    "            continue\n",
    "\n",
    "        train_fold = df.loc[train_mask].copy()\n",
    "        val_fold   = df.loc[val_mask].copy()\n",
    "\n",
    "        print(f\"  [Draw OOF] Season {s}: train seasons {sorted(set(train_fold['Season']))}, \"\n",
    "              f\"n_train={len(train_fold)}, n_val={len(val_fold)}\")\n",
    "\n",
    "    # XGB “draw”\n",
    "        draw_model = fit_draw_xgb_fold(train_fold, feature_cols_draw)\n",
    "\n",
    "    # season p(D)\n",
    "        X_val = val_fold[feature_cols_draw].to_numpy(dtype=np.float32)\n",
    "        pD_val = draw_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # OOF ( index alignment)\n",
    "        oof_pD[val_mask.values] = pD_val\n",
    "\n",
    "    missing_mask = np.isnan(oof_pD)\n",
    "    if missing_mask.any():\n",
    "        print(f\"There are {missing_mask.sum()} samples that are not covered by OOF, which will be filled in using the full model.\")\n",
    "\n",
    "    # draw\n",
    "    draw_model_full = fit_draw_xgb_fold(df, feature_cols_draw)\n",
    "\n",
    "    if missing_mask.any():\n",
    "        X_missing = df.loc[missing_mask, feature_cols_draw].to_numpy(dtype=np.float32)\n",
    "        oof_pD[missing_mask] = draw_model_full.predict_proba(X_missing)[:, 1]\n",
    "\n",
    "    print(\"Draw-specialist OOF completed, shape: \", oof_pD.shape)\n",
    "    return oof_pD, draw_model_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa238245-17e3-40d2-80b4-5af22afc58ff",
   "metadata": {},
   "source": [
    "# RUN_OOF = True\n",
    "DO_TF_OOF = True\n",
    "DO_DC_OOF_PROPER = True \n",
    "TORCH_OK = True\n",
    "\n",
    "# checkpoint for df + OOF \n",
    "OOF_CKPT_PATH = \"oof_full_checkpoint.pkl\"\n",
    "model_tf_loaded, all_token_features_loaded, mu_seq_loaded, sd_seq_loaded, mu_match_loaded, sd_match_loaded = \\\n",
    "    load_enhanced_tf_checkpoint()\n",
    "if RUN_OOF:\n",
    "\n",
    "    if os.path.exists(OOF_CKPT_PATH):\n",
    "        try:\n",
    "            with open(OOF_CKPT_PATH, \"rb\") as f:\n",
    "                ckpt = pickle.load(f)\n",
    "\n",
    "            train_df        = ckpt[\"train_df\"]\n",
    "            val_df          = ckpt[\"val_df\"]\n",
    "            meta_df         = ckpt[\"meta_df\"]\n",
    "            oof_xgb         = ckpt[\"oof_xgb\"]\n",
    "            oof_dc          = ckpt[\"oof_dc\"]\n",
    "            oof_tf          = ckpt[\"oof_tf\"]\n",
    "            seasons_sorted  = ckpt[\"seasons_sorted\"]\n",
    "            done_seasons    = set(ckpt.get(\"done_seasons\", []))\n",
    "\n",
    "            print(\" Recovering data and OOF progress from checkpoint\")\n",
    "            print(\" train_df shape:\", train_df.shape)\n",
    "            print(\" val_df shape:\", val_df.shape)\n",
    "            print(\" meta_df shape:\", meta_df.shape)\n",
    "            print(\" Season completed:\", sorted(done_seasons))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" checkpoint fail to read: {e}\")\n",
    "            print(\" The old checkpoint will be cleared, and OOF will be reconstructed from scratch.\")\n",
    "            os.remove(OOF_CKPT_PATH)\n",
    "            ckpt = None\n",
    "            done_seasons = set()\n",
    "\n",
    "            # Reconstruct the meta_df/oof array\n",
    "            meta_train_seasons = sorted(list(set(train_df[\"Season\"]).union(set(val_df[\"Season\"]))))\n",
    "            meta_df = pd.concat([train_df, val_df], axis=0).copy()\n",
    "            if \"_orig_idx\" not in meta_df.columns:\n",
    "                meta_df[\"_orig_idx\"] = meta_df.index\n",
    "            meta_df = meta_df[meta_df[\"Season\"].isin(meta_train_seasons)].copy()\n",
    "            meta_df = meta_df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "            seasons_sorted = sorted(meta_df[\"Season\"].unique(), key=season_start_year)\n",
    "\n",
    "            oof_xgb = np.full((len(meta_df), 3), np.nan, dtype=float)\n",
    "            oof_dc  = np.full((len(meta_df), 3), np.nan, dtype=float)\n",
    "            oof_tf  = np.full((len(meta_df), 3), np.nan, dtype=float)\n",
    "\n",
    "    else:\n",
    "        # if there is no checkpoint, it is the first run\n",
    "        done_seasons = set()\n",
    "\n",
    "        meta_train_seasons = sorted(list(set(train_df[\"Season\"]).union(set(val_df[\"Season\"]))))\n",
    "        meta_df = pd.concat([train_df, val_df], axis=0).copy()\n",
    "        if \"_orig_idx\" not in meta_df.columns:\n",
    "            meta_df[\"_orig_idx\"] = meta_df.index\n",
    "        meta_df = meta_df[meta_df[\"Season\"].isin(meta_train_seasons)].copy()\n",
    "        meta_df = meta_df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "        seasons_sorted = sorted(meta_df[\"Season\"].unique(), key=season_start_year)\n",
    "\n",
    "        oof_xgb = np.full((len(meta_df), 3), np.nan, dtype=float)\n",
    "        oof_dc  = np.full((len(meta_df), 3), np.nan, dtype=float)\n",
    "        oof_tf  = np.full((len(meta_df), 3), np.nan, dtype=float)\n",
    "\n",
    "    # ckpt ,meta_df / seasons_sorted / oof_* / done_seasons \n",
    "    orig_to_pos = {oid: i for i, oid in enumerate(meta_df[\"_orig_idx\"].values)}\n",
    "\n",
    "    draw_feature_cols = [\n",
    "        \"abs_form_diff\", \"abs_points_diff\", \"abs_gd_diff\",\n",
    "        \"abs_elo_sum_diff\", \"abs_position_diff\", \"abs_h2h_gd_avg\",\n",
    "        \"xg_total\", \"low_xg_flag\", \"attack_mom_sum\", \"defense_mom_sum\",\n",
    "        \"abs_shots_pm_diff\", \"abs_sot_pm_diff\",\n",
    "        \"abs_corners_pm_diff\", \"abs_fouls_pm_diff\",\n",
    "        \"draw_prop_home\", \"draw_prop_away\", \"draw_prop_sum\",\n",
    "        \"h2h_draw_rate\", \"h2h_draw_rate_td\", \"h2h_draw_rate_mean\",\n",
    "        \"ref_draw_rate\", \"high_draw_ref_flag\",\n",
    "        \"both_mid_table\", \"mid_season\", \"late_season\",\n",
    "    ]\n",
    "    # meta_df , KeyError\n",
    "    draw_feature_cols = [c for c in draw_feature_cols if c in meta_df.columns]\n",
    "    print(\"number of draw features:\", len(draw_feature_cols))\n",
    "\n",
    "    oof_pD_draw, draw_model_full = get_draw_specialist_oof(\n",
    "        meta_df=meta_df,\n",
    "        seasons_sorted=seasons_sorted,\n",
    "        draw_feature_cols=draw_feature_cols,\n",
    "        max_train_seasons=8,\n",
    "        target_last_k_seasons=6,\n",
    "    )\n",
    "    meta_df[\"pD_special\"] = oof_pD_draw\n",
    "\n",
    "    def fit_xgb_fold(train_fold):\n",
    " # / xgb_final base Parameters\n",
    "        params = XGB_BASE_PARAMS.copy()\n",
    "    \n",
    "        # \n",
    "        params.pop(\"n_estimators\", None)\n",
    " # learning_rate from params \n",
    "        lr = params.pop(\"learning_rate\", 0.03)\n",
    "    \n",
    "        model = xgb.XGBClassifier(\n",
    "            **params,\n",
    "            n_estimators=BEST_N,\n",
    "            learning_rate=lr,\n",
    "        )\n",
    "    \n",
    "        sw = make_sample_weight(train_fold) if \"make_sample_weight\" in globals() else None\n",
    "    \n",
    " #  numpy XGB, pandas \n",
    "        X_train = train_fold[feature_cols_xgb].to_numpy(dtype=np.float32)\n",
    "        y_train = train_fold[\"y\"].astype(int).to_numpy()\n",
    "    \n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            sample_weight=sw,\n",
    "            verbose=False,\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def fit_dc_fold_proper(train_fold, draws=300, tune=300):\n",
    "        teams_fold = sorted(set(train_fold[\"HomeTeam\"]).union(train_fold[\"AwayTeam\"]))\n",
    "        tmap_fold = {t: i for i, t in enumerate(teams_fold)}\n",
    "        n_teams_fold = len(teams_fold)\n",
    "        \n",
    "        home_idx_fold = train_fold[\"HomeTeam\"].map(tmap_fold).values.astype(\"int64\")\n",
    "        away_idx_fold = train_fold[\"AwayTeam\"].map(tmap_fold).values.astype(\"int64\")\n",
    "        gh_fold = train_fold[\"FTHG\"].astype(int).values\n",
    "        ga_fold = train_fold[\"FTAG\"].astype(int).values\n",
    "        \n",
    "        xi = DECAY_LAMBDA\n",
    "        last_date_fold = train_fold[\"Date\"].max()\n",
    "        delta_days_fold = (last_date_fold - train_fold[\"Date\"]).dt.days.values\n",
    "        w_fold = np.exp(-xi * delta_days_fold)\n",
    "        \n",
    "        with pm.Model() as dc_fold_model:\n",
    "            sigma_att = pm.HalfNormal(\"sigma_att\", sigma=0.7)\n",
    "            sigma_def = pm.HalfNormal(\"sigma_def\", sigma=0.7)\n",
    "            \n",
    "            att_offset = pm.Normal(\"att_offset\", 0.0, 1.0, shape=n_teams_fold)\n",
    "            def_offset = pm.Normal(\"def_offset\", 0.0, 1.0, shape=n_teams_fold)\n",
    "            \n",
    "            attack_raw = att_offset * sigma_att\n",
    "            defense_raw = def_offset * sigma_def\n",
    "            attack = pm.Deterministic(\"attack\", attack_raw - attack_raw[-1])\n",
    "            defense = pm.Deterministic(\"defense\", defense_raw - defense_raw[-1])\n",
    "            \n",
    "            home_adv = pm.Normal(\"home_adv\", 0.0, 0.5)\n",
    "            rho_raw = pm.Normal(\"rho_raw\", 0.0, 0.7)\n",
    "            rho = pm.Deterministic(\"rho\", 0.6 * pm.math.tanh(rho_raw))\n",
    "            \n",
    "            hi = at.as_tensor_variable(home_idx_fold, dtype=\"int64\")\n",
    "            ai = at.as_tensor_variable(away_idx_fold, dtype=\"int64\")\n",
    "            \n",
    "            eta_h = home_adv + attack[hi] - defense[ai]\n",
    "            eta_a = attack[ai] - defense[hi]\n",
    "            lam_h = pm.math.exp(eta_h)\n",
    "            lam_a = pm.math.exp(eta_a)\n",
    "            \n",
    "            gH = at.as_tensor_variable(gh_fold, dtype=\"int64\")\n",
    "            gA = at.as_tensor_variable(ga_fold, dtype=\"int64\")\n",
    "            corr = at.ones_like(gH, dtype=\"float64\")\n",
    "            \n",
    "            m00 = at.and_(at.eq(gH, 0), at.eq(gA, 0))\n",
    "            m01 = at.and_(at.eq(gH, 0), at.eq(gA, 1))\n",
    "            m10 = at.and_(at.eq(gH, 1), at.eq(gA, 0))\n",
    "            m11 = at.and_(at.eq(gH, 1), at.eq(gA, 1))\n",
    "            \n",
    "            corr = at.switch(m00, 1 - lam_h * lam_a * rho, corr)\n",
    "            corr = at.switch(m01, 1 + lam_h * rho, corr)\n",
    "            corr = at.switch(m10, 1 + lam_a * rho, corr)\n",
    "            corr = at.switch(m11, 1 - rho, corr)\n",
    "            corr = at.clip(corr, 1e-6, np.inf)\n",
    "            \n",
    "            logp_home = pm.logp(pm.Poisson.dist(mu=lam_h), gh_fold)\n",
    "            logp_away = pm.logp(pm.Poisson.dist(mu=lam_a), ga_fold)\n",
    "            logp_corr = at.log(corr)\n",
    "            weights = at.as_tensor_variable(w_fold, dtype=\"float64\")\n",
    "            pm.Potential(\"weighted_like\", at.sum(weights * (logp_home + logp_away + logp_corr)))\n",
    "            \n",
    "            trace = pm.sample(\n",
    "                draws=draws, tune=tune,\n",
    "                chains=4, cores=2,\n",
    "                target_accept=0.95,\n",
    "                random_seed=42,\n",
    "                progressbar=False\n",
    "            )\n",
    "        \n",
    "        post = trace.posterior.stack(sample=(\"chain\", \"draw\"))\n",
    "        return {\n",
    "            \"attack\": post[\"attack\"].values,\n",
    "            \"defense\": post[\"defense\"].values,\n",
    "            \"home_adv\": post[\"home_adv\"].values,\n",
    "            \"rho\": post[\"rho\"].values,\n",
    "            \"tmap\": tmap_fold,\n",
    "            \"teams\": teams_fold\n",
    "        }\n",
    "\n",
    "    def predict_dc_fold_proper(state, df_fold, max_goals=5):\n",
    "        if state is None:\n",
    "            return np.full((len(df_fold), 3), 1/3)\n",
    "        attack_s_fold = state[\"attack\"]\n",
    "        defense_s_fold = state[\"defense\"]\n",
    "        home_adv_s_fold = state[\"home_adv\"]\n",
    "        rho_s_fold = state[\"rho\"]\n",
    "        tmap_fold = state[\"tmap\"]\n",
    "        n_samples_fold = attack_s_fold.shape[0]\n",
    "        n_matches = len(df_fold)\n",
    "        out_probs = np.zeros((n_matches, 3))\n",
    "        for i, (_, row) in enumerate(df_fold.iterrows()):\n",
    "            home, away = row[\"HomeTeam\"], row[\"AwayTeam\"]\n",
    "            ih = tmap_fold.get(home)\n",
    "            ia = tmap_fold.get(away)\n",
    "            if ih is None or ia is None:\n",
    "                out_probs[i] = [1/3, 1/3, 1/3]\n",
    "                continue\n",
    "            M_avg = np.zeros((max_goals + 1, max_goals + 1))\n",
    "            for k in range(n_samples_fold):\n",
    "                lam_h_k = np.exp(home_adv_s_fold[k] + attack_s_fold[k, ih] - defense_s_fold[k, ia])\n",
    "                lam_a_k = np.exp(attack_s_fold[k, ia] - defense_s_fold[k, ih])\n",
    "                rho_k = rho_s_fold[k]\n",
    "                M_avg += dc_joint_matrix(lam_h_k, lam_a_k, rho_k, max_goals)\n",
    "            M_avg /= n_samples_fold\n",
    "            p_home = np.triu(M_avg, k=1).sum()\n",
    "            p_away = np.tril(M_avg, k=-1).sum()\n",
    "            p_draw = np.trace(M_avg)\n",
    "            tot = p_home + p_draw + p_away\n",
    "            out_probs[i] = [p_home/tot, p_draw/tot, p_away/tot]\n",
    "        return out_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb7c52",
   "metadata": {},
   "source": [
    "### Class weight adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cf76ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [tf oof] constructing meta_df past sequence\n",
      "Sequence feature number: 64\n",
      "Sequence feature : ['form_home', 'form_away', 'form_diff', 'form_home_v2', 'form_away_v2', 'form_diff_v2', 'home_home_form', 'away_away_form', 'win_streak_home', 'win_streak_away']...\n",
      " standardize past sequence\n",
      "Season 2000/2001: skip (no prior seasons)\n",
      "\n",
      "OOF season 2001/2002: train<= 2000/2001, val=2001/2002, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 15 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[184 101  95], class_weights={0: np.float64(0.6884057971014492), 1: np.float64(1.2541254125412542), 2: np.float64(1.3333333333333333)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 4.40]\n",
      "  home_seq range: [-9.97, 5.66]\n",
      "  away_seq range: [-9.97, 5.66]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1060 | train_acc=0.368\n",
      "         | val_logloss=1.1006\n",
      "         | train_pred_dist: H=0.39 D=0.18 A=0.43\n",
      "         | val_pred_dist:   H=0.36 D=0.13 A=0.51\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0552 | train_acc=0.487\n",
      "         | val_logloss=1.1213\n",
      "         | train_pred_dist: H=0.38 D=0.38 A=0.24\n",
      "         | val_pred_dist:   H=0.41 D=0.13 A=0.46\n",
      "Epoch 06 | lr=0.000500 | train_loss=0.9430 | train_acc=0.566\n",
      "         | val_logloss=1.1779\n",
      "         | train_pred_dist: H=0.41 D=0.33 A=0.26\n",
      "         | val_pred_dist:   H=0.51 D=0.24 A=0.26\n",
      "Early stopping at epoch 7\n",
      "\n",
      " Best val logloss: 1.1006\n",
      "  home_seq range: [-9.97, 5.66]\n",
      "  away_seq range: [-9.97, 5.66]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002']\n",
      "\n",
      "OOF season 2002/2003: train<= 2001/2002, val=2002/2003, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 18 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[349 202 209], class_weights={0: np.float64(0.725883476599809), 1: np.float64(1.2541254125412542), 2: np.float64(1.2121212121212122)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 4.08]\n",
      "  away_seq range: [-9.97, 4.08]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1262 | train_acc=0.355\n",
      "         | val_logloss=1.0675\n",
      "         | train_pred_dist: H=0.37 D=0.35 A=0.28\n",
      "         | val_pred_dist:   H=0.53 D=0.05 A=0.42\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0535 | train_acc=0.476\n",
      "         | val_logloss=1.1141\n",
      "         | train_pred_dist: H=0.47 D=0.21 A=0.32\n",
      "         | val_pred_dist:   H=0.42 D=0.28 A=0.29\n",
      "Epoch 06 | lr=0.000500 | train_loss=1.0145 | train_acc=0.482\n",
      "         | val_logloss=1.1081\n",
      "         | train_pred_dist: H=0.38 D=0.26 A=0.36\n",
      "         | val_pred_dist:   H=0.42 D=0.34 A=0.24\n",
      "Early stopping at epoch 7\n",
      "\n",
      " Best val logloss: 1.0675\n",
      "  home_seq range: [-9.97, 4.08]\n",
      "  away_seq range: [-9.97, 4.08]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003']\n",
      "\n",
      "OOF season 2003/2004: train<= 2002/2003, val=2003/2004, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 24 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[536 292 312], class_weights={0: np.float64(0.7089552238805971), 1: np.float64(1.3013698630136987), 2: np.float64(1.2179487179487178)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 6.01]\n",
      "  away_seq range: [-9.97, 6.01]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1297 | train_acc=0.383\n",
      "         | val_logloss=1.1067\n",
      "         | train_pred_dist: H=0.38 D=0.26 A=0.37\n",
      "         | val_pred_dist:   H=0.49 D=0.19 A=0.32\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0632 | train_acc=0.444\n",
      "         | val_logloss=1.1244\n",
      "         | train_pred_dist: H=0.40 D=0.29 A=0.31\n",
      "         | val_pred_dist:   H=0.37 D=0.19 A=0.44\n",
      "Epoch 06 | lr=0.000500 | train_loss=1.0211 | train_acc=0.468\n",
      "         | val_logloss=1.1326\n",
      "         | train_pred_dist: H=0.36 D=0.29 A=0.35\n",
      "         | val_pred_dist:   H=0.40 D=0.35 A=0.24\n",
      "Early stopping at epoch 7\n",
      "\n",
      " Best val logloss: 1.1067\n",
      "  home_seq range: [-9.97, 6.01]\n",
      "  away_seq range: [-9.97, 6.01]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004']\n",
      "\n",
      "OOF season 2004/2005: train<= 2003/2004, val=2004/2005, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 30 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[703 400 417], class_weights={0: np.float64(0.7207207207207207), 1: np.float64(1.2666666666666666), 2: np.float64(1.2150279776179056)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 3.85]\n",
      "  away_seq range: [-9.97, 3.85]\n",
      "  match_feat range: [-5.58, 3.41]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1126 | train_acc=0.364\n",
      "         | val_logloss=1.1037\n",
      "         | train_pred_dist: H=0.41 D=0.28 A=0.32\n",
      "         | val_pred_dist:   H=0.39 D=0.04 A=0.57\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0834 | train_acc=0.418\n",
      "         | val_logloss=1.1005\n",
      "         | train_pred_dist: H=0.42 D=0.29 A=0.29\n",
      "         | val_pred_dist:   H=0.59 D=0.25 A=0.16\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0555 | train_acc=0.434\n",
      "         | val_logloss=1.1229\n",
      "         | train_pred_dist: H=0.37 D=0.33 A=0.30\n",
      "         | val_pred_dist:   H=0.52 D=0.13 A=0.35\n",
      "Epoch 09 | lr=0.000500 | train_loss=0.9936 | train_acc=0.496\n",
      "         | val_logloss=1.1961\n",
      "         | train_pred_dist: H=0.37 D=0.29 A=0.34\n",
      "         | val_pred_dist:   H=0.47 D=0.28 A=0.25\n",
      "Early stopping at epoch 9\n",
      "\n",
      " Best val logloss: 1.1005\n",
      "  home_seq range: [-9.97, 3.85]\n",
      "  away_seq range: [-9.97, 3.85]\n",
      "  match_feat range: [-5.58, 3.41]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005']\n",
      "\n",
      "OOF season 2005/2006: train<= 2004/2005, val=2005/2006, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 30 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[876 510 514], class_weights={0: np.float64(0.7229832572298326), 1: np.float64(1.2418300653594772), 2: np.float64(1.2321660181582361)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 4.55]\n",
      "  away_seq range: [-9.97, 4.55]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1166 | train_acc=0.349\n",
      "         | val_logloss=1.0839\n",
      "         | train_pred_dist: H=0.37 D=0.28 A=0.35\n",
      "         | val_pred_dist:   H=0.43 D=0.29 A=0.28\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0866 | train_acc=0.399\n",
      "         | val_logloss=1.0675\n",
      "         | train_pred_dist: H=0.38 D=0.27 A=0.35\n",
      "         | val_pred_dist:   H=0.40 D=0.14 A=0.46\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0585 | train_acc=0.456\n",
      "         | val_logloss=1.0980\n",
      "         | train_pred_dist: H=0.40 D=0.30 A=0.30\n",
      "         | val_pred_dist:   H=0.32 D=0.18 A=0.50\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0071 | train_acc=0.479\n",
      "         | val_logloss=1.1168\n",
      "         | train_pred_dist: H=0.34 D=0.31 A=0.35\n",
      "         | val_pred_dist:   H=0.33 D=0.24 A=0.43\n",
      "Early stopping at epoch 9\n",
      "\n",
      " Best val logloss: 1.0675\n",
      "  home_seq range: [-9.97, 4.55]\n",
      "  away_seq range: [-9.97, 4.55]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006']\n",
      "\n",
      "OOF season 2006/2007: train<= 2005/2006, val=2006/2007, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 35 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[1068  587  625], class_weights={0: np.float64(0.7116104868913857), 1: np.float64(1.2947189097103917), 2: np.float64(1.216)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 4.55]\n",
      "  away_seq range: [-9.97, 3.63]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1145 | train_acc=0.361\n",
      "         | val_logloss=1.0901\n",
      "         | train_pred_dist: H=0.38 D=0.31 A=0.31\n",
      "         | val_pred_dist:   H=0.36 D=0.52 A=0.13\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0819 | train_acc=0.432\n",
      "         | val_logloss=1.1250\n",
      "         | train_pred_dist: H=0.40 D=0.24 A=0.37\n",
      "         | val_pred_dist:   H=0.20 D=0.47 A=0.33\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0566 | train_acc=0.422\n",
      "         | val_logloss=1.0864\n",
      "         | train_pred_dist: H=0.35 D=0.30 A=0.36\n",
      "         | val_pred_dist:   H=0.52 D=0.31 A=0.18\n",
      "Epoch 09 | lr=0.001000 | train_loss=1.0385 | train_acc=0.464\n",
      "         | val_logloss=1.1692\n",
      "         | train_pred_dist: H=0.38 D=0.30 A=0.32\n",
      "         | val_pred_dist:   H=0.19 D=0.51 A=0.30\n",
      "Epoch 12 | lr=0.000500 | train_loss=0.9935 | train_acc=0.487\n",
      "         | val_logloss=1.1553\n",
      "         | train_pred_dist: H=0.34 D=0.29 A=0.37\n",
      "         | val_pred_dist:   H=0.31 D=0.42 A=0.28\n",
      "Early stopping at epoch 12\n",
      "\n",
      " Best val logloss: 1.0864\n",
      "  home_seq range: [-9.97, 4.55]\n",
      "  away_seq range: [-9.97, 3.63]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007']\n",
      "\n",
      "OOF season 2007/2008: train<= 2006/2007, val=2007/2008, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 39 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[1250  685  725], class_weights={0: np.float64(0.7093333333333334), 1: np.float64(1.294403892944039), 2: np.float64(1.2229885057471264)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-4.42, 4.48]\n",
      "  away_seq range: [-4.42, 4.48]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1198 | train_acc=0.342\n",
      "         | val_logloss=1.0613\n",
      "         | train_pred_dist: H=0.31 D=0.32 A=0.36\n",
      "         | val_pred_dist:   H=0.38 D=0.05 A=0.57\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0848 | train_acc=0.418\n",
      "         | val_logloss=1.0796\n",
      "         | train_pred_dist: H=0.38 D=0.20 A=0.42\n",
      "         | val_pred_dist:   H=0.13 D=0.29 A=0.58\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0651 | train_acc=0.414\n",
      "         | val_logloss=1.0602\n",
      "         | train_pred_dist: H=0.29 D=0.34 A=0.37\n",
      "         | val_pred_dist:   H=0.49 D=0.27 A=0.24\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0585 | train_acc=0.453\n",
      "         | val_logloss=1.0529\n",
      "         | train_pred_dist: H=0.36 D=0.29 A=0.35\n",
      "         | val_pred_dist:   H=0.41 D=0.24 A=0.35\n",
      "Early stopping at epoch 11\n",
      "\n",
      " Best val logloss: 1.0258\n",
      "  home_seq range: [-4.42, 4.48]\n",
      "  away_seq range: [-4.42, 4.48]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008']\n",
      "\n",
      "OOF season 2008/2009: train<= 2007/2008, val=2008/2009, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 50 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[1426  785  829], class_weights={0: np.float64(0.7106124357176251), 1: np.float64(1.2908704883227176), 2: np.float64(1.222356252513068)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 4.44]\n",
      "  away_seq range: [-9.97, 4.44]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1032 | train_acc=0.388\n",
      "         | val_logloss=1.0692\n",
      "         | train_pred_dist: H=0.35 D=0.31 A=0.34\n",
      "         | val_pred_dist:   H=0.68 D=0.02 A=0.30\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0814 | train_acc=0.412\n",
      "         | val_logloss=1.0724\n",
      "         | train_pred_dist: H=0.35 D=0.24 A=0.42\n",
      "         | val_pred_dist:   H=0.41 D=0.46 A=0.13\n",
      "Epoch 06 | lr=0.000500 | train_loss=1.0575 | train_acc=0.441\n",
      "         | val_logloss=1.0949\n",
      "         | train_pred_dist: H=0.38 D=0.29 A=0.34\n",
      "         | val_pred_dist:   H=0.44 D=0.16 A=0.41\n",
      "Early stopping at epoch 7\n",
      "\n",
      " Best val logloss: 1.0692\n",
      "  home_seq range: [-9.97, 4.44]\n",
      "  away_seq range: [-9.97, 4.44]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009']\n",
      "\n",
      "OOF season 2009/2010: train<= 2008/2009, val=2009/2010, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 48 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[1599  882  939], class_weights={0: np.float64(0.7129455909943715), 1: np.float64(1.2925170068027212), 2: np.float64(1.2140575079872205)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 4.44]\n",
      "  away_seq range: [-9.97, 4.44]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1062 | train_acc=0.389\n",
      "         | val_logloss=1.0536\n",
      "         | train_pred_dist: H=0.39 D=0.28 A=0.33\n",
      "         | val_pred_dist:   H=0.51 D=0.19 A=0.30\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0821 | train_acc=0.423\n",
      "         | val_logloss=1.0689\n",
      "         | train_pred_dist: H=0.38 D=0.24 A=0.38\n",
      "         | val_pred_dist:   H=0.33 D=0.33 A=0.34\n",
      "Epoch 06 | lr=0.000500 | train_loss=1.0644 | train_acc=0.439\n",
      "         | val_logloss=1.0855\n",
      "         | train_pred_dist: H=0.37 D=0.25 A=0.38\n",
      "         | val_pred_dist:   H=0.29 D=0.49 A=0.22\n",
      "Early stopping at epoch 7\n",
      "\n",
      " Best val logloss: 1.0536\n",
      "  home_seq range: [-9.97, 4.44]\n",
      "  away_seq range: [-9.97, 4.44]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010']\n",
      "\n",
      "OOF season 2010/2011: train<= 2009/2010, val=2010/2011, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 48 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[1792  978 1030], class_weights={0: np.float64(0.7068452380952381), 1: np.float64(1.2951601908657124), 2: np.float64(1.2297734627831716)}\n",
      "  home_seq range: [-9.97, 6.18]\n",
      "  away_seq range: [-9.97, 6.18]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 6.90]\n",
      "  away_seq range: [-9.97, 6.90]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1038 | train_acc=0.394\n",
      "         | val_logloss=1.1202\n",
      "         | train_pred_dist: H=0.35 D=0.31 A=0.34\n",
      "         | val_pred_dist:   H=0.44 D=0.03 A=0.53\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0842 | train_acc=0.393\n",
      "         | val_logloss=1.0978\n",
      "         | train_pred_dist: H=0.34 D=0.28 A=0.38\n",
      "         | val_pred_dist:   H=0.62 D=0.05 A=0.33\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0734 | train_acc=0.429\n",
      "         | val_logloss=1.1373\n",
      "         | train_pred_dist: H=0.42 D=0.22 A=0.35\n",
      "         | val_pred_dist:   H=0.26 D=0.51 A=0.23\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0476 | train_acc=0.458\n",
      "         | val_logloss=1.1857\n",
      "         | train_pred_dist: H=0.38 D=0.26 A=0.36\n",
      "         | val_pred_dist:   H=0.33 D=0.46 A=0.21\n",
      "Early stopping at epoch 9\n",
      "\n",
      " Best val logloss: 1.0978\n",
      "  home_seq range: [-9.97, 6.90]\n",
      "  away_seq range: [-9.97, 6.90]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011']\n",
      "\n",
      "OOF season 2011/2012: train<= 2010/2011, val=2011/2012, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 53 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[1971 1089 1120], class_weights={0: np.float64(0.7069169626247251), 1: np.float64(1.2794612794612794), 2: np.float64(1.244047619047619)}\n",
      "  home_seq range: [-9.97, 6.90]\n",
      "  away_seq range: [-9.97, 6.90]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1047 | train_acc=0.380\n",
      "         | val_logloss=1.0695\n",
      "         | train_pred_dist: H=0.38 D=0.25 A=0.37\n",
      "         | val_pred_dist:   H=0.33 D=0.36 A=0.31\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0846 | train_acc=0.416\n",
      "         | val_logloss=1.0661\n",
      "         | train_pred_dist: H=0.37 D=0.28 A=0.35\n",
      "         | val_pred_dist:   H=0.50 D=0.16 A=0.34\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0724 | train_acc=0.419\n",
      "         | val_logloss=1.1006\n",
      "         | train_pred_dist: H=0.38 D=0.24 A=0.39\n",
      "         | val_pred_dist:   H=0.33 D=0.46 A=0.21\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0481 | train_acc=0.462\n",
      "         | val_logloss=1.0994\n",
      "         | train_pred_dist: H=0.38 D=0.26 A=0.37\n",
      "         | val_pred_dist:   H=0.43 D=0.29 A=0.28\n",
      "Early stopping at epoch 9\n",
      "\n",
      " Best val logloss: 1.0661\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012']\n",
      "\n",
      "OOF season 2012/2013: train<= 2011/2012, val=2012/2013, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 59 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[2142 1182 1236], class_weights={0: np.float64(0.7096171802054155), 1: np.float64(1.2859560067681894), 2: np.float64(1.2297734627831716)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-4.91, 4.96]\n",
      "  away_seq range: [-4.91, 4.96]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1045 | train_acc=0.375\n",
      "         | val_logloss=1.0701\n",
      "         | train_pred_dist: H=0.37 D=0.28 A=0.35\n",
      "         | val_pred_dist:   H=0.52 D=0.24 A=0.23\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0865 | train_acc=0.395\n",
      "         | val_logloss=1.0656\n",
      "         | train_pred_dist: H=0.35 D=0.24 A=0.41\n",
      "         | val_pred_dist:   H=0.53 D=0.29 A=0.18\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0751 | train_acc=0.416\n",
      "         | val_logloss=1.0803\n",
      "         | train_pred_dist: H=0.36 D=0.26 A=0.38\n",
      "         | val_pred_dist:   H=0.43 D=0.14 A=0.43\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0540 | train_acc=0.449\n",
      "         | val_logloss=1.1041\n",
      "         | train_pred_dist: H=0.40 D=0.25 A=0.35\n",
      "         | val_pred_dist:   H=0.41 D=0.25 A=0.34\n",
      "Early stopping at epoch 9\n",
      "\n",
      " Best val logloss: 1.0656\n",
      "  home_seq range: [-4.91, 4.96]\n",
      "  away_seq range: [-4.91, 4.96]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013']\n",
      "\n",
      "OOF season 2013/2014: train<= 2012/2013, val=2013/2014, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 64 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[2308 1290 1342], class_weights={0: np.float64(0.7134604274985558), 1: np.float64(1.276485788113695), 2: np.float64(1.22702434177844)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 4.75]\n",
      "  away_seq range: [-9.97, 4.75]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1006 | train_acc=0.382\n",
      "         | val_logloss=1.0735\n",
      "         | train_pred_dist: H=0.39 D=0.25 A=0.36\n",
      "         | val_pred_dist:   H=0.61 D=0.11 A=0.28\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0835 | train_acc=0.408\n",
      "         | val_logloss=1.0898\n",
      "         | train_pred_dist: H=0.38 D=0.31 A=0.31\n",
      "         | val_pred_dist:   H=0.43 D=0.27 A=0.30\n",
      "Epoch 06 | lr=0.000500 | train_loss=1.0699 | train_acc=0.438\n",
      "         | val_logloss=1.0784\n",
      "         | train_pred_dist: H=0.43 D=0.19 A=0.38\n",
      "         | val_pred_dist:   H=0.44 D=0.09 A=0.47\n",
      "Early stopping at epoch 7\n",
      "\n",
      " Best val logloss: 1.0735\n",
      "  home_seq range: [-9.97, 4.75]\n",
      "  away_seq range: [-9.97, 4.75]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014']\n",
      "\n",
      "OOF season 2014/2015: train<= 2013/2014, val=2014/2015, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 92 seconds.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[2487 1368 1465], class_weights={0: np.float64(0.7130411472992897), 1: np.float64(1.2962962962962963), 2: np.float64(1.2104664391353812)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-3.79, 3.80]\n",
      "  away_seq range: [-3.79, 3.80]\n",
      "  match_feat range: [-3.19, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1020 | train_acc=0.386\n",
      "         | val_logloss=1.0808\n",
      "         | train_pred_dist: H=0.41 D=0.23 A=0.36\n",
      "         | val_pred_dist:   H=0.36 D=0.34 A=0.31\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0849 | train_acc=0.404\n",
      "         | val_logloss=1.1108\n",
      "         | train_pred_dist: H=0.41 D=0.24 A=0.36\n",
      "         | val_pred_dist:   H=0.39 D=0.03 A=0.58\n",
      "Epoch 06 | lr=0.000500 | train_loss=1.0731 | train_acc=0.421\n",
      "         | val_logloss=1.0832\n",
      "         | train_pred_dist: H=0.37 D=0.26 A=0.37\n",
      "         | val_pred_dist:   H=0.32 D=0.20 A=0.48\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0631 | train_acc=0.442\n",
      "         | val_logloss=1.0929\n",
      "         | train_pred_dist: H=0.40 D=0.27 A=0.34\n",
      "         | val_pred_dist:   H=0.52 D=0.04 A=0.44\n",
      "Epoch 12 | lr=0.000250 | train_loss=1.0478 | train_acc=0.456\n",
      "         | val_logloss=1.0779\n",
      "         | train_pred_dist: H=0.40 D=0.27 A=0.34\n",
      "         | val_pred_dist:   H=0.47 D=0.08 A=0.46\n",
      "Early stopping at epoch 14\n",
      "\n",
      " Best val logloss: 1.0700\n",
      "  home_seq range: [-3.79, 3.80]\n",
      "  away_seq range: [-3.79, 3.80]\n",
      "  match_feat range: [-3.19, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015']\n",
      "\n",
      "OOF season 2015/2016: train<= 2014/2015, val=2015/2016, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 73 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[2659 1461 1580], class_weights={0: np.float64(0.7145543437382474), 1: np.float64(1.3004791238877482), 2: np.float64(1.2025316455696202)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 3.55]\n",
      "  away_seq range: [-9.97, 3.55]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1019 | train_acc=0.386\n",
      "         | val_logloss=1.0852\n",
      "         | train_pred_dist: H=0.37 D=0.32 A=0.31\n",
      "         | val_pred_dist:   H=0.74 D=0.01 A=0.25\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0879 | train_acc=0.412\n",
      "         | val_logloss=1.0978\n",
      "         | train_pred_dist: H=0.43 D=0.20 A=0.37\n",
      "         | val_pred_dist:   H=0.38 D=0.16 A=0.46\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0791 | train_acc=0.425\n",
      "         | val_logloss=1.0842\n",
      "         | train_pred_dist: H=0.44 D=0.22 A=0.34\n",
      "         | val_pred_dist:   H=0.54 D=0.15 A=0.31\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0727 | train_acc=0.436\n",
      "         | val_logloss=1.0808\n",
      "         | train_pred_dist: H=0.41 D=0.23 A=0.36\n",
      "         | val_pred_dist:   H=0.54 D=0.08 A=0.38\n",
      "Early stopping at epoch 11\n",
      "\n",
      " Best val logloss: 1.0808\n",
      "  home_seq range: [-9.97, 3.55]\n",
      "  away_seq range: [-9.97, 3.55]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016']\n",
      "\n",
      "OOF season 2016/2017: train<= 2015/2016, val=2016/2017, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 74 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[2816 1568 1696], class_weights={0: np.float64(0.7196969696969697), 1: np.float64(1.2925170068027212), 2: np.float64(1.1949685534591195)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-3.72, 4.05]\n",
      "  away_seq range: [-3.72, 4.05]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1023 | train_acc=0.386\n",
      "         | val_logloss=1.0933\n",
      "         | train_pred_dist: H=0.40 D=0.26 A=0.34\n",
      "         | val_pred_dist:   H=0.36 D=0.30 A=0.33\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0859 | train_acc=0.414\n",
      "         | val_logloss=1.0852\n",
      "         | train_pred_dist: H=0.43 D=0.24 A=0.34\n",
      "         | val_pred_dist:   H=0.41 D=0.11 A=0.48\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0795 | train_acc=0.417\n",
      "         | val_logloss=1.0820\n",
      "         | train_pred_dist: H=0.39 D=0.28 A=0.33\n",
      "         | val_pred_dist:   H=0.34 D=0.32 A=0.33\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0721 | train_acc=0.426\n",
      "         | val_logloss=1.1059\n",
      "         | train_pred_dist: H=0.39 D=0.26 A=0.34\n",
      "         | val_pred_dist:   H=0.29 D=0.18 A=0.53\n",
      "Early stopping at epoch 11\n",
      "\n",
      " Best val logloss: 1.0777\n",
      "  home_seq range: [-3.72, 4.05]\n",
      "  away_seq range: [-3.72, 4.05]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017']\n",
      "\n",
      "OOF season 2017/2018: train<= 2016/2017, val=2017/2018, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 85 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[3003 1652 1805], class_weights={0: np.float64(0.7170607170607171), 1: np.float64(1.3034705407586764), 2: np.float64(1.1929824561403508)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 5.35]\n",
      "  away_seq range: [-9.97, 5.35]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1018 | train_acc=0.385\n",
      "         | val_logloss=1.0800\n",
      "         | train_pred_dist: H=0.40 D=0.29 A=0.31\n",
      "         | val_pred_dist:   H=0.44 D=0.10 A=0.47\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0852 | train_acc=0.406\n",
      "         | val_logloss=1.0693\n",
      "         | train_pred_dist: H=0.40 D=0.23 A=0.37\n",
      "         | val_pred_dist:   H=0.72 D=0.03 A=0.25\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0810 | train_acc=0.422\n",
      "         | val_logloss=1.0842\n",
      "         | train_pred_dist: H=0.42 D=0.21 A=0.36\n",
      "         | val_pred_dist:   H=0.44 D=0.02 A=0.54\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0691 | train_acc=0.447\n",
      "         | val_logloss=1.0695\n",
      "         | train_pred_dist: H=0.42 D=0.22 A=0.36\n",
      "         | val_pred_dist:   H=0.63 D=0.04 A=0.32\n",
      "Early stopping at epoch 9\n",
      "\n",
      " Best val logloss: 1.0693\n",
      "  home_seq range: [-9.97, 5.35]\n",
      "  away_seq range: [-9.97, 5.35]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017', '2017/2018']\n",
      "\n",
      "OOF season 2018/2019: train<= 2017/2018, val=2018/2019, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 94 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[3176 1751 1913], class_weights={0: np.float64(0.7178841309823678), 1: np.float64(1.302113078241005), 2: np.float64(1.1918452692106638)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-5.19, 5.26]\n",
      "  away_seq range: [-5.19, 5.26]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.0982 | train_acc=0.385\n",
      "         | val_logloss=1.1008\n",
      "         | train_pred_dist: H=0.39 D=0.27 A=0.34\n",
      "         | val_pred_dist:   H=0.39 D=0.53 A=0.08\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0848 | train_acc=0.419\n",
      "         | val_logloss=1.0810\n",
      "         | train_pred_dist: H=0.43 D=0.18 A=0.38\n",
      "         | val_pred_dist:   H=0.36 D=0.01 A=0.63\n",
      "Epoch 06 | lr=0.000500 | train_loss=1.0803 | train_acc=0.417\n",
      "         | val_logloss=1.0711\n",
      "         | train_pred_dist: H=0.41 D=0.22 A=0.38\n",
      "         | val_pred_dist:   H=0.49 D=0.02 A=0.49\n",
      "Early stopping at epoch 8\n",
      "\n",
      " Best val logloss: 1.0682\n",
      "  home_seq range: [-5.19, 5.26]\n",
      "  away_seq range: [-5.19, 5.26]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017', '2017/2018', '2018/2019']\n",
      "\n",
      "OOF season 2019/2020: train<= 2018/2019, val=2019/2020, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 78 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[3357 1822 2041], class_weights={0: np.float64(0.7169099394300467), 1: np.float64(1.3208927918038784), 2: np.float64(1.179160542217867)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-4.14, 5.26]\n",
      "  away_seq range: [-4.14, 5.26]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1004 | train_acc=0.395\n",
      "         | val_logloss=1.0799\n",
      "         | train_pred_dist: H=0.41 D=0.27 A=0.32\n",
      "         | val_pred_dist:   H=0.71 D=0.01 A=0.28\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0868 | train_acc=0.406\n",
      "         | val_logloss=1.0842\n",
      "         | train_pred_dist: H=0.40 D=0.21 A=0.38\n",
      "         | val_pred_dist:   H=0.32 D=0.36 A=0.32\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0816 | train_acc=0.413\n",
      "         | val_logloss=1.0700\n",
      "         | train_pred_dist: H=0.40 D=0.21 A=0.39\n",
      "         | val_pred_dist:   H=0.61 D=0.01 A=0.38\n",
      "Epoch 09 | lr=0.001000 | train_loss=1.0792 | train_acc=0.420\n",
      "         | val_logloss=1.0904\n",
      "         | train_pred_dist: H=0.40 D=0.26 A=0.34\n",
      "         | val_pred_dist:   H=0.35 D=0.36 A=0.29\n",
      "Epoch 12 | lr=0.000500 | train_loss=1.0648 | train_acc=0.452\n",
      "         | val_logloss=1.1008\n",
      "         | train_pred_dist: H=0.44 D=0.25 A=0.32\n",
      "         | val_pred_dist:   H=0.42 D=0.23 A=0.35\n",
      "Early stopping at epoch 12\n",
      "\n",
      " Best val logloss: 1.0700\n",
      "  home_seq range: [-4.14, 5.26]\n",
      "  away_seq range: [-4.14, 5.26]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017', '2017/2018', '2018/2019', '2019/2020']\n",
      "\n",
      "OOF season 2020/2021: train<= 2019/2020, val=2020/2021, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 80 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[3529 1914 2157], class_weights={0: np.float64(0.7178615282894115), 1: np.float64(1.3235806339254614), 2: np.float64(1.1744707154999228)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-3.72, 4.43]\n",
      "  away_seq range: [-3.65, 4.43]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.1020 | train_acc=0.385\n",
      "         | val_logloss=1.0766\n",
      "         | train_pred_dist: H=0.37 D=0.27 A=0.37\n",
      "         | val_pred_dist:   H=0.67 D=0.01 A=0.32\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0866 | train_acc=0.404\n",
      "         | val_logloss=1.0870\n",
      "         | train_pred_dist: H=0.38 D=0.26 A=0.35\n",
      "         | val_pred_dist:   H=0.49 D=0.23 A=0.28\n",
      "Epoch 06 | lr=0.000500 | train_loss=1.0778 | train_acc=0.442\n",
      "         | val_logloss=1.0791\n",
      "         | train_pred_dist: H=0.47 D=0.21 A=0.32\n",
      "         | val_pred_dist:   H=0.51 D=0.14 A=0.35\n",
      "Early stopping at epoch 7\n",
      "\n",
      " Best val logloss: 1.0766\n",
      "  home_seq range: [-3.72, 4.43]\n",
      "  away_seq range: [-3.65, 4.43]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021']\n",
      "\n",
      "OOF season 2021/2022: train<= 2020/2021, val=2021/2022, n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 86 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DC fold trained independently\n",
      "[TF fold] class_counts=[3673 1997 2310], class_weights={0: np.float64(0.7242036482439422), 1: np.float64(1.3319979969954931), 2: np.float64(1.1515151515151516)}\n",
      "  home_seq range: [-9.97, 7.30]\n",
      "  away_seq range: [-9.97, 7.30]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 4.74]\n",
      "  away_seq range: [-9.97, 4.43]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=64, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.0990 | train_acc=0.381\n",
      "         | val_logloss=1.0738\n",
      "         | train_pred_dist: H=0.37 D=0.27 A=0.36\n",
      "         | val_pred_dist:   H=0.30 D=0.28 A=0.42\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0858 | train_acc=0.409\n",
      "         | val_logloss=1.0876\n",
      "         | train_pred_dist: H=0.39 D=0.23 A=0.38\n",
      "         | val_pred_dist:   H=0.38 D=0.46 A=0.16\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0819 | train_acc=0.426\n",
      "         | val_logloss=1.0710\n",
      "         | train_pred_dist: H=0.42 D=0.20 A=0.38\n",
      "         | val_pred_dist:   H=0.41 D=0.00 A=0.59\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0792 | train_acc=0.435\n",
      "         | val_logloss=1.0783\n",
      "         | train_pred_dist: H=0.44 D=0.21 A=0.35\n",
      "         | val_pred_dist:   H=0.35 D=0.11 A=0.54\n",
      "Early stopping at epoch 11\n",
      "\n",
      " Best val logloss: 1.0605\n",
      "  home_seq range: [-9.97, 4.74]\n",
      "  away_seq range: [-9.97, 4.43]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "checkpoint updated，completed season: ['2000/2001', '2001/2002', '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007', '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021', '2021/2022']\n",
      " OOF built (with proper DC fold training): (7980, 176)\n",
      " ready to start stacking.\n"
     ]
    }
   ],
   "source": [
    "if TORCH_OK and DO_TF_OOF:\n",
    "\n",
    "    def fit_tf_fold(train_fold, val_fold, epochs=25, patience=6):\n",
    "\n",
    "        y_tr = train_fold[\"y\"].astype(int).values\n",
    "        cc = np.bincount(y_tr, minlength=3)\n",
    "        T  = cc.sum()\n",
    "        class_weights = {\n",
    "            0: T / (3 * cc[0]),        # H\n",
    "            1: T / (3 * cc[1]) * 1,  # D \n",
    "            2: T / (3 * cc[2]),        # A\n",
    "        }\n",
    "        print(f\"[TF fold] class_counts={cc}, class_weights={class_weights}\")\n",
    "\n",
    "        train_ds = EnhancedMatchDataset(train_fold, class_weights=class_weights)\n",
    "        val_ds   = EnhancedMatchDataset(val_fold,   class_weights=class_weights)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, pin_memory=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, pin_memory=True)\n",
    "\n",
    "        seq_len_tf   = train_ds.home_seq.shape[1]\n",
    "        feat_dim_tf  = train_ds.home_seq.shape[2]\n",
    "        match_dim_tf = train_ds.match_feat.shape[1]\n",
    "\n",
    "        print(f\"[TF fold] seq_len={seq_len_tf}, feat_dim={feat_dim_tf}, match_feat_dim={match_dim_tf}\")\n",
    "\n",
    "        model = EnhancedMatchTransformer(\n",
    "            seq_len=seq_len_tf,\n",
    "            feat_dim=feat_dim_tf,\n",
    "            match_feat_dim=match_dim_tf,\n",
    "            d_model=64,\n",
    "            nhead=4,\n",
    "            num_layers=2,\n",
    "            ff_dim=128,\n",
    "            dropout=0.3\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    " # ⑤ ( sample_weights etc)\n",
    "        model = train_enhanced_tf(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            class_weights=class_weights,\n",
    "            epochs=epochs,\n",
    "            patience=patience\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict_tf_fold(model, df_fold):\n",
    "        ds = EnhancedMatchDataset(df_fold, class_weights=None)  # \n",
    "        loader = DataLoader(\n",
    "            ds,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for hs, asq, mfeat, yb, sw in loader:\n",
    "                hs    = hs.to(DEVICE)\n",
    "                asq   = asq.to(DEVICE)\n",
    "                mfeat = mfeat.to(DEVICE)\n",
    "\n",
    "                hs    = torch.nan_to_num(hs,    nan=0.0)\n",
    "                asq   = torch.nan_to_num(asq,   nan=0.0)\n",
    "                mfeat = torch.nan_to_num(mfeat, nan=0.0)\n",
    "\n",
    "                logits = model(hs, asq, mfeat)\n",
    "                prob = torch.softmax(logits, dim=1)\n",
    "                prob = torch.clamp(prob, 1e-7, 1 - 1e-7)\n",
    "                probs.append(prob.cpu().numpy())\n",
    "\n",
    "        proba = np.vstack(probs)\n",
    "        proba = proba / proba.sum(axis=1, keepdims=True)\n",
    "        return proba\n",
    "\n",
    " # run_enhanced_tf \n",
    "    safe_features_tf = [\n",
    "        f for f in feature_cols_xgb\n",
    "        if 'pm' in f.lower()\n",
    "        or 'form' in f.lower()\n",
    "        or 'elo' in f.lower()\n",
    "        or 'position' in f.lower()\n",
    "        or 'points' in f.lower()\n",
    "        or 'l10' in f.lower()\n",
    "        or 'win_streak' in f.lower()\n",
    "        or 'unbeaten' in f.lower()\n",
    "    ]\n",
    "    \n",
    "    if len(safe_features_tf) < 10:\n",
    "        print(f\" lack of tf oof secure features, using all xgb features.\")\n",
    "        safe_features_tf = feature_cols_xgb\n",
    "\n",
    "    print(f\" [tf oof] constructing meta_df past sequence\")\n",
    "    meta_df_with_seq, tf_feat_dim = build_team_sequences_fixed(\n",
    "        meta_df, safe_features_tf, seq_len=5\n",
    "    )\n",
    "    \n",
    "    #  meta_df\n",
    "    for col in [\"home_form_seq\", \"away_form_seq\", \"match_features\"]:\n",
    "        if col in meta_df.columns:\n",
    "            meta_df.drop(columns=[col], inplace=True)\n",
    "        meta_df[col] = meta_df_with_seq[col]\n",
    "    print(f\" standardize past sequence\")\n",
    "\n",
    " # standardize_sequences , val/test,\n",
    " # train=val=meta_df ,.\n",
    "    meta_tmp_train = meta_df.copy()\n",
    "    meta_tmp_val   = meta_df.copy()\n",
    "\n",
    "    meta_tmp_train, meta_tmp_val, _, mu_seq_tf, sd_seq_tf, mu_match_tf, sd_match_tf = \\\n",
    "        standardize_sequences(meta_tmp_train, meta_tmp_val, None)\n",
    "\n",
    " # result meta_df\n",
    "    meta_df[\"home_form_seq\"]  = meta_tmp_train[\"home_form_seq\"]\n",
    "    meta_df[\"away_form_seq\"]  = meta_tmp_train[\"away_form_seq\"]\n",
    "    meta_df[\"match_features\"] = meta_tmp_train[\"match_features\"]\n",
    "\n",
    "    for s in seasons_sorted:\n",
    "        if s in done_seasons:\n",
    "            print(f\"Season {s}: has been completed, skip\")\n",
    "            continue\n",
    "\n",
    "        val_fold = meta_df[meta_df[\"Season\"] == s].copy()\n",
    "        train_fold = meta_df[meta_df[\"Season\"].isin([ss for ss in seasons_sorted if ss < s])].copy()\n",
    "        if len(train_fold) == 0:\n",
    "            print(f\"Season {s}: skip (no prior seasons)\")\n",
    "            done_seasons.add(s)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nOOF season {s}: train<= {train_fold['Season'].max()}, val={s}, n_val={len(val_fold)}\")\n",
    "\n",
    "        # XGB\n",
    "        xgb_fold = fit_xgb_fold(train_fold)\n",
    "        X_val = val_fold[feature_cols_xgb].to_numpy(dtype=np.float32)\n",
    "        proba_val_xgb_fold = xgb_fold.predict_proba(X_val)\n",
    "        \n",
    "        # DC\n",
    "        if DO_DC_OOF_PROPER:\n",
    "            try:\n",
    "                dc_state = fit_dc_fold_proper(train_fold, draws=600, tune=300)\n",
    "                proba_val_dc_fold = predict_dc_fold_proper(dc_state, val_fold)\n",
    "                print(\"  DC fold trained independently\")\n",
    "            except Exception as e:\n",
    "                print(f\"  DC fold failed: {e}, using global posterior\")\n",
    "                \n",
    " # ✅ val_fold DC \n",
    "                X_dc_val_fold = dc_scaler.transform(\n",
    "                    val_fold[dc_feature_cols].copy().fillna(0.0)\n",
    "                )\n",
    "                \n",
    "                proba_val_dc_fold, _ = bayes_dc_predict_full(\n",
    "                    val_fold,\n",
    "                    X_dc_val_fold,      # ✅\n",
    "                    attack_s,\n",
    "                    defense_s,\n",
    "                    home_adv_s,\n",
    "                    rho_s,\n",
    "                    beta_h_s,\n",
    "                    beta_a_s,\n",
    "                    tmap,\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            print(f\"  DC fold failed: {e}, using global posterior\")\n",
    "            \n",
    " # ✅ val_fold DC \n",
    "            X_dc_val_fold = dc_scaler.transform(\n",
    "                val_fold[dc_feature_cols].copy().fillna(0.0)\n",
    "            )\n",
    "            \n",
    "            proba_val_dc_fold, _ = bayes_dc_predict_full(\n",
    "                val_fold,\n",
    "                X_dc_val_fold,      # ✅\n",
    "                attack_s,\n",
    "                defense_s,\n",
    "                home_adv_s,\n",
    "                rho_s,\n",
    "                beta_h_s,\n",
    "                beta_a_s,\n",
    "                tmap,\n",
    "            )\n",
    "\n",
    "        # TF\n",
    "        if TORCH_OK and DO_TF_OOF:\n",
    "            tf_fold = fit_tf_fold(train_fold, val_fold)\n",
    "            proba_val_tf_fold = predict_tf_fold(tf_fold, val_fold)\n",
    "        else:\n",
    "            proba_val_tf_fold = np.full_like(proba_val_xgb_fold, 1/3)\n",
    "\n",
    "        #  OOF\n",
    "        for oid, px, pd_, pt_ in zip(\n",
    "            val_fold[\"_orig_idx\"].values,\n",
    "            proba_val_xgb_fold,\n",
    "            proba_val_dc_fold,\n",
    "            proba_val_tf_fold\n",
    "        ):\n",
    "            pos = orig_to_pos[oid]\n",
    "            oof_xgb[pos] = px\n",
    "            oof_dc[pos]  = pd_\n",
    "            oof_tf[pos]  = pt_\n",
    "\n",
    " # ✅, checkpoint once\n",
    "        done_seasons.add(s)\n",
    "        ckpt_to_save = {\n",
    "            \"train_df\":       train_df,\n",
    "            \"val_df\":         val_df,\n",
    "            \"meta_df\":        meta_df,\n",
    "            \"oof_xgb\":        oof_xgb,\n",
    "            \"oof_dc\":         oof_dc,\n",
    "            \"oof_tf\":         oof_tf,\n",
    "            \"seasons_sorted\": seasons_sorted,\n",
    "            \"done_seasons\":   sorted(done_seasons),\n",
    "        }\n",
    "        with open(OOF_CKPT_PATH, \"wb\") as f:\n",
    "            pickle.dump(ckpt_to_save, f)\n",
    "        print(f\"checkpoint updated，completed season: {sorted(done_seasons)}\")\n",
    "\n",
    " # ✅draw OOF (optional,)\n",
    "    mask_ok = ~(\n",
    "        np.isnan(oof_xgb).any(1) |\n",
    "        np.isnan(oof_dc).any(1)  |\n",
    "        np.isnan(oof_tf).any(1)  |\n",
    "        np.isnan(oof_pD_draw)    # \n",
    "    )\n",
    "    \n",
    "    meta_oof_df = meta_df.loc[mask_ok].copy().reset_index(drop=True)\n",
    "    \n",
    "    #  OOF\n",
    "    oof_xgb     = oof_xgb[mask_ok]\n",
    "    oof_dc      = oof_dc[mask_ok]\n",
    "    oof_tf      = oof_tf[mask_ok]\n",
    "    oof_pD_draw = oof_pD_draw[mask_ok]\n",
    "    \n",
    "    # ✅ meta_oof_df[\"pD_special\"]\n",
    "    meta_oof_df[\"pD_special\"] = oof_pD_draw\n",
    "\n",
    "    print(\" OOF built (with proper DC fold training):\", meta_oof_df.shape)\n",
    "    print(\" ready to start stacking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0492ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_oof_df shape: (7980, 176)\n",
      "oof_xgb shape: (7980, 3)\n",
      "oof_dc shape: (7980, 3)\n",
      "oof_tf shape: (7980, 3)\n",
      "oof_pD_draw shape: (7980,)\n",
      "X_oof shape: (7980, 10)\n",
      "y_oof shape: (7980,)\n",
      "season_oof len: 7980\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# features we already have:\n",
    "\n",
    "# meta_oof_df : OOF DataFrame( mask_ok filter)\n",
    "#   oof_xgb, oof_dc, oof_tf: Shape (N_oof, 3)\n",
    "# Draw specialist: Shape (N_oof,)\n",
    "\n",
    "# meta_oof_df[\"pD_special\"] --- oof_pD_draw\n",
    "\n",
    "print(\"meta_oof_df shape:\", meta_oof_df.shape)\n",
    "print(\"oof_xgb shape:\", oof_xgb.shape)\n",
    "print(\"oof_dc shape:\", oof_dc.shape)\n",
    "print(\"oof_tf shape:\", oof_tf.shape)\n",
    "print(\"oof_pD_draw shape:\", oof_pD_draw.shape)\n",
    "\n",
    "base_meta_oof = np.hstack([oof_xgb, oof_dc, oof_tf])     # (N_oof, 9)\n",
    "\n",
    "# Draw specialist\n",
    "pD_special_oof = oof_pD_draw.reshape(-1, 1)              # (N_oof, 1)\n",
    "\n",
    "# 👉 final OOF meta features: ,10 \n",
    "X_oof = np.hstack([base_meta_oof, pD_special_oof])       # (N_oof, 10)\n",
    "\n",
    "y_oof      = meta_oof_df[\"y\"].astype(int).values\n",
    "season_oof = meta_oof_df[\"Season\"].values\n",
    "\n",
    "print(\"X_oof shape:\", X_oof.shape)   # (N_oof, 10)\n",
    "print(\"y_oof shape:\", y_oof.shape)\n",
    "print(\"season_oof len:\", len(season_oof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fbc47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def build_draw_special_oof(X, y, seasons, cw=2.5, C=1.0):\n",
    "    n = len(X); pD_oof = np.zeros(n)\n",
    "    uniq = sorted(np.unique(seasons), key=season_start_year)\n",
    "    for s in uniq:\n",
    "        tr, va = seasons != s, seasons == s\n",
    "        if tr.sum() == 0 or va.sum() == 0: continue\n",
    "        clf = LogisticRegression(solver=\"lbfgs\", C=C, class_weight={0:1.0,1:cw}, max_iter=500)\n",
    "        clf.fit(X[tr], (y[tr] == 1).astype(int))\n",
    "        pD_oof[va] = clf.predict_proba(X[va])[:,1]\n",
    "    clf_final = LogisticRegression(solver=\"lbfgs\", C=C, class_weight={0:1.0,1:cw}, max_iter=500)\n",
    "    clf_final.fit(X, (y == 1).astype(int))\n",
    "    return pD_oof, clf_final\n",
    "\n",
    "pD_special_oof, draw_clf_final = build_draw_special_oof(X_oof, y_oof, season_oof, cw=2.5, C=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440e4d9",
   "metadata": {},
   "source": [
    "### Meta model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a30c214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF Season 2001/2002: best alpha=1.000, val logloss=1.0290, mean_pD=0.269\n",
      "OOF Season 2002/2003: best alpha=0.900, val logloss=1.0468, mean_pD=0.237\n",
      "OOF Season 2003/2004: best alpha=1.000, val logloss=1.0512, mean_pD=0.260\n",
      "OOF Season 2004/2005: best alpha=1.000, val logloss=1.0187, mean_pD=0.271\n",
      "OOF Season 2005/2006: best alpha=0.800, val logloss=0.9940, mean_pD=0.228\n",
      "OOF Season 2006/2007: best alpha=1.000, val logloss=1.0212, mean_pD=0.256\n",
      "OOF Season 2007/2008: best alpha=1.000, val logloss=0.9742, mean_pD=0.266\n",
      "OOF Season 2008/2009: best alpha=0.950, val logloss=0.9915, mean_pD=0.257\n",
      "OOF Season 2009/2010: best alpha=0.950, val logloss=0.9917, mean_pD=0.255\n",
      "OOF Season 2010/2011: best alpha=1.000, val logloss=1.0518, mean_pD=0.266\n",
      "OOF Season 2011/2012: best alpha=0.900, val logloss=0.9973, mean_pD=0.245\n",
      "OOF Season 2012/2013: best alpha=1.000, val logloss=1.0044, mean_pD=0.266\n",
      "OOF Season 2013/2014: best alpha=0.800, val logloss=0.9675, mean_pD=0.223\n",
      "OOF Season 2014/2015: best alpha=0.950, val logloss=1.0150, mean_pD=0.243\n",
      "OOF Season 2015/2016: best alpha=1.000, val logloss=1.0460, mean_pD=0.261\n",
      "OOF Season 2016/2017: best alpha=0.800, val logloss=0.9880, mean_pD=0.224\n",
      "OOF Season 2017/2018: best alpha=1.000, val logloss=0.9928, mean_pD=0.259\n",
      "OOF Season 2018/2019: best alpha=0.800, val logloss=0.9519, mean_pD=0.222\n",
      "OOF Season 2019/2020: best alpha=0.950, val logloss=1.0062, mean_pD=0.243\n",
      "OOF Season 2020/2021: best alpha=0.850, val logloss=1.0242, mean_pD=0.222\n",
      "OOF Season 2021/2022: best alpha=0.900, val logloss=1.0199, mean_pD=0.237\n",
      "using CV average parameters: cw_draw=1.100, alpha=0.931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def fit_two_stage(X_tr, y_tr,\n",
    "                  cw_draw=2.0,\n",
    "                  C1=0.5,\n",
    "                  C2=0.5):\n",
    "    \"\"\"\n",
    "      stage1: Draw vs Non-Draw\n",
    "    parameter:\n",
    "    \"\"\"\n",
    "    y_bin = (y_tr == 1).astype(int)\n",
    "\n",
    "    stage1 = LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        C=C1,\n",
    "        class_weight={0: 1.0, 1: cw_draw},\n",
    "        max_iter=500,\n",
    "    ).fit(X_tr, y_bin)\n",
    "\n",
    "    mask_nd = (y_tr != 1)\n",
    "    X2 = X_tr[mask_nd]\n",
    "    y2 = y_tr[mask_nd]\n",
    "    y2_bin = (y2 == 2).astype(int)  # 2=Away ->1, 0=Home->0\n",
    "\n",
    "    stage2 = LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        C=C2,\n",
    "        class_weight=\"balanced\",   #  balanced\n",
    "        max_iter=500,\n",
    "    ).fit(X2, y2_bin)\n",
    "\n",
    "    return stage1, stage2\n",
    "\n",
    "# Two-stage\n",
    "#     \"\"\"\n",
    "# + alpha draw:\n",
    "#       pD = alpha * p_draw\n",
    "# 1-pD Then by p_away H/A\n",
    "#     \"\"\"\n",
    "#     p_draw = np.clip(np.nan_to_num(p_draw, nan=0.0), 0, 1)\n",
    "#     p_away = np.clip(np.nan_to_num(p_away, nan=0.5), 0, 1)\n",
    "\n",
    "#     pD = np.clip(alpha * p_draw, 0, 1)\n",
    "#     pA = (1 - pD) * p_away\n",
    "#     pH = (1 - pD) * (1 - p_away)\n",
    "\n",
    "#     proba = np.stack([pH, pD, pA], axis=1)\n",
    "#     proba = proba / proba.sum(axis=1, keepdims=True)\n",
    "#     proba = np.clip(proba, 1e-7, 1-1e-7)\n",
    "#     return proba\n",
    "\n",
    "def make_proba_two_stage_alpha(p_draw, p_away, alpha):\n",
    "    \"\"\"\n",
    "        pD = alpha * p_draw\n",
    "    \"\"\"\n",
    "    p_draw = np.clip(np.nan_to_num(p_draw, nan=0.0), 0, 1)\n",
    "    p_away = np.clip(np.nan_to_num(p_away, nan=0.5), 0, 1)\n",
    "\n",
    "    pD = np.clip(alpha * p_draw, 0, 1)  # ← use alpha scaling\n",
    "    pA = (1 - pD) * p_away\n",
    "    pH = (1 - pD) * (1 - p_away)\n",
    "\n",
    "    proba = np.stack([pH, pD, pA], axis=1)\n",
    "    proba = proba / proba.sum(axis=1, keepdims=True)\n",
    "    return proba\n",
    "\n",
    "# draw\n",
    "TARGET_D = 0.26\n",
    "TOL      = 0.04\n",
    "target_low, target_high = TARGET_D - TOL, TARGET_D + TOL\n",
    "\n",
    "alpha_grid = np.linspace(0.5, 1.0, 11)\n",
    "seasons    = sorted(pd.unique(season_oof), key=season_start_year)\n",
    "\n",
    "fold_alphas = []\n",
    "\n",
    "for s in seasons:\n",
    "    tr_mask = (season_oof != s)\n",
    "    va_mask = (season_oof == s)\n",
    "\n",
    "    X_tr, y_tr = X_oof[tr_mask], y_oof[tr_mask]\n",
    "    X_va, y_va = X_oof[va_mask], y_oof[va_mask]\n",
    "\n",
    "    if len(X_tr) == 0 or len(X_va) == 0:\n",
    "        continue\n",
    "\n",
    "    stage1, stage2 = fit_two_stage(X_tr, y_tr, cw_draw=DRAW_CLASS_MULT)\n",
    "    p_draw_va = stage1.predict_proba(X_va)[:, 1]\n",
    "    p_away_va = stage2.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    best_feasible = None\n",
    "    best_any      = None\n",
    "\n",
    "    for a in alpha_grid:\n",
    "        proba_va = make_proba_two_stage_alpha(p_draw_va, p_away_va, a)\n",
    "        ll = log_loss(y_va, proba_va, labels=[0,1,2])\n",
    "        mean_pD = proba_va[:, 1].mean()\n",
    "\n",
    "        if best_any is None or ll < best_any[0]:\n",
    "            best_any = (ll, float(a), mean_pD)\n",
    "\n",
    "        if target_low <= mean_pD <= target_high:\n",
    "            if best_feasible is None or ll < best_feasible[0]:\n",
    "                best_feasible = (ll, float(a), mean_pD)\n",
    "\n",
    "    best = best_feasible if best_feasible is not None else best_any\n",
    "    fold_alphas.append(best[1])\n",
    "    print(f\"OOF Season {s}: best alpha={best[1]:.3f}, val logloss={best[0]:.4f}, mean_pD={best[2]:.3f}\")\n",
    "\n",
    "import numpy as np\n",
    "alpha_final = np.mean(fold_alphas)\n",
    "\n",
    "# cw_draw , DRAW_CLASS_MULT\n",
    "# if DRAW_CLASS_MULT ,,for example 3.0\n",
    "cw_draw_best = DRAW_CLASS_MULT \n",
    "\n",
    "print(\"using CV average parameters: cw_draw=%.3f, alpha=%.3f\" % (cw_draw_best, alpha_final))\n",
    "\n",
    "# \n",
    "stage1_all, stage2_all = fit_two_stage(\n",
    "    X_oof, y_oof,\n",
    "    cw_draw=cw_draw_best,\n",
    "    C1=0.5,\n",
    "    C2=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e04e75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(proba, tau=0.27, draw_bias=0.0):\n",
    "    n = len(proba)\n",
    "    preds = np.zeros(n, dtype=int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        p_h, p_d, p_a = proba[i]\n",
    "        p_d_adj = p_d + draw_bias\n",
    "        \n",
    "        if abs(p_h - p_a) < tau and p_d_adj > 0.22:\n",
    "            preds[i] = 1\n",
    "        else:\n",
    "            preds[i] = np.argmax([p_h, p_d_adj, p_a])\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dbfc5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global true draw ratio: 0.24862155388471177\n",
      "Season CV search results (top 10 by score)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cw_draw</th>\n",
       "      <th>alpha</th>\n",
       "      <th>mean_loss</th>\n",
       "      <th>mean_macroF1</th>\n",
       "      <th>mean_f1D</th>\n",
       "      <th>mean_pred_draw_ratio</th>\n",
       "      <th>ratio_penalty</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.018364</td>\n",
       "      <td>0.432084</td>\n",
       "      <td>0.385784</td>\n",
       "      <td>0.544110</td>\n",
       "      <td>0.295489</td>\n",
       "      <td>-0.850451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.014155</td>\n",
       "      <td>0.428811</td>\n",
       "      <td>0.387779</td>\n",
       "      <td>0.555890</td>\n",
       "      <td>0.307268</td>\n",
       "      <td>-0.855778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.011762</td>\n",
       "      <td>0.425389</td>\n",
       "      <td>0.388933</td>\n",
       "      <td>0.565915</td>\n",
       "      <td>0.317293</td>\n",
       "      <td>-0.862081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.011249</td>\n",
       "      <td>0.424016</td>\n",
       "      <td>0.389104</td>\n",
       "      <td>0.569298</td>\n",
       "      <td>0.320677</td>\n",
       "      <td>-0.864721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.010996</td>\n",
       "      <td>0.421857</td>\n",
       "      <td>0.390087</td>\n",
       "      <td>0.575564</td>\n",
       "      <td>0.326942</td>\n",
       "      <td>-0.869755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.011237</td>\n",
       "      <td>0.421452</td>\n",
       "      <td>0.391484</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.332080</td>\n",
       "      <td>-0.873430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.011709</td>\n",
       "      <td>0.420951</td>\n",
       "      <td>0.392042</td>\n",
       "      <td>0.583208</td>\n",
       "      <td>0.334586</td>\n",
       "      <td>-0.875773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.012271</td>\n",
       "      <td>0.419651</td>\n",
       "      <td>0.392509</td>\n",
       "      <td>0.587845</td>\n",
       "      <td>0.339223</td>\n",
       "      <td>-0.880284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.013250</td>\n",
       "      <td>0.417967</td>\n",
       "      <td>0.392638</td>\n",
       "      <td>0.592732</td>\n",
       "      <td>0.344110</td>\n",
       "      <td>-0.885769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.013785</td>\n",
       "      <td>0.416775</td>\n",
       "      <td>0.392342</td>\n",
       "      <td>0.594612</td>\n",
       "      <td>0.345990</td>\n",
       "      <td>-0.888462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cw_draw  alpha  mean_loss  mean_macroF1  mean_f1D  mean_pred_draw_ratio  \\\n",
       "0       1.5   0.60   1.018364      0.432084  0.385784              0.544110   \n",
       "1       1.5   0.65   1.014155      0.428811  0.387779              0.555890   \n",
       "2       1.5   0.70   1.011762      0.425389  0.388933              0.565915   \n",
       "11      2.0   0.60   1.011249      0.424016  0.389104              0.569298   \n",
       "3       1.5   0.75   1.010996      0.421857  0.390087              0.575564   \n",
       "12      2.0   0.65   1.011237      0.421452  0.391484              0.580702   \n",
       "4       1.5   0.80   1.011709      0.420951  0.392042              0.583208   \n",
       "22      2.5   0.60   1.012271      0.419651  0.392509              0.587845   \n",
       "13      2.0   0.70   1.013250      0.417967  0.392638              0.592732   \n",
       "5       1.5   0.85   1.013785      0.416775  0.392342              0.594612   \n",
       "\n",
       "    ratio_penalty     score  \n",
       "0        0.295489 -0.850451  \n",
       "1        0.307268 -0.855778  \n",
       "2        0.317293 -0.862081  \n",
       "11       0.320677 -0.864721  \n",
       "3        0.326942 -0.869755  \n",
       "12       0.332080 -0.873430  \n",
       "4        0.334586 -0.875773  \n",
       "22       0.339223 -0.880284  \n",
       "13       0.344110 -0.885769  \n",
       "5        0.345990 -0.888462  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best combination\n",
      "cw_draw=1.500, alpha=0.600\n",
      "mean_loss=1.0184, macroF1=0.432, f1D=0.386\n",
      "mean_pred_draw_ratio=0.544, penalty=0.295, score=-0.8505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, classification_report\n",
    "\n",
    "cw_grid     = [1.5, 2.0, 2.5, 3.0]\n",
    "alpha_grid  = np.linspace(0.6, 1.1, 11)   # \n",
    "C1, C2      = 0.5, 0.5\n",
    "\n",
    "TAU       = 0.27\n",
    "DRAW_BIAS = 0.05\n",
    "\n",
    "seasons = sorted(pd.unique(season_oof), key=season_start_year)\n",
    "true_draw_ratio_global = (y_oof == 1).mean()\n",
    "print(\"Global true draw ratio:\", true_draw_ratio_global)\n",
    "\n",
    "best_combo = None\n",
    "cv_records = []\n",
    "\n",
    "for cw in cw_grid:\n",
    "    for alpha in alpha_grid:\n",
    "        fold_losses = []\n",
    "        fold_macroF1 = []\n",
    "        fold_f1D = []\n",
    "        fold_draw_ratio = []\n",
    "\n",
    "        for s in seasons:\n",
    "            va_mask = (season_oof == s)\n",
    "            tr_mask = ~va_mask\n",
    "\n",
    "            X_tr, y_tr = X_oof[tr_mask], y_oof[tr_mask]\n",
    "            X_va, y_va = X_oof[va_mask], y_oof[va_mask]\n",
    "\n",
    "            if len(X_tr) == 0 or len(X_va) == 0:\n",
    "                continue\n",
    "\n",
    "            st1, st2 = fit_two_stage(X_tr, y_tr, cw_draw=cw, C1=C1, C2=C2)\n",
    "\n",
    " # fold p_draw, p_away\n",
    "            p_draw_va = st1.predict_proba(X_va)[:, 1]\n",
    "            p_away_va = st2.predict_proba(X_va)[:, 1]\n",
    "\n",
    "            proba_va = make_proba_two_stage_alpha(p_draw_va, p_away_va, alpha)\n",
    "\n",
    " # logloss( proba_va)\n",
    "            loss = log_loss(y_va, proba_va, labels=[0, 1, 2])\n",
    "\n",
    " # proba_va (, proba_val_meta)\n",
    "            y_pred_va = predict_with_threshold(\n",
    "                proba_va,\n",
    "                tau=TAU,\n",
    "                draw_bias=DRAW_BIAS\n",
    "            )\n",
    "\n",
    "            rep = classification_report(\n",
    "                y_va, y_pred_va,\n",
    "                digits=3, zero_division=0, output_dict=True\n",
    "            )\n",
    "\n",
    "            macro_f1 = rep[\"macro avg\"][\"f1-score\"]\n",
    "            f1_D     = rep[\"1\"][\"f1-score\"]\n",
    "\n",
    "            dist = np.bincount(y_pred_va, minlength=3) / len(y_pred_va)\n",
    "            pred_draw_ratio = dist[1]\n",
    "\n",
    "            fold_losses.append(loss)\n",
    "            fold_macroF1.append(macro_f1)\n",
    "            fold_f1D.append(f1_D)\n",
    "            fold_draw_ratio.append(pred_draw_ratio)\n",
    "\n",
    "        if not fold_losses:\n",
    "            continue\n",
    "\n",
    "        mean_loss        = np.mean(fold_losses)\n",
    "        mean_macroF1     = np.mean(fold_macroF1)\n",
    "        mean_f1D         = np.mean(fold_f1D)\n",
    "        mean_draw_ratio  = np.mean(fold_draw_ratio)\n",
    "\n",
    "        ratio_penalty = abs(mean_draw_ratio - true_draw_ratio_global)\n",
    "\n",
    "        score = (\n",
    "            - mean_loss       * 1.0   #  logloss small\n",
    "            + mean_macroF1    * 0.4   # \n",
    "            + mean_f1D        * 0.6   # draw F1 high\n",
    "            - ratio_penalty   * 0.8   # draw\n",
    "        )\n",
    "\n",
    "        cv_records.append((cw, alpha, mean_loss, mean_macroF1, mean_f1D,\n",
    "                           mean_draw_ratio, ratio_penalty, score))\n",
    "\n",
    "        if (best_combo is None) or (score > best_combo[-1]):\n",
    "            best_combo = (cw, alpha, mean_loss, mean_macroF1, mean_f1D,\n",
    "                          mean_draw_ratio, ratio_penalty, score)\n",
    "\n",
    "print(\"Season CV search results (top 10 by score)\")\n",
    "cv_df = pd.DataFrame(cv_records, columns=[\n",
    "    \"cw_draw\", \"alpha\", \"mean_loss\", \"mean_macroF1\", \"mean_f1D\",\n",
    "    \"mean_pred_draw_ratio\", \"ratio_penalty\", \"score\"\n",
    "])\n",
    "display(cv_df.sort_values(\"score\", ascending=False).head(10))\n",
    "\n",
    "print(\" best combination\")\n",
    "print(\"cw_draw=%.3f, alpha=%.3f\" % (best_combo[0], best_combo[1]))\n",
    "print(\"mean_loss=%.4f, macroF1=%.3f, f1D=%.3f\" %\n",
    "      (best_combo[2], best_combo[3], best_combo[4]))\n",
    "print(\"mean_pred_draw_ratio=%.3f, penalty=%.3f, score=%.4f\" %\n",
    "      (best_combo[5], best_combo[6], best_combo[7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c585eca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_meta_val shape: (380, 10)\n",
      "X_meta_test shape: (1140, 10)\n",
      "val logloss (two-stage+alpha base): 0.8598354826297515\n",
      "test logloss (two-stage+alpha base): 1.0044843060753992\n",
      "test mean pD (two-stage+alpha base): 0.24216534668492923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# val/test ( logloss / )\n",
    "y_val  = val_df[\"y\"].astype(int).values\n",
    "y_test = test_df[\"y\"].astype(int).values\n",
    "\n",
    "# OOF two-stage model(=10)\n",
    "stage1_all, stage2_all = fit_two_stage(X_oof, y_oof, cw_draw=DRAW_CLASS_MULT)\n",
    "\n",
    "# Construct val/test 9 \n",
    "X_base_val  = np.hstack([proba_val_xgb,  proba_val_dc,  proba_val_tf])   # (N_val, 9)\n",
    "X_base_test = np.hstack([proba_test_xgb, proba_test_dc, proba_test_tf])  # (N_test, 9)\n",
    "\n",
    "# match-level draw draw_model_full in val/test pD_special\n",
    "X_draw_val  = val_df[draw_feature_cols].to_numpy(dtype=np.float32)\n",
    "X_draw_test = test_df[draw_feature_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "pD_special_val  = draw_model_full.predict_proba(X_draw_val)[:, 1]   # (N_val,)\n",
    "pD_special_test = draw_model_full.predict_proba(X_draw_test)[:, 1]  # (N_test,)\n",
    "\n",
    "# 10 meta features:9() + 1(pD_special)\n",
    "X_meta_val  = np.hstack([X_base_val,  pD_special_val.reshape(-1, 1)])   # (N_val, 10)\n",
    "X_meta_test = np.hstack([X_base_test, pD_special_test.reshape(-1, 1)])  # (N_test,10)\n",
    "\n",
    "print(\"X_meta_val shape:\",  X_meta_val.shape)\n",
    "print(\"X_meta_test shape:\", X_meta_test.shape)\n",
    "\n",
    "# + alpha generate val/test \n",
    "p_draw_val  = stage1_all.predict_proba(X_meta_val)[:, 1]\n",
    "p_away_val  = stage2_all.predict_proba(X_meta_val)[:, 1]\n",
    "p_draw_test = stage1_all.predict_proba(X_meta_test)[:, 1]\n",
    "p_away_test = stage2_all.predict_proba(X_meta_test)[:, 1]\n",
    "\n",
    "proba_val_base  = make_proba_two_stage_alpha(p_draw_val,  p_away_val,  alpha_final)\n",
    "proba_test_base = make_proba_two_stage_alpha(p_draw_test, p_away_test, alpha_final)\n",
    "\n",
    "print(\"val logloss (two-stage+alpha base):\",  log_loss(y_val,  proba_val_base,  labels=[0,1,2]))\n",
    "print(\"test logloss (two-stage+alpha base):\", log_loss(y_test, proba_test_base, labels=[0,1,2]))\n",
    "print(\"test mean pD (two-stage+alpha base):\", proba_test_base[:,1].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70a082c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val logloss (fused): 0.8436318796016304\n",
      "test logloss (fused): 1.0106493996290882\n",
      "test mean pD (fused): 0.2823256971683488\n",
      "\n",
      "--- Isotonic Calibration (on fused OOF probabilities) ---\n",
      "val logloss (fused+isotonic): 0.8218106745642301\n",
      "test logloss (fused+isotonic): 0.9997378061973027\n",
      "test mean pD (fused+isotonic): 0.24243344157105828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "def fuse_draw_proba(proba_base, pD_special, lam=0.5):\n",
    "    \"\"\"\n",
    "      pD_new = lam * pD_base + (1-lam) * pD_special\n",
    "    \"\"\"\n",
    "    pH_base, pD_base, pA_base = proba_base[:,0], proba_base[:,1], proba_base[:,2]\n",
    "    pD_special = np.clip(pD_special, 1e-7, 1-1e-7)\n",
    "\n",
    "    pD_new = lam * pD_base + (1 - lam) * pD_special\n",
    "    pD_new = np.clip(pD_new, 1e-7, 1-1e-7)\n",
    "\n",
    "    scale = (1.0 - pD_new) / (1.0 - pD_base + 1e-8)\n",
    "    pH_new = pH_base * scale\n",
    "    pA_new = pA_base * scale\n",
    "\n",
    "    proba_new = np.stack([pH_new, pD_new, pA_new], axis=1)\n",
    "    proba_new = proba_new / proba_new.sum(axis=1, keepdims=True)\n",
    "    proba_new = np.clip(proba_new, 1e-7, 1-1e-7)\n",
    "    return proba_new\n",
    "\n",
    "LAM = 0.5\n",
    "\n",
    "# in val/test fuse \n",
    "proba_val_fused  = fuse_draw_proba(proba_val_base,  pD_special_val,  lam=LAM)\n",
    "proba_test_fused = fuse_draw_proba(proba_test_base, pD_special_test, lam=LAM)\n",
    "\n",
    "print(\"val logloss (fused):\",  log_loss(y_val,  proba_val_fused,  labels=[0,1,2]))\n",
    "print(\"test logloss (fused):\", log_loss(y_test, proba_test_fused, labels=[0,1,2]))\n",
    "print(\"test mean pD (fused):\", proba_test_fused[:,1].mean())\n",
    "\n",
    "# Isotonic \n",
    "USE_ISOTONIC = True\n",
    "\n",
    "if USE_ISOTONIC:\n",
    "    print(\"\\n--- Isotonic Calibration (on fused OOF probabilities) ---\")\n",
    "    \n",
    "    n_samples_oof = X_oof.shape[0]\n",
    "    p_draw_oof = np.zeros(n_samples_oof, dtype=float)\n",
    "    p_away_oof = np.zeros(n_samples_oof, dtype=float)\n",
    "\n",
    "    seasons = sorted(pd.unique(season_oof), key=season_start_year)\n",
    "\n",
    "    for s in seasons:\n",
    "        va_mask = (season_oof == s)\n",
    "        tr_mask = ~va_mask\n",
    "\n",
    "        X_tr, y_tr = X_oof[tr_mask], y_oof[tr_mask]\n",
    "        X_va, y_va = X_oof[va_mask], y_oof[va_mask]\n",
    "\n",
    "        if len(X_tr) == 0 or len(X_va) == 0:\n",
    "            continue\n",
    "\n",
    "        stage1_cv, stage2_cv = fit_two_stage(X_tr, y_tr, cw_draw=DRAW_CLASS_MULT)\n",
    "        p_draw_va = stage1_cv.predict_proba(X_va)[:, 1]\n",
    "        p_away_va = stage2_cv.predict_proba(X_va)[:, 1]\n",
    "\n",
    "        p_draw_oof[va_mask] = p_draw_va\n",
    "        p_away_oof[va_mask] = p_away_va\n",
    "\n",
    "    proba_oof_base = make_proba_two_stage_alpha(\n",
    "        p_draw_oof,\n",
    "        p_away_oof,\n",
    "        alpha_final\n",
    "    )\n",
    "\n",
    " # 2) in OOF draw-fuse(pD_special_oof oof_pD_draw)\n",
    "    pD_special_oof = oof_pD_draw               # (N_oof,)\n",
    "    proba_oof_fused = fuse_draw_proba(proba_oof_base, pD_special_oof, lam=LAM)\n",
    "\n",
    " # Isotonic \n",
    "    iso_calibrators = []\n",
    "    for c in range(3):\n",
    "        iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "        iso.fit(proba_oof_fused[:, c], (y_oof == c).astype(float))\n",
    "        iso_calibrators.append(iso)\n",
    "\n",
    " # pair fused val/test \n",
    "    proba_val_iso  = np.zeros_like(proba_val_fused)\n",
    "    proba_test_iso = np.zeros_like(proba_test_fused)\n",
    "\n",
    "    for c in range(3):\n",
    "        proba_val_iso[:,  c] = iso_calibrators[c].transform(proba_val_fused[:,  c])\n",
    "        proba_test_iso[:, c] = iso_calibrators[c].transform(proba_test_fused[:, c])\n",
    "\n",
    "    proba_val_iso  = np.clip(proba_val_iso,  1e-7, 1 - 1e-7)\n",
    "    proba_test_iso = np.clip(proba_test_iso, 1e-7, 1 - 1e-7)\n",
    "    proba_val_iso  = proba_val_iso  / proba_val_iso.sum(axis=1,  keepdims=True)\n",
    "    proba_test_iso = proba_test_iso / proba_test_iso.sum(axis=1, keepdims=True)\n",
    "\n",
    "    print(\"val logloss (fused+isotonic):\",  log_loss(y_val,  proba_val_iso,  labels=[0, 1, 2]))\n",
    "    print(\"test logloss (fused+isotonic):\", log_loss(y_test, proba_test_iso, labels=[0, 1, 2]))\n",
    "    print(\"test mean pD (fused+isotonic):\", proba_test_iso[:, 1].mean())\n",
    "\n",
    "    proba_val_final  = proba_val_iso\n",
    "    proba_test_final = proba_test_iso\n",
    "\n",
    "else:\n",
    "    proba_val_final  = proba_val_fused\n",
    "    proba_test_final = proba_test_fused\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23a3de",
   "metadata": {},
   "source": [
    "### Final test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee7a9ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL tau: 0.26875000000000004 (score=0.6789)\n",
      "val pred dist: [0.40789474 0.24210526 0.35      ]\n",
      "val macro-F1: 0.7075871781330516\n",
      "val D precision=0.652 recall=0.682 F1=0.667\n",
      "\n",
      "==================================================\n",
      "FINAL TEST RESULTS\n",
      "==================================================\n",
      "Accuracy: 0.5114035087719299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.624     0.603     0.613       514\n",
      "           1      0.248     0.198     0.220       262\n",
      "           2      0.510     0.607     0.555       364\n",
      "\n",
      "    accuracy                          0.511      1140\n",
      "   macro avg      0.461     0.470     0.463      1140\n",
      "weighted avg      0.501     0.511     0.504      1140\n",
      "\n",
      "Prediction distribution: [0.43596491 0.18421053 0.37982456]\n",
      "FINAL test logloss: 0.9997378061973027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "def apply_draw_threshold(proba, tau):\n",
    "    pH, pD, pA = proba[:,0], proba[:,1], proba[:,2]\n",
    "    return np.where(pD > tau, 1, np.where(pA > pH, 2, 0))\n",
    "\n",
    "taus = np.linspace(0.20, 0.45, 81)\n",
    "\n",
    "best = None\n",
    "for tau in taus:\n",
    "    pred_val = apply_draw_threshold(proba_val_final, tau)\n",
    "    rep_val  = classification_report(y_val, pred_val, output_dict=True, zero_division=0)\n",
    "\n",
    "    macro_f1 = rep_val[\"macro avg\"][\"f1-score\"]\n",
    "    prec_D   = rep_val[\"1\"][\"precision\"]\n",
    "    rec_D    = rep_val[\"1\"][\"recall\"]\n",
    "    f1_D     = rep_val[\"1\"][\"f1-score\"]\n",
    "    dist_val = np.bincount(pred_val, minlength=3)/len(pred_val)\n",
    "\n",
    " # draw\n",
    "    pred_draw_ratio = dist_val[1]\n",
    "    if not (0.10 <= pred_draw_ratio <= 0.30):\n",
    "        continue\n",
    "\n",
    "    score = 0.7 * f1_D + 0.3 * macro_f1\n",
    "\n",
    "    if (best is None) or (score > best[0]):\n",
    "        best = (score, float(tau), dist_val, macro_f1, prec_D, rec_D, f1_D)\n",
    "\n",
    "if best is None:\n",
    "    best = (-1, float(taus[0]), np.array([0,0,0]), 0, 0, 0, 0)\n",
    "\n",
    "tau_final = best[1]\n",
    "print(\"\\nFINAL tau:\", tau_final, \"(score=%.4f)\" % best[0])\n",
    "print(\"val pred dist:\", best[2])\n",
    "print(\"val macro-F1:\", best[3])\n",
    "print(\"val D precision=%.3f recall=%.3f F1=%.3f\" % (best[4], best[5], best[6]))\n",
    "\n",
    "pred_test = apply_draw_threshold(proba_test_final, tau_final)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_test))\n",
    "print(classification_report(y_test, pred_test, digits=3, zero_division=0))\n",
    "print(\"Prediction distribution:\", np.bincount(pred_test, minlength=3)/len(pred_test))\n",
    "print(\"FINAL test logloss:\", log_loss(y_test, proba_test_final, labels=[0,1,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ff50af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_meta_val_pure shape: (380, 10)\n",
      "X_meta_test_pure shape: (1140, 10)\n",
      "\n",
      "===== PURE TWO-STAGE + ALPHA (NO FUSE / NO ISO / NO TAU) =====\n",
      "\n",
      "val logloss (pure): 0.8598354826297515\n",
      "test logloss (pure): 1.0044843060753992\n",
      "\n",
      "VAL classification report (pure):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.741     0.755     0.748       163\n",
      "           1      0.000     0.000     0.000        88\n",
      "           2      0.537     0.891     0.671       129\n",
      "\n",
      "    accuracy                          0.626       380\n",
      "   macro avg      0.426     0.549     0.473       380\n",
      "weighted avg      0.500     0.626     0.548       380\n",
      "\n",
      "VAL macro-F1 (pure): 0.4727581002005666\n",
      "VAL pred distribution: [0.43684211 0.         0.56315789]\n",
      "\n",
      "TEST classification report (pure):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.653     0.607     0.629       514\n",
      "           1      0.000     0.000     0.000       262\n",
      "           2      0.447     0.813     0.577       364\n",
      "\n",
      "    accuracy                          0.533      1140\n",
      "   macro avg      0.367     0.473     0.402      1140\n",
      "weighted avg      0.437     0.533     0.468      1140\n",
      "\n",
      "TEST macro-F1 (pure): 0.4020101029155924\n",
      "TEST pred distribution: [0.41929825 0.         0.58070175]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, classification_report\n",
    "\n",
    "# Two-stage\n",
    "# draw-fuse isotonic τ hyperparameter tuning\n",
    "# Direct argmax(proba) \n",
    "\n",
    "#  val/test Label\n",
    "y_val_baseline  = val_df[\"y\"].astype(int).values\n",
    "y_test_baseline = test_df[\"y\"].astype(int).values\n",
    "\n",
    "# OOF Logistic model(input = 10 X_oof)\n",
    "stage1_pure, stage2_pure = fit_two_stage(X_oof, y_oof, cw_draw=DRAW_CLASS_MULT)\n",
    "\n",
    "# Construct val/test 10 meta Feature\n",
    "# 1 9 (XGB/DC/TF)\n",
    "X_base_val_pure  = np.hstack([proba_val_xgb,  proba_val_dc,  proba_val_tf])   # (N_val, 9)\n",
    "X_base_test_pure = np.hstack([proba_test_xgb, proba_test_dc, proba_test_tf])  # (N_test, 9)\n",
    "\n",
    "# 2 1 match-level draw pD_special( X_oof 10 )\n",
    "X_draw_val_pure  = val_df[draw_feature_cols].to_numpy(dtype=np.float32)\n",
    "X_draw_test_pure = test_df[draw_feature_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "pD_special_val_pure  = draw_model_full.predict_proba(X_draw_val_pure)[:, 1]\n",
    "pD_special_test_pure = draw_model_full.predict_proba(X_draw_test_pure)[:, 1]\n",
    "\n",
    "X_meta_val_pure  = np.hstack([X_base_val_pure,  pD_special_val_pure.reshape(-1, 1)])   # (N_val, 10)\n",
    "X_meta_test_pure = np.hstack([X_base_test_pure, pD_special_test_pure.reshape(-1, 1)])  # (N_test,10)\n",
    "\n",
    "print(\"X_meta_val_pure shape:\",  X_meta_val_pure.shape)\n",
    "print(\"X_meta_test_pure shape:\", X_meta_test_pure.shape)\n",
    "\n",
    "# two-stage + alpha ( baseline)\n",
    "p_draw_val_pure  = stage1_pure.predict_proba(X_meta_val_pure)[:, 1]\n",
    "p_away_val_pure  = stage2_pure.predict_proba(X_meta_val_pure)[:, 1]\n",
    "p_draw_test_pure = stage1_pure.predict_proba(X_meta_test_pure)[:, 1]\n",
    "p_away_test_pure = stage2_pure.predict_proba(X_meta_test_pure)[:, 1]\n",
    "\n",
    "proba_val_pure  = make_proba_two_stage_alpha(p_draw_val_pure,  p_away_val_pure,  alpha_final)\n",
    "proba_test_pure = make_proba_two_stage_alpha(p_draw_test_pure, p_away_test_pure, alpha_final)\n",
    "\n",
    "# argmax ( τ / draw-fuse / isotonic)\n",
    "y_pred_val_pure  = proba_val_pure.argmax(axis=1)\n",
    "y_pred_test_pure = proba_test_pure.argmax(axis=1)\n",
    "\n",
    "# Evaluation:logloss + macro-F1 + distribution\n",
    "print(\"\\n===== PURE TWO-STAGE + ALPHA (NO FUSE / NO ISO / NO TAU) =====\\n\")\n",
    "\n",
    "print(\"val logloss (pure):\",  log_loss(y_val_baseline,  proba_val_pure,  labels=[0,1,2]))\n",
    "print(\"test logloss (pure):\", log_loss(y_test_baseline, proba_test_pure, labels=[0,1,2]))\n",
    "\n",
    "rep_val_pure  = classification_report(y_val_baseline,  y_pred_val_pure,  digits=3, zero_division=0, output_dict=True)\n",
    "rep_test_pure = classification_report(y_test_baseline, y_pred_test_pure, digits=3, zero_division=0, output_dict=True)\n",
    "\n",
    "print(\"\\nVAL classification report (pure):\")\n",
    "print(classification_report(y_val_baseline, y_pred_val_pure, digits=3, zero_division=0))\n",
    "\n",
    "print(\"VAL macro-F1 (pure):\", rep_val_pure[\"macro avg\"][\"f1-score\"])\n",
    "print(\"VAL pred distribution:\", np.bincount(y_pred_val_pure, minlength=3)/len(y_pred_val_pure))\n",
    "\n",
    "print(\"\\nTEST classification report (pure):\")\n",
    "print(classification_report(y_test_baseline, y_pred_test_pure, digits=3, zero_division=0))\n",
    "\n",
    "print(\"TEST macro-F1 (pure):\", rep_test_pure[\"macro avg\"][\"f1-score\"])\n",
    "print(\"TEST pred distribution:\", np.bincount(y_pred_test_pure, minlength=3)/len(y_pred_test_pure))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79260080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OOF label distribution:\n",
      "  counts: [3652, 1984, 2344]\n",
      "  ratio : [H=0.458, D=0.249, A=0.294]\n",
      "\n",
      "VAL label distribution:\n",
      "  counts: [163, 88, 129]\n",
      "  ratio : [H=0.429, D=0.232, A=0.339]\n",
      "\n",
      "TEST label distribution:\n",
      "  counts: [514, 262, 364]\n",
      "  ratio : [H=0.451, D=0.230, A=0.319]\n"
     ]
    }
   ],
   "source": [
    "def print_label_dist(name, y):\n",
    "    counts = np.bincount(y, minlength=3)\n",
    "    dist = counts / counts.sum()\n",
    "    print(f\"\\n{name} label distribution:\")\n",
    "    print(\"  counts:\", counts.tolist())\n",
    "    print(\"  ratio : [H=%.3f, D=%.3f, A=%.3f]\" % (dist[0], dist[1], dist[2]))\n",
    "\n",
    "print_label_dist(\"OOF\",  y_oof)\n",
    "print_label_dist(\"VAL\",  val_df[\"y\"].astype(int).values)\n",
    "print_label_dist(\"TEST\", test_df[\"y\"].astype(int).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5617fa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGB =====\n",
      "VAL pred dist : [0.36842105 0.25263158 0.37894737]\n",
      "TEST pred dist: [0.35       0.21140351 0.43859649]\n",
      "\n",
      "VAL report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.843     0.724     0.779       163\n",
      "           1      0.729     0.795     0.761        88\n",
      "           2      0.757     0.845     0.799       129\n",
      "\n",
      "    accuracy                          0.782       380\n",
      "   macro avg      0.776     0.788     0.779       380\n",
      "weighted avg      0.787     0.782     0.781       380\n",
      "\n",
      "\n",
      "TEST report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.647     0.502     0.565       514\n",
      "           1      0.220     0.202     0.211       262\n",
      "           2      0.468     0.643     0.542       364\n",
      "\n",
      "    accuracy                          0.478      1140\n",
      "   macro avg      0.445     0.449     0.439      1140\n",
      "weighted avg      0.492     0.478     0.476      1140\n",
      "\n",
      "\n",
      "===== Dixon-Coles =====\n",
      "VAL pred dist : [0.58947368 0.         0.41052632]\n",
      "TEST pred dist: [0.66842105 0.         0.33157895]\n",
      "\n",
      "VAL report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.540     0.742     0.625       163\n",
      "           1      0.000     0.000     0.000        88\n",
      "           2      0.500     0.605     0.547       129\n",
      "\n",
      "    accuracy                          0.524       380\n",
      "   macro avg      0.347     0.449     0.391       380\n",
      "weighted avg      0.401     0.524     0.454       380\n",
      "\n",
      "\n",
      "TEST report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.535     0.794     0.639       514\n",
      "           1      0.000     0.000     0.000       262\n",
      "           2      0.492     0.511     0.501       364\n",
      "\n",
      "    accuracy                          0.521      1140\n",
      "   macro avg      0.342     0.435     0.380      1140\n",
      "weighted avg      0.399     0.521     0.448      1140\n",
      "\n",
      "\n",
      "===== Transformer =====\n",
      "VAL pred dist : [0.42894737 0.27105263 0.3       ]\n",
      "TEST pred dist: [0.39473684 0.2754386  0.32982456]\n",
      "\n",
      "VAL report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.589     0.589     0.589       163\n",
      "           1      0.282     0.330     0.304        88\n",
      "           2      0.553     0.488     0.519       129\n",
      "\n",
      "    accuracy                          0.495       380\n",
      "   macro avg      0.474     0.469     0.470       380\n",
      "weighted avg      0.505     0.495     0.499       380\n",
      "\n",
      "\n",
      "TEST report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.578     0.506     0.539       514\n",
      "           1      0.220     0.263     0.240       262\n",
      "           2      0.426     0.440     0.432       364\n",
      "\n",
      "    accuracy                          0.429      1140\n",
      "   macro avg      0.408     0.403     0.404      1140\n",
      "weighted avg      0.447     0.429     0.436      1140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_val_arr  = val_df[\"y\"].astype(int).values\n",
    "y_test_arr = test_df[\"y\"].astype(int).values\n",
    "\n",
    "def eval_base(name, proba_val, proba_test):\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    ypv = proba_val.argmax(axis=1)\n",
    "    ypt = proba_test.argmax(axis=1)\n",
    "\n",
    "    dist_val  = np.bincount(ypv, minlength=3) / len(ypv)\n",
    "    dist_test = np.bincount(ypt, minlength=3) / len(ypt)\n",
    "\n",
    "    print(\"VAL pred dist :\", dist_val)\n",
    "    print(\"TEST pred dist:\", dist_test)\n",
    "\n",
    "    print(\"\\nVAL report:\")\n",
    "    print(classification_report(y_val_arr, ypv, digits=3, zero_division=0))\n",
    "    print(\"\\nTEST report:\")\n",
    "    print(classification_report(y_test_arr, ypt, digits=3, zero_division=0))\n",
    "\n",
    "eval_base(\"XGB\",        proba_val_xgb,  proba_test_xgb)\n",
    "eval_base(\"Dixon-Coles\",proba_val_dc,   proba_test_dc)\n",
    "eval_base(\"Transformer\",proba_val_tf,   proba_test_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c54b9cd-2d32-4881-abb5-0be51d28aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current base_df columns: 174\n",
      "Total number of numeric columns: 164\n",
      "Number of candidate features after removing 'y': 163\n",
      " remove 24 leakage features: ['FTHG', 'FTAG', 'HTHG', 'HTAG', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR', 'shots_for', 'shots_against', 'sot_for', 'sot_against', 'corners_for', 'corners_against', 'shot_accuracy', 'opp_shot_accuracy']\n",
      "✅ Features after cleaning: 163 → 139\n",
      "Number of candidate features after removing leaks: 139\n"
     ]
    }
   ],
   "source": [
    "# if DC model uses only recent seasons,e.g. dc_train_df,can replace base_df with that one;\n",
    "# otherwise,just use train_df as base.\n",
    "base_df = train_df   # or dc_train_df,depending on what you feed to DC \n",
    "\n",
    "print(\"Current base_df columns:\", len(base_df.columns))\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_cols = base_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Total number of numeric columns:\", len(numeric_cols))\n",
    "\n",
    "meta_cols = [\"y\"]  # if match_id,\n",
    "candidate_features = [c for c in numeric_cols if c not in meta_cols]\n",
    "\n",
    "print(\"Number of candidate features after removing 'y':\", len(candidate_features))\n",
    "\n",
    "# Remove leakage features\n",
    "candidate_features = remove_leak_features(candidate_features)\n",
    "\n",
    "print(\"Number of candidate features after removing leaks:\", len(candidate_features))\n",
    "\n",
    "if len(candidate_features) == 0:\n",
    "    print(\" candidate_features is empty, preview of the first 50 values:\")\n",
    "    print(numeric_cols[:50])\n",
    "    raise ValueError(\n",
    "        \"candidate_features :\"\n",
    "        \" base_df ,score,\"\n",
    "        \" base_df  compute_all_features  df.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48c63c8f-b02b-4044-a0ae-f289d7a5853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_scores_once_full(\n",
    "    df,\n",
    "    candidate_features,\n",
    "    target_col=\"y\",\n",
    "    methods=('permutation', 'mutual_info', 'rf_importance'),\n",
    "    n_estimators_rf=200,\n",
    "    n_repeats_perm=10,\n",
    "    random_state_base=42,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "      - RandomForest + permutation_importance\n",
    "      - mutual_info_classif\n",
    "      - RandomForest feature_importances_\n",
    "    return:\n",
    "    \"\"\"\n",
    "    available_features = [f for f in candidate_features if f in df.columns]\n",
    "    if verbose:\n",
    "        print(f\" avaliable number of features : {len(available_features)}\")\n",
    "\n",
    "    if not available_features:\n",
    "        if verbose:\n",
    "            print(\"No features available, return empty result.\")\n",
    "        return pd.Series(dtype=float), {}\n",
    "\n",
    "    X = df[available_features].copy().fillna(0.0)\n",
    "    y = df[target_col].astype(int)\n",
    "\n",
    "    method_scores = {}\n",
    "\n",
    "    if 'permutation' in methods:\n",
    "        if verbose:\n",
    "            print(\" [1/3] calculate Permutation...\")\n",
    "        rf_perm = RandomForestClassifier(\n",
    "            n_estimators=n_estimators_rf,\n",
    "            random_state=random_state_base,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_perm.fit(X, y)\n",
    "        perm = permutation_importance(\n",
    "            rf_perm, X, y,\n",
    "            n_repeats=n_repeats_perm,\n",
    "            random_state=random_state_base\n",
    "        )\n",
    "        perm_scores = pd.Series(perm.importances_mean, index=X.columns)\n",
    "        method_scores['permutation'] = perm_scores\n",
    "\n",
    "    if 'mutual_info' in methods:\n",
    "        if verbose:\n",
    "            print(\"  [2/3] calculate Mutual Information...\")\n",
    "        mi_vals = mutual_info_classif(X, y, random_state=random_state_base)\n",
    "        mi_scores = pd.Series(mi_vals, index=X.columns)\n",
    "        method_scores['mutual_info'] = mi_scores\n",
    "\n",
    "    if 'rf_importance' in methods:\n",
    "        if verbose:\n",
    "            print(\" [3/3] calculate RandomForest...\")\n",
    "        rf_imp = RandomForestClassifier(\n",
    "            n_estimators=n_estimators_rf,\n",
    "            random_state=random_state_base + 1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf_imp.fit(X, y)\n",
    "        rf_scores = pd.Series(rf_imp.feature_importances_, index=X.columns)\n",
    "        method_scores['rf_importance'] = rf_scores\n",
    "\n",
    "    if not method_scores:\n",
    "        if verbose:\n",
    "            print(\" No scoring method was enabled; returned empty.\")\n",
    "        return pd.Series(dtype=float), {}\n",
    "\n",
    "    if verbose:\n",
    "        print(\" Combining scores from multiple methods (averaging after z-score standardization)...\")\n",
    "\n",
    "    # Construct DataFrame: line=Feature, column=method\n",
    "    score_df = pd.DataFrame(method_scores)\n",
    "\n",
    " # z-score standardization, dominate\n",
    "    score_df_z = (score_df - score_df.mean()) / (score_df.std(ddof=0) + 1e-9)\n",
    "\n",
    "    # \n",
    "    combined_scores = score_df_z.mean(axis=1)\n",
    "    combined_scores = combined_scores.sort_values(ascending=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(\" Single scoring completed, Top 10 comprehensive characteristics:\")\n",
    "        print(combined_scores.head(10).to_dict())\n",
    "\n",
    "    return combined_scores, method_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad08521c-f882-4951-b55c-07f08db7968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection function definition complete\n"
     ]
    }
   ],
   "source": [
    "def clean_tf_features(df, cols):\n",
    "\n",
    "    good = []\n",
    "    for c in cols:\n",
    "        if not isinstance(c, str) or c not in df.columns:\n",
    "            continue\n",
    "        s = df[c]\n",
    "        if isinstance(s, pd.DataFrame):\n",
    "            if all(pd.api.types.is_numeric_dtype(s[col]) for col in s.columns):\n",
    "                if (s.nunique(axis=1) == 1).all():\n",
    "                    s = s.iloc[:, 0]\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        if not pd.api.types.is_numeric_dtype(s.dtype):\n",
    "            continue\n",
    "        good.append(c)\n",
    "    return good\n",
    "\n",
    "def select_unified_features(df, verbose=True):\n",
    "    \"\"\"\n",
    "    return:(all_features, xgb_features, dc_features, tf_features)\n",
    "    \"\"\"\n",
    "    # \n",
    "    LEAK_COLS = {\n",
    "        'FTHG', 'FTAG', 'HTHG', 'HTAG', 'FTR', 'HTR',\n",
    "        'HS', 'AS', 'HST', 'AST', 'HC', 'AC', \n",
    "        'HF', 'AF', 'HY', 'AY', 'HR', 'AR',\n",
    "        'shots_for', 'shots_against', 'sot_for', 'sot_against',\n",
    "        'corners_for', 'corners_against', 'shot_accuracy', 'opp_shot_accuracy',\n",
    "    }\n",
    "    \n",
    "    # \n",
    "    manual_core = [\n",
    "        # form/form\n",
    "        \"form_home\", \"form_away\", \"form_diff\",\n",
    "        \"form_home_v2\", \"form_away_v2\", \"form_diff_v2\",\n",
    "        \"home_home_form\", \"away_away_form\",\n",
    "        \"win_streak_home\", \"win_streak_away\", \"unbeaten_home\", \"unbeaten_away\",\n",
    "        # L5/L10\n",
    "        \"L5HWR\", \"L5HDR\", \"L5AWR\", \"L5ADR\",\n",
    "        \"L10HWR\", \"L10HDR\", \"L10AWR\", \"L10ADR\",\n",
    "        \"L5_home_adv\", \"L10_home_adv\",\n",
    "        # integration/position\n",
    "        \"points_home\", \"points_away\", \"points_diff\",\n",
    "        \"points_home_v2\", \"points_away_v2\", \"points_diff_v2\",\n",
    "        \"position_home\", \"position_away\", \"position_diff\",\n",
    "        \"position_home_v2\", \"position_away_v2\", \"position_diff_v2\",\n",
    "        \"gd_home\", \"gd_away\", \"gd_diff\", \"season_gd_diff\",\n",
    "        # Elo\n",
    "        \"elo_home\", \"elo_away\", \"elo_diff\",\n",
    "        \"elo_att_home\", \"elo_def_home\", \"elo_att_away\", \"elo_def_away\",\n",
    "        # momentum\n",
    "        \"attack_momentum_home\", \"attack_momentum_away\",\n",
    "        \"defense_momentum_home\", \"defense_momentum_away\",\n",
    "        \"attack_vs_defense_home\", \"attack_vs_defense_away\",\n",
    "        # goal\n",
    "        \"goals_pm_home\", \"goals_pm_away\", \"conceded_pm_home\", \"conceded_pm_away\",\n",
    "        \"goals_scored_pm_home\", \"goals_scored_pm_away\",\n",
    "        \"goals_conceded_pm_home\", \"goals_conceded_pm_away\",\n",
    "        \"gd_pm_home\", \"gd_pm_away\", \"gd_pm_diff\",\n",
    "        # xG\n",
    "        \"xg_home\", \"xg_away\", \"xg_diff\", \"xg_total\",\n",
    "        # H2H\n",
    "        \"h2h_home_rate\", \"h2h_draw_rate\", \"h2h_away_rate\",\n",
    "        \"h2h_home_rate_v2\", \"h2h_draw_rate_v2\", \"h2h_away_rate_v2\",\n",
    "        \"h2h_home_rate_td\", \"h2h_draw_rate_td\", \"h2h_away_rate_td\",\n",
    "        \"h2h_goal_diff_avg\", \"h2h_matches\", \"h2h_matches_v2\", \"h2h_matches_td\",\n",
    "        # rest\n",
    "        \"rest_days_home\", \"rest_days_away\", \"rest_diff\",\n",
    "        \"rest_days_home_v2\", \"rest_days_away_v2\", \"rest_diff_v2\",\n",
    "        # draw\n",
    "        \"draw_prop_home\", \"draw_prop_away\", \"draw_prop_sum\",\n",
    "        \"draw_prop_home_v2\", \"draw_prop_away_v2\", \"draw_prop_sum_v2\", \"draw_prop_diff_v2\",\n",
    "        # \n",
    "        \"abs_form_diff\", \"abs_points_diff\", \"abs_gd_diff\", \"abs_position_diff\", \"abs_h2h_gd_avg\",\n",
    "        # pm\n",
    "        \"shots_for_pm_home\", \"shots_for_pm_away\", \"sot_for_pm_home\", \"sot_for_pm_away\",\n",
    "        \"shots_against_pm_home\", \"shots_against_pm_away\", \"sot_against_pm_home\", \"sot_against_pm_away\",\n",
    "        \"shots_pm_diff\", \"sot_pm_diff\",\n",
    "        # corners/fouls/pm\n",
    "        \"corners_for_pm_home\", \"corners_for_pm_away\", \"corners_pm_diff\",\n",
    "        \"fouls_pm_home\", \"fouls_pm_away\", \"fouls_pm_diff\",\n",
    "        \"yellows_pm_home\", \"yellows_pm_away\", \"yellows_pm_diff\",\n",
    "        # referee\n",
    "        \"ref_home_rate\", \"ref_draw_rate\", \"ref_away_rate\",\n",
    "        \"ref_home_rate_v2\", \"ref_draw_rate_v2\", \"ref_away_rate_v2\",\n",
    "        \"ref_matches\", \"ref_matches_v2\", \"ref_home_bias\", \"ref_home_bias_v2\",\n",
    "        \"ref_avg_yellow\", \"ref_avg_red\", \"ref_avg_fouls\",\n",
    "        # \n",
    "        \"match_week\", \"is_late_season\", \"early_season\", \"mid_season\", \"late_season\", \"both_mid_table\",\n",
    "    ]\n",
    "    \n",
    "    manual_core = [c for c in manual_core if c in df.columns]\n",
    "    \n",
    "\n",
    "    extra_cols = []\n",
    "    meta_cols = {\"date\", \"season\", \"hometeam\", \"awayteam\", \"ftr\", \"y\", \"referee\"}\n",
    "    for c in df.columns:\n",
    "        name = str(c).lower()\n",
    "        if c in manual_core or name in meta_cols or c in LEAK_COLS:\n",
    "            continue\n",
    "        if any(kw in name for kw in [\"momentum\", \"adv\", \"streak\", \"prop\"]):\n",
    "            extra_cols.append(c)\n",
    "    \n",
    "    candidate = list(dict.fromkeys(manual_core + extra_cols))\n",
    "    candidate = [c for c in candidate if c not in LEAK_COLS]\n",
    "    all_features = clean_tf_features(df, candidate)\n",
    "    \n",
    "\n",
    "    xgb_features = all_features.copy()\n",
    "    \n",
    "    dc_priority = [\n",
    "        \"elo_diff\", \"form_diff\", \"form_diff_v2\", \"points_diff\", \"position_diff\", \"gd_diff\",\n",
    "        \"h2h_home_rate\", \"h2h_draw_rate\", \"rest_diff\", \"xg_diff\", \"L5_home_adv\", \"draw_prop_sum_v2\",\n",
    "    ]\n",
    "    dc_features = [f for f in dc_priority if f in all_features]\n",
    "    for f in all_features:\n",
    "        if f not in dc_features and len(dc_features) < 12:\n",
    "            dc_features.append(f)\n",
    "    dc_features = dc_features[:12]\n",
    "    \n",
    "    tf_features = all_features.copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\" Unified feature selection completed: total{len(all_features)}, XGB={len(xgb_features)}, DC={len(dc_features)}, TF={len(tf_features)}\")\n",
    "        print(f\" DC features: {dc_features}\")\n",
    "    \n",
    "    return all_features, xgb_features, dc_features, tf_features\n",
    "\n",
    "print(\"Feature selection function definition complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3818ac5d-7383-4e64-a805-75610610be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features have been loaded from unified_features.pkl\n",
      "Final feature count: XGB=120, DC=12, TF=120\n",
      "DC features: ['elo_diff', 'form_diff', 'form_diff_v2', 'points_diff', 'position_diff', 'gd_diff', 'h2h_home_rate', 'h2h_draw_rate', 'rest_diff', 'xg_diff', 'draw_prop_sum_v2', 'form_home']\n"
     ]
    }
   ],
   "source": [
    "RESELECT_FEATURES = False  #  Change to True to force reselection \n",
    "\n",
    "if (not RESELECT_FEATURES) and os.path.exists(\"unified_features.pkl\"):\n",
    "    with open(\"unified_features.pkl\", \"rb\") as f:\n",
    "        saved = pickle.load(f)\n",
    "    feature_cols_xgb = saved[\"feature_cols_xgb\"]\n",
    "    dc_feature_cols = saved[\"dc_feature_cols\"]\n",
    "    tf_token_features = saved[\"tf_token_features\"]\n",
    "    \n",
    "    LEAK_CHECK = {'FTHG','FTAG','HS','AS','HST','AST','HC','AC','HF','AF','HY','AY','HR','AR',\n",
    "                  'shots_for','shots_against','sot_for','sot_against','corners_for','corners_against'}\n",
    "    feature_cols_xgb = [f for f in feature_cols_xgb if f not in LEAK_CHECK]\n",
    "    dc_feature_cols = [f for f in dc_feature_cols if f not in LEAK_CHECK]\n",
    "    tf_token_features = [f for f in tf_token_features if f not in LEAK_CHECK]\n",
    "    \n",
    "    print(\"Features have been loaded from unified_features.pkl\")\n",
    "\n",
    "else:\n",
    " # ( train_df)\n",
    "    _, feature_cols_xgb, dc_feature_cols, tf_token_features = select_unified_features(train_df, verbose=True)\n",
    "    \n",
    "    # Save\n",
    "    with open(\"unified_features.json\", \"w\") as f:\n",
    "        json.dump({\"xgb\": feature_cols_xgb, \"dc\": dc_feature_cols, \"tf\": tf_token_features}, f, indent=2)\n",
    "    with open(\"unified_features.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"feature_cols_xgb\": feature_cols_xgb, \"dc_feature_cols\": dc_feature_cols, \n",
    "                     \"tf_token_features\": tf_token_features}, f)\n",
    "    print(\" unified_features.pkl\")\n",
    "\n",
    "print(f\"Final feature count: XGB={len(feature_cols_xgb)}, DC={len(dc_feature_cols)}, TF={len(tf_token_features)}\")\n",
    "print(f\"DC features: {dc_feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa014195",
   "metadata": {},
   "source": [
    "## 6. Results & save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f0bc0b5-62c5-4685-972c-64028708a35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draw_feature_cols_to_save: 24\n",
      "tf_token_features_to_save: 120\n",
      "✅ Model state saved (using dill)\n",
      "   XGB features: 120\n",
      "   DC features: 12\n",
      "   Draw features: 24\n",
      "   TF features: 120\n",
      "   FeatureEngineering: ✅\n"
     ]
    }
   ],
   "source": [
    "draw_feature_cols_to_save = [\n",
    "    \"abs_form_diff\", \"abs_points_diff\", \"abs_gd_diff\",\n",
    "    \"abs_elo_sum_diff\", \"abs_position_diff\", \"abs_h2h_gd_avg\",\n",
    "    \"xg_total\", \"low_xg_flag\", \"attack_mom_sum\", \"defense_mom_sum\",\n",
    "    \"abs_shots_pm_diff\", \"abs_sot_pm_diff\",\n",
    "    \"abs_corners_pm_diff\", \"abs_fouls_pm_diff\",\n",
    "    \"draw_prop_home\", \"draw_prop_away\", \"draw_prop_sum\",\n",
    "    \"h2h_draw_rate\", \"h2h_draw_rate_td\", \"h2h_draw_rate_mean\",\n",
    "    \"ref_draw_rate\", \"high_draw_ref_flag\",\n",
    "    \"both_mid_table\", \"mid_season\", \"late_season\",\n",
    "]\n",
    "draw_feature_cols_to_save = [c for c in draw_feature_cols_to_save if c in train_df.columns]\n",
    "\n",
    "if \"tf_token_features\" not in globals() or len(tf_token_features) == 0:\n",
    "    tf_token_features_to_save = select_tf_token_features(cleaned_df)\n",
    "else:\n",
    "    tf_token_features_to_save = tf_token_features\n",
    "\n",
    "print(f\"draw_feature_cols_to_save: {len(draw_feature_cols_to_save)}\")\n",
    "print(f\"tf_token_features_to_save: {len(tf_token_features_to_save)}\")\n",
    "\n",
    "model_state = {\n",
    "    \"xgb_model\": xgb_final,\n",
    "    \"stage1\": stage1_all,\n",
    "    \"stage2\": stage2_all,\n",
    "    \"alpha\": alpha_final,\n",
    "    \"tau\": tau_final,\n",
    "    \"feature_cols\": feature_cols_xgb,\n",
    "\n",
    "    # ✅ TF model + TF related assets\n",
    "    \"tf_model\": model_tf_loaded,\n",
    "    \"tf_token_features\": tf_token_features_to_save,\n",
    "\n",
    "    # ✅ (strongly recommended) TF normalization params + feature list used by checkpoint\n",
    "    \"tf_all_token_features\": all_token_features_loaded,\n",
    "    \"tf_mu_seq\": mu_seq_loaded,\n",
    "    \"tf_sd_seq\": sd_seq_loaded,\n",
    "    \"tf_mu_match\": mu_match_loaded,\n",
    "    \"tf_sd_match\": sd_match_loaded,\n",
    "\n",
    "    \"dc_posterior\": {\n",
    "        \"attack\": attack_s,\n",
    "        \"defense\": defense_s,\n",
    "        \"home_adv\": home_adv_s,\n",
    "        \"rho\": rho_s,\n",
    "        \"tmap\": tmap,\n",
    "        \"teams\": teams\n",
    "    },\n",
    "\n",
    "    \"draw_model_full\": draw_model_full,\n",
    "    \"draw_feature_cols\": draw_feature_cols_to_save,\n",
    "\n",
    "    \"dc_feature_cols\": dc_feature_cols,\n",
    "    \"dc_scaler\": dc_scaler,\n",
    "    \"beta_h_s\": beta_h_s,\n",
    "    \"beta_a_s\": beta_a_s,\n",
    "\n",
    "    \"fe\": fe,\n",
    "}\n",
    "\n",
    "\n",
    "if \"iso_calibrators\" in globals() and iso_calibrators is not None:\n",
    "    model_state[\"iso_calibrators\"] = iso_calibrators\n",
    "\n",
    "with open(\"epl_model_state.pkl\", \"wb\") as f:\n",
    "    dill.dump(model_state, f)\n",
    "\n",
    "print(\"✅ Model state saved (using dill)\")\n",
    "print(f\"   XGB features: {len(feature_cols_xgb)}\")\n",
    "print(f\"   DC features: {len(dc_feature_cols)}\")\n",
    "print(f\"   Draw features: {len(draw_feature_cols_to_save)}\")\n",
    "print(f\"   TF features: {len(tf_token_features_to_save)}\")\n",
    "print(f\"   FeatureEngineering: ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93a81edd-ba41-4538-8697-20496755cc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENV] DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, dill\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "from sklearn.metrics import log_loss, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[ENV] DEVICE:\", DEVICE)\n",
    "\n",
    "OOF_CKPT_PATH = \"oof_full_checkpoint.pkl\"\n",
    "TRAINING_CSV  = \"epl-training.csv\"\n",
    "MODEL_STATE_PATH = \"epl_model_state.pkl\"\n",
    "STACK_V2_PATH = \"epl_model_state_stack_v2.pkl\"\n",
    "REFIT_OUT     = \"epl_model_state_refit.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ddb4c06-64ea-4d86-aa61-a56bfae4df57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] xgb_model_old: True\n",
      "[LOAD] tf_model_old : True\n",
      "[LOAD] draw_model   : True\n",
      "[LOAD] feature_cols_xgb: 120\n",
      "[LOAD] dc_feature_cols : 12\n",
      "[LOAD] draw_feature_cols: 24\n",
      "[LOAD] tf_token_features: 120\n",
      "[LOAD] fe: True\n"
     ]
    }
   ],
   "source": [
    "with open(MODEL_STATE_PATH, \"rb\") as f:\n",
    "    ms = dill.load(f)\n",
    "\n",
    "# ---- fitted base models (IMPORTANT) ----\n",
    "xgb_model_old = ms.get(\"xgb_model\", None)\n",
    "tf_model_old  = ms.get(\"tf_model\", None)\n",
    "draw_model_full = ms.get(\"draw_model_full\", None)\n",
    "\n",
    "# ---- feature lists ----\n",
    "feature_cols_xgb   = ms.get(\"feature_cols\", [])\n",
    "dc_feature_cols    = ms.get(\"dc_feature_cols\", [])\n",
    "draw_feature_cols  = ms.get(\"draw_feature_cols\", [])\n",
    "tf_token_features  = ms.get(\"tf_token_features\", [])\n",
    "\n",
    "# ---- DC stuff ----\n",
    "dc_scaler = ms.get(\"dc_scaler\", None)\n",
    "beta_h_s  = ms.get(\"beta_h_s\", None)\n",
    "beta_a_s  = ms.get(\"beta_a_s\", None)\n",
    "\n",
    "dc_post = ms.get(\"dc_posterior\", {}) if isinstance(ms.get(\"dc_posterior\", None), dict) else {}\n",
    "attack_s   = dc_post.get(\"attack\", None)\n",
    "defense_s  = dc_post.get(\"defense\", None)\n",
    "home_adv_s = dc_post.get(\"home_adv\", None)\n",
    "rho_s      = dc_post.get(\"rho\", None)\n",
    "tmap       = dc_post.get(\"tmap\", None)\n",
    "teams      = dc_post.get(\"teams\", None)\n",
    "\n",
    "# ---- FE state ----\n",
    "fe = ms.get(\"fe\", None)\n",
    "\n",
    "# ---- optional ----\n",
    "iso_calibrators = ms.get(\"iso_calibrators\", None)\n",
    "\n",
    "print(\"[LOAD] xgb_model_old:\", xgb_model_old is not None)\n",
    "print(\"[LOAD] tf_model_old :\", tf_model_old is not None)\n",
    "print(\"[LOAD] draw_model   :\", draw_model_full is not None)\n",
    "print(\"[LOAD] feature_cols_xgb:\", len(feature_cols_xgb))\n",
    "print(\"[LOAD] dc_feature_cols :\", len(dc_feature_cols))\n",
    "print(\"[LOAD] draw_feature_cols:\", len(draw_feature_cols))\n",
    "print(\"[LOAD] tf_token_features:\", len(tf_token_features))\n",
    "print(\"[LOAD] fe:\", fe is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1eef31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] last_old_date: 2022-05-22 00:00:00\n",
      "[NEW] new_results: (1240, 23) seasons tail: ['2022/2023', '2023/2024', '2024/2025']\n",
      "Added 99 features\n",
      "corner, foul, yellow/red cards history features are added\n",
      "Added advanced H2H features (time-decayed & directional)\n",
      " Added extended draw-specific features\n",
      "[NEW] new_results_fe: (1240, 162) seasons tail: ['2022/2023', '2023/2024', '2024/2025']\n"
     ]
    }
   ],
   "source": [
    "training_all = pd.read_csv(TRAINING_CSV)\n",
    "training_all[\"Date\"] = pd.to_datetime(training_all[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "training_all = training_all.dropna(subset=[\"Date\"]).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "if \"Season\" not in training_all.columns:\n",
    "    training_all[\"Season\"] = training_all[\"Date\"].apply(season_label)\n",
    "\n",
    "with open(OOF_CKPT_PATH, \"rb\") as f:\n",
    "    ckpt = pickle.load(f)\n",
    "\n",
    "meta_df_old = ckpt[\"meta_df\"].copy()\n",
    "meta_df_old[\"Date\"] = pd.to_datetime(meta_df_old[\"Date\"], errors=\"coerce\")\n",
    "meta_df_old = meta_df_old.dropna(subset=[\"Date\"]).sort_values(\"Date\").reset_index(drop=True)\n",
    "last_old_date = meta_df_old[\"Date\"].max()\n",
    "print(\"[NEW] last_old_date:\", last_old_date)\n",
    "\n",
    "if \"FTR\" not in training_all.columns:\n",
    "    raise ValueError(\"epl-training.csv missing required column: FTR\")\n",
    "\n",
    "new_results = training_all[(training_all[\"Date\"] > last_old_date) & (training_all[\"FTR\"].notna())].copy()\n",
    "print(\"[NEW] new_results:\", new_results.shape,\n",
    "      \"seasons tail:\", sorted(new_results[\"Season\"].unique())[-5:])\n",
    "\n",
    "if len(new_results) == 0:\n",
    "    raise ValueError(\"No new completed matches found after last_old_date. Check csv Date/FTR.\")\n",
    "\n",
    "new_results_fe, fe = compute_all_features(\n",
    "    new_results,\n",
    "    fe=fe,\n",
    "    is_train=True,\n",
    "    use_state_features=True,\n",
    "    use_all_adv_block=True,\n",
    "    use_shot_corners=True,\n",
    "    use_td_h2h=True,\n",
    "    use_draw_block=True,\n",
    ")\n",
    "\n",
    "new_results_fe[\"Date\"] = pd.to_datetime(new_results_fe[\"Date\"], errors=\"coerce\")\n",
    "new_results_fe = new_results_fe.dropna(subset=[\"Date\"]).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "print(\"[NEW] new_results_fe:\", new_results_fe.shape,\n",
    "      \"seasons tail:\", sorted(new_results_fe[\"Season\"].unique())[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7badf7e-13a7-4f03-aa54-63afa82e7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_sequences_from_cleaned(\n",
    "    *,\n",
    "    cleaned_df,\n",
    "    target_dfs,              # dict: {\"train_df\": df, ...}\n",
    "    tf_token_features,\n",
    "    seq_len=5,\n",
    "):\n",
    "    # 1) 白名单 TF 特征\n",
    "    safe_features_tf = [f for f in tf_token_features if f in cleaned_df.columns]\n",
    "    if len(safe_features_tf) == 0:\n",
    "        raise ValueError(\"No TF safe features found in cleaned_df\")\n",
    "    print(\"TF safe features:\", len(safe_features_tf))\n",
    "\n",
    "    # 2) 做稳定 key（避免 reset_index 后 join 错位）\n",
    "    def _make_rid(df):\n",
    "        d = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "        return (\n",
    "            d.dt.strftime(\"%Y-%m-%d\").astype(str) + \"||\" +\n",
    "            df[\"HomeTeam\"].astype(str) + \"||\" +\n",
    "            df[\"AwayTeam\"].astype(str)\n",
    "        )\n",
    "\n",
    "    base = cleaned_df.copy()\n",
    "    base[\"Date\"] = pd.to_datetime(base[\"Date\"], errors=\"coerce\")\n",
    "    base = base.sort_values(\"Date\").reset_index(drop=True)\n",
    "    base[\"_rid\"] = _make_rid(base)\n",
    "\n",
    "    # 3) 在 base 上一次性 build seq（你原来的 build_team_sequences_fixed）\n",
    "    base_with_seq, feat_dim_tf = build_team_sequences_fixed(base, safe_features_tf, seq_len=seq_len)\n",
    "    print(f\"[TF seq] seq_len={seq_len}, feat_dim={feat_dim_tf}\")\n",
    "\n",
    "    seq_cols = [\"home_form_seq\", \"away_form_seq\", \"match_features\"]\n",
    "    seq_store = base_with_seq[[\"_rid\"] + seq_cols].copy()\n",
    "\n",
    "    # 4) merge 回各 df（用 _rid，不用 index）\n",
    "    out = {}\n",
    "    for name, df in target_dfs.items():\n",
    "        if df is None:\n",
    "            continue\n",
    "        dfx = df.copy()\n",
    "        dfx[\"Date\"] = pd.to_datetime(dfx[\"Date\"], errors=\"coerce\")\n",
    "        dfx[\"_rid\"] = _make_rid(dfx)\n",
    "\n",
    "        dfx = dfx.drop(columns=seq_cols, errors=\"ignore\")\n",
    "        dfx = dfx.merge(seq_store, on=\"_rid\", how=\"left\")\n",
    "\n",
    "        # 清理\n",
    "        dfx = dfx.drop(columns=[\"_rid\"], errors=\"ignore\")\n",
    "        out[name] = dfx\n",
    "\n",
    "    return out, safe_features_tf, feat_dim_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14bec93b-de6c-4743-a2ec-57951ee2b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _season_start_year(season_str: str) -> int:\n",
    "    try:\n",
    "        return int(str(season_str).split(\"/\")[0])\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def _ensure_y(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"y\" in df.columns:\n",
    "        df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\")\n",
    "    elif \"FTR\" in df.columns:\n",
    "        ftr = df[\"FTR\"].astype(str).str.strip().str.upper()\n",
    "        m = {\"H\":0,\"D\":1,\"A\":2,\"HOME\":0,\"DRAW\":1,\"AWAY\":2}\n",
    "        df[\"y\"] = ftr.map(m)\n",
    "    else:\n",
    "        raise ValueError(\"Need y or FTR.\")\n",
    "    df = df.dropna(subset=[\"y\"]).copy()\n",
    "    df[\"y\"] = df[\"y\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def _ensure_cols(df: pd.DataFrame, cols, fill=0.0) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = fill\n",
    "    return df\n",
    "\n",
    "def _sanitize_proba(P):\n",
    "    P = np.asarray(P, dtype=np.float32)\n",
    "    P = np.nan_to_num(P, nan=1/3, posinf=1/3, neginf=1/3)\n",
    "    P = np.clip(P, 1e-7, 1.0)\n",
    "    s = P.sum(axis=1, keepdims=True)\n",
    "    s = np.where(s <= 0, 1.0, s)\n",
    "    return P / s\n",
    "\n",
    "def _sanitize_p(p, default=0.25):\n",
    "    p = np.asarray(p, dtype=np.float32).reshape(-1)\n",
    "    p = np.nan_to_num(p, nan=default, posinf=default, neginf=default)\n",
    "    return np.clip(p, 1e-6, 1-1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f2d58cf-830a-420c-8a39-0640eb2e2e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ xgb_model_old is already fitted\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Ensure XGB fitted for stacking-expand (refit if needed)\n",
    "#   - Use ONLY past data (Date <= last_old_date) to avoid leakage\n",
    "#   - Train a temp model solely for meta_new proba construction\n",
    "# ==========================================================\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "def _xgb_is_fitted(m):\n",
    "    try:\n",
    "        _ = m.get_booster()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def ensure_xgb_for_expand(\n",
    "    *,\n",
    "    xgb_model_candidate,\n",
    "    feature_cols_xgb,\n",
    "    # data sources for refit\n",
    "    training_all_df,     # full epl-training.csv (raw)\n",
    "    last_old_date,       # from OOF ckpt meta_df_old max date\n",
    "    fe,                  # your feature engine state (already exists)\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a fitted XGB model for stacking-expand.\n",
    "    If candidate is not fitted, refit on historical finished matches up to last_old_date.\n",
    "    \"\"\"\n",
    "    if (xgb_model_candidate is not None) and _xgb_is_fitted(xgb_model_candidate):\n",
    "        print(\"✅ xgb_model_old is already fitted\")\n",
    "        return xgb_model_candidate\n",
    "\n",
    "    print(\"⚠️ xgb_model_old NOT fitted -> refitting a temporary XGB on past data (<= last_old_date)\")\n",
    "\n",
    "    # 1) take only finished matches up to last_old_date\n",
    "    hist = training_all_df.copy()\n",
    "    hist[\"Date\"] = pd.to_datetime(hist[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    hist = hist.dropna(subset=[\"Date\"]).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    if \"Season\" not in hist.columns:\n",
    "        hist[\"Season\"] = hist[\"Date\"].apply(season_label)\n",
    "\n",
    "    hist = hist[(hist[\"Date\"] <= last_old_date) & (hist[\"FTR\"].notna())].copy()\n",
    "    if len(hist) == 0:\n",
    "        raise ValueError(\"No historical finished matches found for XGB refit (Date<=last_old_date).\")\n",
    "\n",
    "    # 2) compute features (IMPORTANT: use a COPY of fe to not pollute your global fe state)\n",
    "    #    如果你 compute_all_features 会更新 fe 状态，这里用浅拷贝避免影响后续流程\n",
    "    import copy\n",
    "    fe_tmp = copy.deepcopy(fe)\n",
    "\n",
    "    hist_fe, _ = compute_all_features(\n",
    "        hist,\n",
    "        fe=fe_tmp,\n",
    "        is_train=True,\n",
    "        use_state_features=True,\n",
    "        use_all_adv_block=True,\n",
    "        use_shot_corners=True,\n",
    "        use_td_h2h=True,\n",
    "        use_draw_block=True,\n",
    "    )\n",
    "\n",
    "    # ensure y exists\n",
    "    if \"y\" not in hist_fe.columns:\n",
    "        ftr = hist_fe[\"FTR\"].astype(str).str.strip().str.upper()\n",
    "        mp = {\"H\":0,\"D\":1,\"A\":2,\"HOME\":0,\"DRAW\":1,\"AWAY\":2}\n",
    "        hist_fe[\"y\"] = ftr.map(mp)\n",
    "\n",
    "    hist_fe = hist_fe.dropna(subset=[\"y\"]).copy()\n",
    "    hist_fe[\"y\"] = hist_fe[\"y\"].astype(int)\n",
    "\n",
    "    # 3) ensure columns\n",
    "    for c in feature_cols_xgb:\n",
    "        if c not in hist_fe.columns:\n",
    "            hist_fe[c] = 0.0\n",
    "\n",
    "    X = hist_fe[feature_cols_xgb].fillna(0.0).to_numpy(np.float32)\n",
    "    y = hist_fe[\"y\"].to_numpy()\n",
    "\n",
    "    # 4) fit a stable temp model (use hist CPU/GPU same as your environment)\n",
    "    xgb_tmp = xgb.XGBClassifier(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        min_child_weight=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=2.0,\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=3,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    xgb_tmp.fit(X, y, verbose=False)\n",
    "    print(\"✅ temporary XGB refit done:\", X.shape)\n",
    "\n",
    "    return xgb_tmp\n",
    "\n",
    "\n",
    "# ---------- RUN THIS ONCE ----------\n",
    "# 你必须有 training_all（就是你从 epl-training.csv 读的那个全量 df）\n",
    "# last_old_date 你前面已经算过\n",
    "xgb_model_old = ensure_xgb_for_expand(\n",
    "    xgb_model_candidate=(xgb_model_old if \"xgb_model_old\" in globals() else None),\n",
    "    feature_cols_xgb=feature_cols_xgb,\n",
    "    training_all_df=training_all,      # ✅ 用你已经读入的 training_all\n",
    "    last_old_date=last_old_date,       # ✅ 用你已经算出的 last_old_date\n",
    "    fe=fe,                             # ✅ 用当前 fe（内部会 deepcopy）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff10167d-7a74-4e4d-8e8b-83515b85ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax_np(logits):\n",
    "    x = logits - logits.max(axis=1, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "def apply_tf_standardization(df_seq, mu_seq, sd_seq, mu_match, sd_match):\n",
    "    mu_seq = np.asarray(mu_seq, dtype=np.float32)\n",
    "    sd_seq = np.asarray(sd_seq, dtype=np.float32)\n",
    "    mu_match = np.asarray(mu_match, dtype=np.float32)\n",
    "    sd_match = np.asarray(sd_match, dtype=np.float32)\n",
    "\n",
    "    sd_seq_safe = sd_seq.copy()\n",
    "    sd_seq_safe[sd_seq_safe < 1e-6] = 1.0\n",
    "    sd_match_safe = sd_match.copy()\n",
    "    sd_match_safe[sd_match_safe < 1e-6] = 1.0\n",
    "\n",
    "    def _norm_seq(x):\n",
    "        x = np.nan_to_num(np.asarray(x, dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return (x - mu_seq) / sd_seq_safe\n",
    "\n",
    "    def _norm_match(x):\n",
    "        x = np.nan_to_num(np.asarray(x, dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return (x - mu_match) / sd_match_safe\n",
    "\n",
    "    df_seq[\"home_form_seq\"] = df_seq[\"home_form_seq\"].apply(_norm_seq)\n",
    "    df_seq[\"away_form_seq\"] = df_seq[\"away_form_seq\"].apply(_norm_seq)\n",
    "    df_seq[\"match_features\"] = df_seq[\"match_features\"].apply(_norm_match)\n",
    "    return df_seq\n",
    "\n",
    "def predict_tf_on_df(model, df_seq):\n",
    "    # 不用 EnhancedMatchDataset（它要 y）\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    home_seq = np.stack(df_seq[\"home_form_seq\"].values).astype(np.float32)\n",
    "    away_seq = np.stack(df_seq[\"away_form_seq\"].values).astype(np.float32)\n",
    "    match_feat = np.stack(df_seq[\"match_features\"].values).astype(np.float32)\n",
    "\n",
    "    home_t = torch.tensor(home_seq, dtype=torch.float32).to(DEVICE)\n",
    "    away_t = torch.tensor(away_seq, dtype=torch.float32).to(DEVICE)\n",
    "    mf_t   = torch.tensor(match_feat, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(home_t, away_t, mf_t).detach().cpu().numpy()\n",
    "    proba = _softmax_np(logits).astype(np.float32)\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52c5997b-cdad-4385-9d7c-e33c5eaa0eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence feature number: 101\n",
      "Sequence feature : ['form_home', 'form_away', 'form_diff', 'form_home_v2', 'form_away_v2', 'form_diff_v2', 'home_home_form', 'away_away_form', 'win_streak_home', 'win_streak_away']...\n",
      "[EXPAND] all: (9220, 10) (9220,)\n",
      "[EXPAND] ✅ stage1/stage2 refit done\n",
      "[EXPAND] alpha = 0.629\n",
      "[EXPAND] tau = 0.270 (macro-F1=0.4421)\n",
      "[EXPAND] ✅ saved -> epl_model_state_stack_v2.pkl\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Load OOF checkpoint (STRICT alignment)\n",
    "# =====================================================\n",
    "with open(OOF_CKPT_PATH, \"rb\") as f:\n",
    "    ckpt = pickle.load(f)\n",
    "\n",
    "meta_df_ckpt = ckpt[\"meta_df\"].copy()\n",
    "oof_xgb = ckpt[\"oof_xgb\"]\n",
    "oof_dc  = ckpt[\"oof_dc\"]\n",
    "oof_tf  = ckpt[\"oof_tf\"]\n",
    "\n",
    "assert len(meta_df_ckpt) == oof_xgb.shape[0], \"❌ meta_df_ckpt / oof mismatch\"\n",
    "\n",
    "# =====================================================\n",
    "# Load TF checkpoint\n",
    "# =====================================================\n",
    "(\n",
    "    model_tf_loaded,\n",
    "    all_token_features_loaded,\n",
    "    mu_seq_loaded,\n",
    "    sd_seq_loaded,\n",
    "    mu_match_loaded,\n",
    "    sd_match_loaded,\n",
    ") = load_enhanced_tf_checkpoint()\n",
    "\n",
    "# =====================================================\n",
    "# OLD OOF PART\n",
    "# =====================================================\n",
    "mask_ok_old = ~(\n",
    "    np.isnan(oof_xgb).any(axis=1) |\n",
    "    np.isnan(oof_dc).any(axis=1)  |\n",
    "    np.isnan(oof_tf).any(axis=1)\n",
    ")\n",
    "\n",
    "meta_oof_old = meta_df_ckpt.loc[mask_ok_old].reset_index(drop=True)\n",
    "meta_oof_old = _ensure_y(meta_oof_old)\n",
    "\n",
    "y_old = meta_oof_old[\"y\"].astype(int).values\n",
    "season_old = meta_oof_old[\"Season\"].values\n",
    "\n",
    "oof_xgb_ok = _sanitize_proba(oof_xgb[mask_ok_old])\n",
    "oof_dc_ok  = _sanitize_proba(oof_dc[mask_ok_old])\n",
    "oof_tf_ok  = _sanitize_proba(oof_tf[mask_ok_old])\n",
    "\n",
    "X_base_old = np.hstack([oof_xgb_ok, oof_dc_ok, oof_tf_ok]).astype(np.float32)\n",
    "\n",
    "# ---- pD_special (OLD)\n",
    "if draw_model_full is not None and draw_feature_cols:\n",
    "    meta_oof_old = _ensure_cols(meta_oof_old, draw_feature_cols, fill=0.0)\n",
    "    X_draw_old = meta_oof_old[draw_feature_cols].fillna(0.0).to_numpy(np.float32)\n",
    "    pD_special_old = _sanitize_p(\n",
    "        draw_model_full.predict_proba(X_draw_old)[:, 1], default=0.25\n",
    "    )\n",
    "else:\n",
    "    pD_special_old = _sanitize_p(\n",
    "        (oof_xgb_ok[:,1] + oof_dc_ok[:,1] + oof_tf_ok[:,1]) / 3.0, default=0.25\n",
    "    )\n",
    "\n",
    "X_meta_old = np.hstack([X_base_old, pD_special_old[:, None]]).astype(np.float32)\n",
    "\n",
    "# =====================================================\n",
    "# NEW DATA PART\n",
    "# =====================================================\n",
    "meta_new = _ensure_y(new_results_fe).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "# ---- XGB\n",
    "meta_new = _ensure_cols(meta_new, feature_cols_xgb, fill=0.0)\n",
    "X_new_xgb = meta_new[feature_cols_xgb].fillna(0.0).to_numpy(np.float32)\n",
    "proba_new_xgb = _sanitize_proba(xgb_model_old.predict_proba(X_new_xgb))\n",
    "\n",
    "# ---- DC\n",
    "if dc_scaler is not None and dc_feature_cols and beta_h_s is not None:\n",
    "    meta_new = _ensure_cols(meta_new, dc_feature_cols, fill=0.0)\n",
    "    X_dc = dc_scaler.transform(meta_new[dc_feature_cols].fillna(0.0))\n",
    "    try:\n",
    "        proba_new_dc, _ = bayes_dc_predict_full(\n",
    "            meta_new, X_dc,\n",
    "            attack_s, defense_s, home_adv_s, rho_s,\n",
    "            beta_h_s, beta_a_s, tmap\n",
    "        )\n",
    "        proba_new_dc = _sanitize_proba(proba_new_dc)\n",
    "    except Exception as e:\n",
    "        print(\"[EXPAND] ⚠️ DC failed → uniform\", e)\n",
    "        proba_new_dc = np.full_like(proba_new_xgb, 1/3)\n",
    "else:\n",
    "    proba_new_dc = np.full_like(proba_new_xgb, 1/3)\n",
    "\n",
    "# ---- TF (FIXED & SAFE)\n",
    "def tf_predict_from_ckpt(df_feat):\n",
    "    df_seq, _ = build_team_sequences_fixed(\n",
    "        df_feat.copy(), all_token_features_loaded, seq_len=5\n",
    "    )\n",
    "    df_seq = apply_tf_standardization(\n",
    "        df_seq,\n",
    "        mu_seq_loaded, sd_seq_loaded,\n",
    "        mu_match_loaded, sd_match_loaded\n",
    "    )\n",
    "    return predict_tf_on_df(model_tf_loaded, df_seq)\n",
    "\n",
    "if TORCH_OK and model_tf_loaded is not None:\n",
    "    try:\n",
    "        proba_new_tf = _sanitize_proba(tf_predict_from_ckpt(meta_new))\n",
    "    except Exception as e:\n",
    "        print(\"[EXPAND] ⚠️ TF failed → uniform\", e)\n",
    "        proba_new_tf = np.full_like(proba_new_xgb, 1/3)\n",
    "else:\n",
    "    proba_new_tf = np.full_like(proba_new_xgb, 1/3)\n",
    "\n",
    "# ---- pD_special (NEW)\n",
    "if draw_model_full is not None and draw_feature_cols:\n",
    "    meta_new = _ensure_cols(meta_new, draw_feature_cols, fill=0.0)\n",
    "    X_draw_new = meta_new[draw_feature_cols].fillna(0.0).to_numpy(np.float32)\n",
    "    pD_special_new = _sanitize_p(\n",
    "        draw_model_full.predict_proba(X_draw_new)[:, 1], default=0.25\n",
    "    )\n",
    "else:\n",
    "    pD_special_new = _sanitize_p(\n",
    "        (proba_new_xgb[:,1] + proba_new_dc[:,1] + proba_new_tf[:,1]) / 3.0, default=0.25\n",
    "    )\n",
    "\n",
    "X_meta_new = np.hstack([\n",
    "    proba_new_xgb,\n",
    "    proba_new_dc,\n",
    "    proba_new_tf,\n",
    "    pD_special_new[:, None]\n",
    "]).astype(np.float32)\n",
    "\n",
    "y_new = meta_new[\"y\"].astype(int).values\n",
    "season_new = meta_new[\"Season\"].values\n",
    "\n",
    "# =====================================================\n",
    "# MERGE + CLEAN\n",
    "# =====================================================\n",
    "X_meta_all = np.vstack([X_meta_old, X_meta_new])\n",
    "y_all = np.concatenate([y_old, y_new])\n",
    "season_all = np.concatenate([season_old, season_new])\n",
    "\n",
    "ok = np.isfinite(X_meta_all).all(axis=1)\n",
    "X_meta_all = X_meta_all[ok]\n",
    "y_all = y_all[ok]\n",
    "season_all = season_all[ok]\n",
    "\n",
    "print(\"[EXPAND] all:\", X_meta_all.shape, y_all.shape)\n",
    "\n",
    "# =====================================================\n",
    "# FINAL two-stage REFIT + alpha/tau\n",
    "# =====================================================\n",
    "cw_draw = 2.0\n",
    "stage1_all, stage2_all = fit_two_stage(\n",
    "    X_meta_all, y_all, cw_draw=cw_draw, C1=0.5, C2=0.5\n",
    ")\n",
    "print(\"[EXPAND] ✅ stage1/stage2 refit done\")\n",
    "\n",
    "seasons_all = sorted(pd.unique(season_all), key=_season_start_year)\n",
    "alpha_grid = np.linspace(0.5, 1.0, 11)\n",
    "tau_grid   = np.linspace(0.22, 0.35, 14)\n",
    "\n",
    "# ---- alpha\n",
    "alphas = []\n",
    "for s in seasons_all:\n",
    "    tr, va = season_all != s, season_all == s\n",
    "    if tr.sum() == 0 or va.sum() == 0:\n",
    "        continue\n",
    "    st1, st2 = fit_two_stage(X_meta_all[tr], y_all[tr], cw_draw=cw_draw)\n",
    "    pD = st1.predict_proba(X_meta_all[va])[:,1]\n",
    "    pA = st2.predict_proba(X_meta_all[va])[:,1]\n",
    "    best = min(alpha_grid, key=lambda a: log_loss(\n",
    "        y_all[va], make_proba_two_stage_alpha(pD, pA, a), labels=[0,1,2]\n",
    "    ))\n",
    "    alphas.append(best)\n",
    "\n",
    "alpha_final = float(np.mean(alphas)) if alphas else 0.85\n",
    "print(f\"[EXPAND] alpha = {alpha_final:.3f}\")\n",
    "\n",
    "# ---- tau\n",
    "best_tau, best_f1 = 0.25, -1\n",
    "for tau in tau_grid:\n",
    "    f1s = []\n",
    "    for s in seasons_all:\n",
    "        tr, va = season_all != s, season_all == s\n",
    "        if tr.sum() == 0 or va.sum() == 0:\n",
    "            continue\n",
    "        st1, st2 = fit_two_stage(X_meta_all[tr], y_all[tr], cw_draw=cw_draw)\n",
    "        P = make_proba_two_stage_alpha(\n",
    "            st1.predict_proba(X_meta_all[va])[:,1],\n",
    "            st2.predict_proba(X_meta_all[va])[:,1],\n",
    "            alpha_final\n",
    "        )\n",
    "        yhat = np.where(P[:,1] > tau, 1, np.where(P[:,2] > P[:,0], 2, 0))\n",
    "        f1s.append(f1_score(y_all[va], yhat, average=\"macro\"))\n",
    "    if f1s and np.mean(f1s) > best_f1:\n",
    "        best_f1, best_tau = np.mean(f1s), tau\n",
    "\n",
    "print(f\"[EXPAND] tau = {best_tau:.3f} (macro-F1={best_f1:.4f})\")\n",
    "\n",
    "# =====================================================\n",
    "# SAVE\n",
    "# =====================================================\n",
    "ms_stack_v2 = {\n",
    "    \"stage1\": stage1_all,\n",
    "    \"stage2\": stage2_all,\n",
    "    \"alpha\": alpha_final,\n",
    "    \"tau\": best_tau,\n",
    "}\n",
    "if iso_calibrators is not None:\n",
    "    ms_stack_v2[\"iso_calibrators\"] = iso_calibrators\n",
    "\n",
    "with open(STACK_V2_PATH, \"wb\") as f:\n",
    "    dill.dump(ms_stack_v2, f)\n",
    "\n",
    "print(f\"[EXPAND] ✅ saved -> {STACK_V2_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d1e1c50-1c17-4bee-aaaf-0b3c552f127d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD overlap with TF seq cols: set()\n"
     ]
    }
   ],
   "source": [
    "bad = set(feature_cols_xgb + dc_feature_cols) & {\n",
    "    \"home_form_seq\", \"away_form_seq\", \"match_features\"\n",
    "}\n",
    "print(\"BAD overlap with TF seq cols:\", bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47c76cfd-32eb-479d-91b6-72b80a97000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[train_df] rows=7980 seasons=['2018/2019', '2019/2020', '2020/2021']\n",
      "  home_form_seq: exists=True, ndarray_ratio=1.000\n",
      "  away_form_seq: exists=True, ndarray_ratio=1.000\n",
      "  match_features: exists=True, ndarray_ratio=1.000\n",
      "\n",
      "[val_df] rows=380 seasons=['2021/2022']\n",
      "  home_form_seq: exists=True, ndarray_ratio=1.000\n",
      "  away_form_seq: exists=True, ndarray_ratio=1.000\n",
      "  match_features: exists=True, ndarray_ratio=1.000\n",
      "\n",
      "[test_df] rows=1140 seasons=['2022/2023', '2023/2024', '2024/2025']\n",
      "  home_form_seq: exists=True, ndarray_ratio=1.000\n",
      "  away_form_seq: exists=True, ndarray_ratio=1.000\n",
      "  match_features: exists=True, ndarray_ratio=1.000\n"
     ]
    }
   ],
   "source": [
    "def check_seq_integrity(df, name):\n",
    "    cols = [\"home_form_seq\",\"away_form_seq\",\"match_features\"]\n",
    "    print(f\"\\n[{name}] rows={len(df)} seasons={sorted(df['Season'].unique())[-3:]}\")\n",
    "    for c in cols:\n",
    "        ok = df[c].apply(lambda x: isinstance(x, np.ndarray)).mean() if c in df.columns else 0\n",
    "        print(f\"  {c}: exists={c in df.columns}, ndarray_ratio={ok:.3f}\")\n",
    "        if c in df.columns:\n",
    "            bad = df[~df[c].apply(lambda x: isinstance(x, np.ndarray))][c].head(5).tolist()\n",
    "            if bad:\n",
    "                print(\"    examples of bad values:\", bad)\n",
    "\n",
    "check_seq_integrity(train_df, \"train_df\")\n",
    "check_seq_integrity(val_df,   \"val_df\")\n",
    "check_seq_integrity(test_df,  \"test_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98443390-4f90-4fab-be21-bcfa67b790ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OOF] season=2022/2023  train<= 2021/2022  n_train=8360  n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 92 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF fold] class_counts=[3836 2085 2439], class_weights={0: np.float64(0.7264511644073688), 1: np.float64(1.3365307753796962), 2: np.float64(1.1425447587809212)}\n",
      "  home_seq range: [-47.39, 47.38]\n",
      "  away_seq range: [-47.39, 47.38]\n",
      "  match_feat range: [-5.58, 5.60]\n",
      "  home_seq range: [-9.97, 7.09]\n",
      "  away_seq range: [-9.97, 7.09]\n",
      "  match_feat range: [-5.18, 5.60]\n",
      "[TF fold] seq_len=5, feat_dim=120, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.0964 | train_acc=0.389\n",
      "         | val_logloss=1.0417\n",
      "         | train_pred_dist: H=0.39 D=0.26 A=0.34\n",
      "         | val_pred_dist:   H=0.37 D=0.40 A=0.23\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0835 | train_acc=0.414\n",
      "         | val_logloss=1.0461\n",
      "         | train_pred_dist: H=0.38 D=0.26 A=0.36\n",
      "         | val_pred_dist:   H=0.54 D=0.02 A=0.44\n",
      "Epoch 06 | lr=0.000500 | train_loss=1.0716 | train_acc=0.423\n",
      "         | val_logloss=1.0633\n",
      "         | train_pred_dist: H=0.36 D=0.33 A=0.31\n",
      "         | val_pred_dist:   H=0.38 D=0.12 A=0.50\n",
      "Early stopping at epoch 7\n",
      "\n",
      " Best val logloss: 1.0417\n",
      "  home_seq range: [-9.97, 7.09]\n",
      "  away_seq range: [-9.97, 7.09]\n",
      "  match_feat range: [-5.18, 5.60]\n",
      "[OOF] wrote season 2022/2023: filled rows=380\n",
      "\n",
      "[OOF] season=2023/2024  train<= 2022/2023  n_train=8740  n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 97 seconds.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF fold] class_counts=[4020 2172 2548], class_weights={0: np.float64(0.724709784411277), 1: np.float64(1.3413136893799877), 2: np.float64(1.1433804290947147)}\n",
      "  home_seq range: [-48.34, 48.34]\n",
      "  away_seq range: [-48.34, 48.34]\n",
      "  match_feat range: [-5.59, 5.61]\n",
      "  home_seq range: [-10.07, 7.85]\n",
      "  away_seq range: [-10.07, 7.85]\n",
      "  match_feat range: [-3.59, 5.61]\n",
      "[TF fold] seq_len=5, feat_dim=120, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.0954 | train_acc=0.399\n",
      "         | val_logloss=1.0897\n",
      "         | train_pred_dist: H=0.38 D=0.26 A=0.36\n",
      "         | val_pred_dist:   H=0.17 D=0.38 A=0.46\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0827 | train_acc=0.418\n",
      "         | val_logloss=1.0525\n",
      "         | train_pred_dist: H=0.41 D=0.24 A=0.35\n",
      "         | val_pred_dist:   H=0.44 D=0.24 A=0.32\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0783 | train_acc=0.418\n",
      "         | val_logloss=1.0354\n",
      "         | train_pred_dist: H=0.37 D=0.30 A=0.33\n",
      "         | val_pred_dist:   H=0.49 D=0.02 A=0.49\n",
      "Epoch 09 | lr=0.001000 | train_loss=1.0720 | train_acc=0.431\n",
      "         | val_logloss=1.0454\n",
      "         | train_pred_dist: H=0.39 D=0.24 A=0.37\n",
      "         | val_pred_dist:   H=0.47 D=0.02 A=0.51\n",
      "Epoch 12 | lr=0.000500 | train_loss=1.0532 | train_acc=0.447\n",
      "         | val_logloss=1.0916\n",
      "         | train_pred_dist: H=0.36 D=0.30 A=0.34\n",
      "         | val_pred_dist:   H=0.18 D=0.22 A=0.60\n",
      "Early stopping at epoch 12\n",
      "\n",
      " Best val logloss: 1.0354\n",
      "  home_seq range: [-10.07, 7.85]\n",
      "  away_seq range: [-10.07, 7.85]\n",
      "  match_feat range: [-3.59, 5.61]\n",
      "[OOF] wrote season 2023/2024: filled rows=380\n",
      "\n",
      "[OOF] season=2024/2025  train<= 2023/2024  n_train=9120  n_val=380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [sigma_att, sigma_def, att_offset, def_offset, home_adv, rho_raw]\n",
      "Sampling 4 chains for 300 tune and 600 draw iterations (1_200 + 2_400 draws total) took 103 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF fold] class_counts=[4195 2254 2671], class_weights={0: np.float64(0.7246722288438617), 1: np.float64(1.3487133984028394), 2: np.float64(1.1381505054286785)}\n",
      "  home_seq range: [-49.23, 49.23]\n",
      "  away_seq range: [-49.23, 49.23]\n",
      "  match_feat range: [-5.60, 5.61]\n",
      "  home_seq range: [-66.94, 66.93]\n",
      "  away_seq range: [-66.94, 66.93]\n",
      "  match_feat range: [-5.60, 5.61]\n",
      "[TF fold] seq_len=5, feat_dim=120, match_feat_dim=3\n",
      "Epoch 01 | lr=0.001000 | train_loss=1.0947 | train_acc=0.397\n",
      "         | val_logloss=1.1118\n",
      "         | train_pred_dist: H=0.38 D=0.28 A=0.34\n",
      "         | val_pred_dist:   H=0.20 D=0.38 A=0.42\n",
      "Epoch 03 | lr=0.001000 | train_loss=1.0804 | train_acc=0.426\n",
      "         | val_logloss=1.0908\n",
      "         | train_pred_dist: H=0.42 D=0.23 A=0.35\n",
      "         | val_pred_dist:   H=0.49 D=0.16 A=0.35\n",
      "Epoch 06 | lr=0.001000 | train_loss=1.0761 | train_acc=0.427\n",
      "         | val_logloss=1.1303\n",
      "         | train_pred_dist: H=0.39 D=0.26 A=0.35\n",
      "         | val_pred_dist:   H=0.26 D=0.14 A=0.60\n",
      "Epoch 09 | lr=0.000500 | train_loss=1.0622 | train_acc=0.438\n",
      "         | val_logloss=1.1013\n",
      "         | train_pred_dist: H=0.35 D=0.32 A=0.33\n",
      "         | val_pred_dist:   H=0.43 D=0.22 A=0.35\n",
      "Early stopping at epoch 9\n",
      "\n",
      " Best val logloss: 1.0908\n",
      "  home_seq range: [-66.94, 66.93]\n",
      "  away_seq range: [-66.94, 66.93]\n",
      "  match_feat range: [-5.60, 5.61]\n",
      "[OOF] wrote season 2024/2025: filled rows=380\n",
      "\n",
      "[OOF] filled rows: 1140 / 9500\n",
      "[OOF] filled seasons: ['2022/2023', '2023/2024', '2024/2025']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# EXPANDING OOF for target seasons (e.g. test seasons)\n",
    "#   - meta_df_all = train+val+test\n",
    "#   - for each season s: train = all seasons < s, val = season==s\n",
    "#   - fit XGB/DC/TF on train_fold, predict on val_fold, write OOF\n",
    "#   - TF fold uses standardize_sequences(train_fold, val_fold) BEFORE training/infer\n",
    "# ==========================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------- you choose which seasons to OOF ----------\n",
    "target_oof_seasons = [\"2022/2023\", \"2023/2024\", \"2024/2025\"]  # change if needed\n",
    "\n",
    "# -------- build meta_df_all ----------\n",
    "meta_df_all = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "meta_df_all = meta_df_all.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "# ensure _orig_idx for OOF write-back\n",
    "meta_df_all[\"_orig_idx\"] = np.arange(len(meta_df_all))\n",
    "orig_to_pos = {i:i for i in range(len(meta_df_all))}\n",
    "\n",
    "# -------- utilities ----------\n",
    "def _ensure_cols(df: pd.DataFrame, cols, fill=0.0):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = fill\n",
    "    return df\n",
    "\n",
    "def _sanitize_seq_cell(x, seq_len, feat_dim):\n",
    "    # x should be ndarray (seq_len, feat_dim)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        arr = x.astype(np.float32, copy=False)\n",
    "        if arr.ndim == 2 and arr.shape == (seq_len, feat_dim):\n",
    "            return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # fallback\n",
    "    return np.zeros((seq_len, feat_dim), dtype=np.float32)\n",
    "\n",
    "def _sanitize_match_cell(x, match_dim):\n",
    "    # x should be ndarray (match_dim,)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        arr = x.astype(np.float32, copy=False)\n",
    "        if arr.ndim == 1 and arr.shape[0] == match_dim:\n",
    "            return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return np.zeros((match_dim,), dtype=np.float32)\n",
    "\n",
    "def _infer_seq_shapes(df):\n",
    "    # find first valid row to infer dims\n",
    "    for v in df[\"home_form_seq\"].values:\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 2:\n",
    "            return int(v.shape[0]), int(v.shape[1])\n",
    "    raise ValueError(\"Cannot infer TF seq shape: no ndarray found in home_form_seq.\")\n",
    "\n",
    "def _infer_match_dim(df):\n",
    "    for v in df[\"match_features\"].values:\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 1:\n",
    "            return int(v.shape[0])\n",
    "    raise ValueError(\"Cannot infer match_features dim: no ndarray found.\")\n",
    "\n",
    "def _safe_predict_tf(model, df_fold):\n",
    "    # assume df_fold already standardized and has TF seq cols\n",
    "    return predict_tf_fold(model, df_fold)\n",
    "\n",
    "# -------- check TF seq cols exist if TF is enabled ----------\n",
    "TF_SEQ_COLS = [\"home_form_seq\", \"away_form_seq\", \"match_features\"]\n",
    "if TORCH_OK and DO_TF_OOF:\n",
    "    missing = [c for c in TF_SEQ_COLS if c not in meta_df_all.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"❌ meta_df_all missing TF seq cols {missing}. \"\n",
    "            \"Run your build_tf_sequences_from_cleaned(...) join step BEFORE this cell.\"\n",
    "        )\n",
    "\n",
    "# -------- allocate OOF arrays for ALL rows (we'll only fill targets) ----------\n",
    "N = len(meta_df_all)\n",
    "oof_xgb = np.full((N, 3), np.nan, dtype=np.float32)\n",
    "oof_dc  = np.full((N, 3), np.nan, dtype=np.float32)\n",
    "oof_tf  = np.full((N, 3), np.nan, dtype=np.float32)\n",
    "\n",
    "# -------- run expanding OOF on selected seasons ----------\n",
    "for s in target_oof_seasons:\n",
    "    # expanding: train uses strictly earlier seasons\n",
    "    s_year = season_start_year(s)\n",
    "    train_fold = meta_df_all[meta_df_all[\"Season\"].apply(season_start_year) < s_year].copy()\n",
    "    val_fold   = meta_df_all[meta_df_all[\"Season\"] == s].copy()\n",
    "\n",
    "    if len(train_fold) == 0 or len(val_fold) == 0:\n",
    "        print(f\"[OOF] skip {s}: train={len(train_fold)} val={len(val_fold)}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n[OOF] season={s}  train<= {train_fold['Season'].max()}  n_train={len(train_fold)}  n_val={len(val_fold)}\")\n",
    "\n",
    "    # ---------------- XGB ----------------\n",
    "    train_fold = _ensure_cols(train_fold, feature_cols_xgb, fill=0.0)\n",
    "    val_fold   = _ensure_cols(val_fold,   feature_cols_xgb, fill=0.0)\n",
    "\n",
    "    xgb_fold = fit_xgb_fold(train_fold)   # your existing function\n",
    "    Xv = val_fold[feature_cols_xgb].fillna(0.0).to_numpy(np.float32)\n",
    "    proba_xgb = xgb_fold.predict_proba(Xv).astype(np.float32)\n",
    "\n",
    "    # ---------------- DC ----------------\n",
    "    # (you can switch to your proper fold training or global posterior fallback)\n",
    "    try:\n",
    "        if DO_DC_OOF_PROPER:\n",
    "            dc_state = fit_dc_fold_proper(train_fold, draws=600, tune=300)\n",
    "            proba_dc = predict_dc_fold_proper(dc_state, val_fold).astype(np.float32)\n",
    "        else:\n",
    "            raise RuntimeError(\"DO_DC_OOF_PROPER=False\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [OOF] DC fallback to global posterior: {e}\")\n",
    "        val_fold = _ensure_cols(val_fold, dc_feature_cols, fill=0.0)\n",
    "        X_dc = dc_scaler.transform(val_fold[dc_feature_cols].copy().fillna(0.0))\n",
    "        proba_dc, _ = bayes_dc_predict_full(\n",
    "            val_fold, X_dc,\n",
    "            attack_s, defense_s, home_adv_s, rho_s,\n",
    "            beta_h_s, beta_a_s, tmap\n",
    "        )\n",
    "        proba_dc = np.asarray(proba_dc, dtype=np.float32)\n",
    "\n",
    "    # ---------------- TF ----------------\n",
    "    if TORCH_OK and DO_TF_OOF:\n",
    "        # infer dims & sanitize seq cells (prevents float/None causing astype crash)\n",
    "        seq_len, feat_dim = _infer_seq_shapes(train_fold)\n",
    "        match_dim = _infer_match_dim(train_fold)\n",
    "\n",
    "        for col in [\"home_form_seq\", \"away_form_seq\"]:\n",
    "            train_fold[col] = train_fold[col].apply(lambda x: _sanitize_seq_cell(x, seq_len, feat_dim))\n",
    "            val_fold[col]   = val_fold[col].apply(lambda x: _sanitize_seq_cell(x, seq_len, feat_dim))\n",
    "        train_fold[\"match_features\"] = train_fold[\"match_features\"].apply(lambda x: _sanitize_match_cell(x, match_dim))\n",
    "        val_fold[\"match_features\"]   = val_fold[\"match_features\"].apply(lambda x: _sanitize_match_cell(x, match_dim))\n",
    "\n",
    "        # IMPORTANT: standardize using TRAIN stats only\n",
    "        train_fold_std, val_fold_std, _, *_ = standardize_sequences(train_fold.copy(), val_fold.copy(), None)\n",
    "\n",
    "        tf_fold = fit_tf_fold(train_fold_std, val_fold_std, epochs=25, patience=6)  # your function\n",
    "        proba_tf = _safe_predict_tf(tf_fold, val_fold_std).astype(np.float32)\n",
    "    else:\n",
    "        proba_tf = np.full_like(proba_xgb, 1/3, dtype=np.float32)\n",
    "\n",
    "    # ---------------- write OOF back ----------------\n",
    "    for oid, px, pd_, pt_ in zip(val_fold[\"_orig_idx\"].values, proba_xgb, proba_dc, proba_tf):\n",
    "        pos = orig_to_pos[int(oid)]\n",
    "        oof_xgb[pos] = px\n",
    "        oof_dc[pos]  = pd_\n",
    "        oof_tf[pos]  = pt_\n",
    "\n",
    "    print(f\"[OOF] wrote season {s}: filled rows={len(val_fold)}\")\n",
    "\n",
    "# -------- quick sanity ----------\n",
    "mask_done = ~np.isnan(oof_xgb).any(1)\n",
    "print(\"\\n[OOF] filled rows:\", int(mask_done.sum()), \"/\", N)\n",
    "print(\"[OOF] filled seasons:\", sorted(meta_df_all.loc[mask_done, \"Season\"].unique(), key=season_start_year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac00f74d-247a-4c78-83df-62f239c5bddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved production refit model as epl_model_state_refit.pkl\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Safe pack refit_state\n",
    "# ===========================\n",
    "def _get(name, default=None):\n",
    "    return globals().get(name, default)\n",
    "\n",
    "refit_state = {\n",
    "    # ---- base models (final refit versions, if you have them) ----\n",
    "    \"xgb_model\": _get(\"xgb_final\", _get(\"xgb_model_old\", _get(\"xgb_model\", None))),\n",
    "    \"dc_posterior_refit\": _get(\"dc_final_state\", None),          # may be None\n",
    "    \"tf_model_refit\": _get(\"model_tf_final\", _get(\"model_tf_loaded\", None)),\n",
    "\n",
    "    # ---- stack/meta ----\n",
    "    \"stage1\": _get(\"stage1_all_new\", _get(\"stage1_all\", None)),\n",
    "    \"stage2\": _get(\"stage2_all_new\", _get(\"stage2_all\", None)),\n",
    "    \"alpha\":  _get(\"alpha_final_new\", _get(\"alpha_final\", None)),\n",
    "    \"tau\":    _get(\"tau_final_new\", _get(\"tau_final\", None)),\n",
    "    \"iso_calibrators\": _get(\"iso_calibrators\", None),\n",
    "\n",
    "    # ---- feature/schema ----\n",
    "    \"feature_cols\": _get(\"feature_cols_xgb\", []),\n",
    "    \"dc_feature_cols\": _get(\"dc_feature_cols\", []),\n",
    "    \"dc_scaler\": _get(\"dc_scaler\", None),\n",
    "\n",
    "    \"tf_token_features\": _get(\"tf_token_features\", _get(\"all_token_features_loaded\", [])) \\\n",
    "        if isinstance(_get(\"tf_token_features\", _get(\"all_token_features_loaded\", [])), (list, tuple)) else [],\n",
    "\n",
    "    \"draw_feature_cols\": _get(\"draw_feature_cols\", []) if isinstance(_get(\"draw_feature_cols\", []), (list, tuple)) else [],\n",
    "    \"draw_model_full\": _get(\"draw_model_full\", None),\n",
    "    \"fe\": _get(\"fe\", None),\n",
    "\n",
    "    # ---- TF standardization params if available ----\n",
    "    \"tf_norm\": {\n",
    "        \"mu_seq\": _get(\"mu_seq_loaded\", None),\n",
    "        \"sd_seq\": _get(\"sd_seq_loaded\", None),\n",
    "        \"mu_match\": _get(\"mu_match_loaded\", None),\n",
    "        \"sd_match\": _get(\"sd_match_loaded\", None),\n",
    "    },\n",
    "\n",
    "    # ---- global DC posterior snapshot (if exists) ----\n",
    "    \"dc_posterior_global\": {\n",
    "        \"attack\": _get(\"attack_s\", None),\n",
    "        \"defense\": _get(\"defense_s\", None),\n",
    "        \"home_adv\": _get(\"home_adv_s\", None),\n",
    "        \"rho\": _get(\"rho_s\", None),\n",
    "        \"beta_h_s\": _get(\"beta_h_s\", None),\n",
    "        \"beta_a_s\": _get(\"beta_a_s\", None),\n",
    "        \"tmap\": _get(\"tmap\", None),\n",
    "        \"teams\": _get(\"teams\", None),\n",
    "    },\n",
    "\n",
    "    \"refit_info\": {\n",
    "        \"type\": \"STACK_V2_PLUS_FINAL_REFIT\",\n",
    "        \"time_decay_tau\": _get(\"TIME_DECAY_TAU\", None),\n",
    "        \"draw_boost\": _get(\"DRAW_CLASS_MULT\", None),\n",
    "        \"target_oof_seasons\": _get(\"target_oof_seasons\", None),\n",
    "    }\n",
    "}\n",
    "\n",
    "# choose output path safely\n",
    "REFIT_OUT_SAFE = _get(\"REFIT_OUT\", \"refit_state.pkl\")\n",
    "\n",
    "with open(REFIT_OUT_SAFE, \"wb\") as f:\n",
    "    dill.dump(refit_state, f)\n",
    "\n",
    "print(f\"✅ Saved production refit model as {REFIT_OUT_SAFE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c096cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      " Elo :  ()\n",
      " Dixon-Coles OOF:  fold  posterior\n",
      " : H2H \n",
      " : \n",
      " : /\n",
      " : lambda=0.00325 (~tau~308d) + draw boost x3\n",
      " : draw threshold grid 0.22-0.35 (target~0.25)\n",
      " Isotonic : \n",
      "\n",
      ": 120\n",
      " alpha: 0.629\n",
      " tau: 0.269\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\")\n",
    "print(\"=\"*60)\n",
    "print(\" Elo :  ()\")\n",
    "print(\" Dixon-Coles OOF:  fold  posterior\")\n",
    "print(\" : H2H \")\n",
    "print(\" : \")\n",
    "print(\" : /\")\n",
    "print(\" : lambda=0.00325 (~tau~308d) + draw boost x3\")\n",
    "print(\" : draw threshold grid 0.22-0.35 (target~0.25)\")\n",
    "print(\" Isotonic : \" if USE_ISOTONIC else \" Isotonic : \")\n",
    "print(f\"\\n: {len(feature_cols_xgb)}\")\n",
    "print(f\" alpha: {alpha_final:.3f}\")\n",
    "print(f\" tau: {tau_final:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57765b9",
   "metadata": {},
   "source": [
    "## 7. Final predictions on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089bfbe7",
   "metadata": {},
   "source": [
    "### Output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a5c331a5-905e-45ae-8848-448e8a9447ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_with_threshold_v2(\n",
    "    proba,\n",
    "    tau=0.269,\n",
    "    draw_bias=0.0,\n",
    "    ha_margin=0.06,        # ✅ 调小：让像热刺这种(差0.052)更容易进②从而保持A\n",
    "    draw_margin=0.03,\n",
    "    ha_close=0.03,         # ✅ 新增：H/A 很接近(<=0.03)时更偏向给D\n",
    "    tau_soft=0.21          # ✅ 新增：兜底给D时，pD至少要到这个水平\n",
    "):\n",
    "    \"\"\"\n",
    "    目标：\n",
    "    - ① 若 D 接近最大且 >= tau -> D（强D）\n",
    "    - ② 若 H/A 差距明显 -> 选更大者（强方向）\n",
    "    - ③ 否则：若 H/A 非常接近 且 pD 不太低 -> D（弱D）\n",
    "    - ④ 否则：回到 H/A（谁大选谁）\n",
    "    \"\"\"\n",
    "    pH, pD, pA = proba[:, 0], proba[:, 1], proba[:, 2]\n",
    "    pD = pD + draw_bias\n",
    "\n",
    "    preds = np.zeros(len(proba), dtype=int)\n",
    "\n",
    "    for i in range(len(proba)):\n",
    "        h, d, a = float(pH[i]), float(pD[i]), float(pA[i])\n",
    "        best = max(h, d, a)\n",
    "\n",
    "        # ① 平局几乎是最可能（或非常接近）且过硬阈值\n",
    "        if (d >= best - draw_margin) and (d >= tau):\n",
    "            preds[i] = 1\n",
    "            continue\n",
    "\n",
    "        # ② H/A 有明显优势\n",
    "        if abs(h - a) >= ha_margin:\n",
    "            preds[i] = 0 if h > a else 2\n",
    "            continue\n",
    "\n",
    "        # ③ 兜底：H/A 很接近 + d 不太低 -> 给 D\n",
    "        if (abs(h - a) <= ha_close) and (d >= tau_soft):\n",
    "            preds[i] = 1\n",
    "        else:\n",
    "            preds[i] = 0 if h > a else 2\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b3ae7569-642c-4a17-b4ff-db64f552a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the pre-trained model state\n",
      " Transformer model (from pkl): True\n",
      "Added 99 features\n",
      "corner, foul, yellow/red cards history features are added\n",
      "Added advanced H2H features (time-decayed & directional)\n",
      " Added extended draw-specific features\n",
      "The FeatureEngineering status has been updated using 159 new matches.\n",
      " FeatureEngineering has been loaded from pkl\n",
      " tf_token_features loaded from pkl: 120 features\n",
      "\n",
      "--- loading status ---\n",
      " XGB: True, number of features: 120\n",
      " Draw specialist: True, number of features: 24\n",
      " DC posterior: True, number of features: 12\n",
      " DC scaler: True\n",
      " Beta_h/Beta_a: True\n",
      " FeatureEngineering: True\n",
      " TF features: 120\n",
      " Transformer model (in memory): True\n",
      "length of epl-test: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>Brentford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Brighton</td>\n",
       "      <td>Everton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>West Ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Arsenal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Newcastle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Man United</td>\n",
       "      <td>Fulham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Nottingham Forest</td>\n",
       "      <td>Crystal Palace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>Burnley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>Man City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Wolves</td>\n",
       "      <td>Bournemouth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date           HomeTeam        AwayTeam\n",
       "0 2026-01-31        Aston Villa       Brentford\n",
       "1 2026-01-31           Brighton         Everton\n",
       "2 2026-01-31            Chelsea        West Ham\n",
       "3 2026-01-31              Leeds         Arsenal\n",
       "4 2026-01-31          Liverpool       Newcastle\n",
       "5 2026-01-31         Man United          Fulham\n",
       "6 2026-01-31  Nottingham Forest  Crystal Palace\n",
       "7 2026-01-31         Sunderland         Burnley\n",
       "8 2026-01-31          Tottenham        Man City\n",
       "9 2026-01-31             Wolves     Bournemouth"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 98 features\n",
      "corner, foul, yellow/red cards history features are added\n",
      "Added advanced H2H features (time-decayed & directional)\n",
      " Added extended draw-specific features\n",
      "Feature engineering done. Shape: (10, 161)\n",
      " XGB proba shape: (10, 3)\n",
      " Draw specialist pD_special, number of features = 24\n",
      " DC proba shape: (10, 3)\n",
      " Expected input dimension of the model: 101\n",
      " Current number of tf_token_features: 120\n",
      " Number of safe_features after filtering: 101\n",
      "Sequence feature number: 101\n",
      "Sequence feature : ['form_home', 'form_away', 'form_diff', 'form_home_v2', 'form_away_v2', 'form_diff_v2', 'home_home_form', 'away_away_form', 'win_streak_home', 'win_streak_away']...\n",
      "️ Actual FEAT_DIM constructed: 101\n",
      " home_seq shape: (10, 10, 101)\n",
      " Transformer proba shape: (10, 3)\n",
      "Meta feature shape: (10, 10)\n",
      " using Isotonic calibration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Load the pre-trained model state\")\n",
    "\n",
    "with open(\"epl_model_state.pkl\", \"rb\") as f:\n",
    "    ms = dill.load(f)\n",
    "    \n",
    "model_tf = ms.get(\"tf_model\", None)\n",
    "print(f\" Transformer model (from pkl): {model_tf is not None}\")\n",
    "fe = ms[\"fe\"]\n",
    "\n",
    "new_results = pd.read_csv(\"new_results.csv\")\n",
    "new_results[\"Date\"] = pd.to_datetime(new_results[\"Date\"], dayfirst=True)\n",
    "\n",
    "new_results[\"Season\"] = new_results[\"Date\"].apply(season_label)\n",
    "#  fe form\n",
    "new_results_fe, fe = compute_all_features(\n",
    "    new_results,\n",
    "    fe=fe,\n",
    "    is_train=True, \n",
    "    use_state_features=True,\n",
    "    use_all_adv_block=True,\n",
    "    use_shot_corners=True,\n",
    "    use_td_h2h=True,\n",
    "    use_draw_block=True,\n",
    ")\n",
    "\n",
    "#  fe\n",
    "ms[\"fe\"] = fe\n",
    "with open(\"epl_model_state.pkl\", \"wb\") as f:\n",
    "    dill.dump(ms, f)\n",
    "\n",
    "print(f\"The FeatureEngineering status has been updated using {len(new_results)} new matches.\")\n",
    "\n",
    "# Base models\n",
    "xgb_final        = ms[\"xgb_model\"]\n",
    "stage1_all       = ms[\"stage1\"]\n",
    "stage2_all       = ms[\"stage2\"]\n",
    "alpha_final      = ms[\"alpha\"]\n",
    "tau_final        = ms[\"tau\"]\n",
    "feature_cols_xgb = ms[\"feature_cols\"]\n",
    "iso_calibrators  = ms.get(\"iso_calibrators\", None)\n",
    "\n",
    "# Draw specialist\n",
    "draw_model_full   = ms.get(\"draw_model_full\", None)\n",
    "draw_feature_cols = ms.get(\"draw_feature_cols\", [])\n",
    "\n",
    "# DC Correlated\n",
    "dc_feature_cols = ms.get(\"dc_feature_cols\", [])\n",
    "dc_scaler       = ms.get(\"dc_scaler\", None)\n",
    "beta_h_s        = ms.get(\"beta_h_s\", None)\n",
    "beta_a_s        = ms.get(\"beta_a_s\", None)\n",
    "\n",
    "fe = ms.get(\"fe\", None)\n",
    "if fe is None:\n",
    "    print(\"If fe is not in pkl, fe in memory will be used (if it exists).\")\n",
    "    if \"fe\" not in globals():\n",
    "        raise ValueError(\" The feature is neither in pkl nor in memory. Please run feature engineering first!\")\n",
    "else:\n",
    "    print(\" FeatureEngineering has been loaded from pkl\")\n",
    "\n",
    "tf_token_features_from_pkl = ms.get(\"tf_token_features\", [])\n",
    "if len(tf_token_features_from_pkl) > 0:\n",
    "    tf_token_features = tf_token_features_from_pkl\n",
    "    print(f\" tf_token_features loaded from pkl: {len(tf_token_features)} features\")\n",
    "elif \"tf_token_features\" in globals():\n",
    "    print(f\" Using tf_token_features in memory: {len(tf_token_features)} features\")\n",
    "else:\n",
    "    tf_token_features = []\n",
    "    print(\" tf_token_features unavailable\")\n",
    "    \n",
    "# DC posterior\n",
    "dc_post = ms.get(\"dc_posterior\", None)\n",
    "if dc_post is not None:\n",
    "    attack_s   = dc_post[\"attack\"]\n",
    "    defense_s  = dc_post[\"defense\"]\n",
    "    home_adv_s = dc_post[\"home_adv\"]\n",
    "    rho_s      = dc_post[\"rho\"]\n",
    "    tmap       = dc_post[\"tmap\"]\n",
    "    teams      = dc_post[\"teams\"]\n",
    "\n",
    "print(f\"\\n--- loading status ---\")\n",
    "print(f\" XGB: {xgb_final is not None}, number of features: {len(feature_cols_xgb)}\")\n",
    "print(f\" Draw specialist: {draw_model_full is not None}, number of features: {len(draw_feature_cols)}\")\n",
    "print(f\" DC posterior: {dc_post is not None}, number of features: {len(dc_feature_cols)}\")\n",
    "print(f\" DC scaler: {dc_scaler is not None}\")\n",
    "print(f\" Beta_h/Beta_a: {beta_h_s is not None}\")\n",
    "print(f\" FeatureEngineering: {fe is not None}\")\n",
    "print(f\" TF features: {len(tf_token_features)}\")\n",
    "print(f\" Transformer model (in memory): {'model_tf' in globals() and model_tf is not None}\")\n",
    "\n",
    "\n",
    "fixtures = pd.read_csv(\"epl-test.csv\")\n",
    "fixtures[\"Date\"] = pd.to_datetime(fixtures[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "fixtures[\"Season\"] = fixtures[\"Date\"].apply(season_label)\n",
    "\n",
    "result_cols = [\n",
    "    \"FTR\", \"FTHG\", \"FTAG\", \"HTHG\", \"HTAG\",\n",
    "    \"HS\", \"AS\", \"HST\", \"AST\", \"HC\", \"AC\", \n",
    "    \"HF\", \"AF\", \"HY\", \"AY\", \"HR\", \"AR\", \"Referee\",\n",
    "]\n",
    "for c in result_cols:\n",
    "    if c not in fixtures.columns:\n",
    "        fixtures[c] = np.nan\n",
    "\n",
    "fixtures = fixtures.sort_values([\"Date\", \"HomeTeam\", \"AwayTeam\"]).reset_index(drop=True)\n",
    "print(f\"length of epl-test: {len(fixtures)}\")\n",
    "display(fixtures[[\"Date\", \"HomeTeam\", \"AwayTeam\"]].head(10))\n",
    "\n",
    "###################################################\n",
    "# feature engineering\n",
    "###################################################\n",
    "\n",
    "fixtures_fe, _ = compute_all_features(\n",
    "    fixtures,\n",
    "    fe=fe,\n",
    "    is_train=False,\n",
    "    use_state_features=True,\n",
    "    use_all_adv_block=True,\n",
    "    use_shot_corners=True,\n",
    "    use_td_h2h=True,\n",
    "    use_draw_block=True,\n",
    ")\n",
    "print(f\"Feature engineering done. Shape: {fixtures_fe.shape}\")\n",
    "\n",
    "###################################################\n",
    "# XGB forecast\n",
    "###################################################\n",
    "\n",
    "X_fix_xgb = pd.DataFrame(index=fixtures_fe.index)\n",
    "for col in feature_cols_xgb:\n",
    "    if col in fixtures_fe.columns:\n",
    "        X_fix_xgb[col] = fixtures_fe[col]\n",
    "    else:\n",
    "        X_fix_xgb[col] = 0.0\n",
    "\n",
    "if \"train_means\" in globals():\n",
    "    for col in feature_cols_xgb:\n",
    "        if col in train_means.index:\n",
    "            X_fix_xgb[col] = X_fix_xgb[col].fillna(train_means[col])\n",
    "        else:\n",
    "            X_fix_xgb[col] = X_fix_xgb[col].fillna(0.0)\n",
    "else:\n",
    "    X_fix_xgb = X_fix_xgb.fillna(0.0)\n",
    "\n",
    "proba_fix_xgb = xgb_final.predict_proba(X_fix_xgb.values.astype(np.float32))\n",
    "print(f\" XGB proba shape: {proba_fix_xgb.shape}\")\n",
    "\n",
    "###################################################\n",
    "# Draw specialist\n",
    "###################################################\n",
    "\n",
    "if draw_model_full is not None and len(draw_feature_cols) > 0:\n",
    "    X_draw_fix = pd.DataFrame(index=fixtures_fe.index)\n",
    "    for col in draw_feature_cols:\n",
    "        if col in fixtures_fe.columns:\n",
    "            X_draw_fix[col] = fixtures_fe[col]\n",
    "        else:\n",
    "            X_draw_fix[col] = 0.0\n",
    "    \n",
    "    X_draw_fix = X_draw_fix[draw_feature_cols].fillna(0.0)\n",
    "    pD_fix_special = draw_model_full.predict_proba(X_draw_fix.values.astype(np.float32))[:, 1]\n",
    "    print(f\" Draw specialist pD_special, number of features = {len(draw_feature_cols)}\")\n",
    "\n",
    "elif draw_model_full is not None:\n",
    "    model_draw_cols = list(getattr(draw_model_full, \"feature_names_in_\", []))\n",
    "    if len(model_draw_cols) > 0:\n",
    "        X_draw_fix = fixtures_fe[model_draw_cols].fillna(0.0)\n",
    "        pD_fix_special = draw_model_full.predict_proba(X_draw_fix.values.astype(np.float32))[:, 1]\n",
    "        print(f\" Draw specialist (from model.feature_names_in_)\")\n",
    "    else:\n",
    "        pD_fix_special = proba_fix_xgb[:, 1]\n",
    "        print(\" draw_model_full has no feature name, fallback: pD = XGB pD\")\n",
    "else:\n",
    "    pD_fix_special = proba_fix_xgb[:, 1]\n",
    "    print(\"draw_model_full not loaded, fallback: pD = XGB pD\")\n",
    "\n",
    "###################################################\n",
    "# Bayes Dixon–Coles\n",
    "###################################################\n",
    "\n",
    "if dc_post is not None and dc_scaler is not None and beta_h_s is not None and len(dc_feature_cols) > 0:\n",
    "    try:\n",
    "        X_dc_fix_raw = pd.DataFrame(index=fixtures_fe.index)\n",
    "        for col in dc_feature_cols:\n",
    "            if col in fixtures_fe.columns:\n",
    "                X_dc_fix_raw[col] = fixtures_fe[col]\n",
    "            else:\n",
    "                X_dc_fix_raw[col] = 0.0\n",
    "        \n",
    "        X_dc_fix = dc_scaler.transform(X_dc_fix_raw[dc_feature_cols].fillna(0.0))\n",
    "        \n",
    "        proba_fix_dc, _ = bayes_dc_predict_full(\n",
    "            fixtures_fe,\n",
    "            X_dc_fix,\n",
    "            attack_s, defense_s,\n",
    "            home_adv_s, rho_s,\n",
    "            beta_h_s, beta_a_s,\n",
    "            tmap,\n",
    "        )\n",
    "        print(f\" DC proba shape: {proba_fix_dc.shape}\")\n",
    "    except Exception as e:\n",
    "        print(\"DC prediction fail\")\n",
    "        proba_fix_dc = np.full_like(proba_fix_xgb, 1/3)\n",
    "else:\n",
    "    print(\"The DC-related variables are incomplete; use a uniform 1/3.\")\n",
    "    proba_fix_dc = np.full_like(proba_fix_xgb, 1/3)\n",
    "\n",
    "###################################################\n",
    "# Transformer forecast\n",
    "###################################################\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if \"model_tf\" in globals() and model_tf is not None and len(tf_token_features) > 0:\n",
    "    try:\n",
    "        expected_feat_dim = model_tf.input_proj[0].in_features\n",
    "        print(f\" Expected input dimension of the model: {expected_feat_dim}\")\n",
    "        print(f\" Current number of tf_token_features: {len(tf_token_features)}\")\n",
    "        \n",
    "        name_map = {f: str(f).lower() for f in tf_token_features}\n",
    "        safe_features = []\n",
    "        for f in tf_token_features:\n",
    "            name = name_map[f]\n",
    "            if f not in fixtures_fe.columns:\n",
    "                continue\n",
    "            if (\n",
    "                (\"pm\" in name) or\n",
    "                (\"form\" in name) or\n",
    "                (\"elo\" in name) or\n",
    "                (\"position\" in name) or\n",
    "                (\"points\" in name) or\n",
    "                (\"l10\" in name) or\n",
    "                (\"win_streak\" in name) or\n",
    "                (\"unbeaten\" in name) or\n",
    "                (\"draw\" in name) or\n",
    "                (\"xg\" in name) or\n",
    "                (\"h2h\" in name) or\n",
    "                (\"rest\" in name) or\n",
    "                (\"momentum\" in name) or\n",
    "                (\"attack\" in name) or\n",
    "                (\"defense\" in name)\n",
    "            ):\n",
    "                safe_features.append(f)\n",
    "        \n",
    "        print(f\" Number of safe_features after filtering: {len(safe_features)}\")\n",
    "        \n",
    "        if len(safe_features) != expected_feat_dim:\n",
    "            print(f\" Feature count mismatch ({len(safe_features)} vs {expected_feat_dim}), truncating the first {expected_feat_dim} features\")\n",
    "            safe_features = safe_features[:expected_feat_dim]\n",
    "        \n",
    "        fixtures_with_seq, FEAT_DIM = build_team_sequences_fixed(\n",
    "            fixtures_fe,\n",
    "            safe_features,\n",
    "            seq_len=10\n",
    "        )\n",
    "        \n",
    "        print(f\"️ Actual FEAT_DIM constructed: {FEAT_DIM}\")\n",
    "        \n",
    "        home_seq = np.stack(fixtures_with_seq[\"home_form_seq\"].values).astype(np.float32)\n",
    "        away_seq = np.stack(fixtures_with_seq[\"away_form_seq\"].values).astype(np.float32)\n",
    "        match_feat = np.stack(fixtures_with_seq[\"match_features\"].values).astype(np.float32)\n",
    "        \n",
    "        home_seq = np.nan_to_num(home_seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        away_seq = np.nan_to_num(away_seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        match_feat = np.nan_to_num(match_feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        print(f\" home_seq shape: {home_seq.shape}\")\n",
    "        \n",
    "        home_seq_t = torch.tensor(home_seq, dtype=torch.float32).to(DEVICE)\n",
    "        away_seq_t = torch.tensor(away_seq, dtype=torch.float32).to(DEVICE)\n",
    "        match_feat_t = torch.tensor(match_feat, dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        model_tf.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model_tf(home_seq_t, away_seq_t, match_feat_t)\n",
    "            proba_fix_tf = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        print(f\" Transformer proba shape: {proba_fix_tf.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Transformer fail to predict: {e}\")\n",
    "        traceback.print_exc()\n",
    "        proba_fix_tf = np.full_like(proba_fix_xgb, 1/3)\n",
    "else:\n",
    "    missing = []\n",
    "    if \"model_tf\" not in globals() or model_tf is None:\n",
    "        missing.append(\"model_tf\")\n",
    "    if len(tf_token_features) == 0:\n",
    "        missing.append(\"tf_token_features\")\n",
    "    print(f\" Transformer missing: {missing}, using even 1/3 placeholders\")\n",
    "    proba_fix_tf = np.full_like(proba_fix_xgb, 1/3)\n",
    "\n",
    "###################################################\n",
    "# build meta Feature\n",
    "###################################################\n",
    "\n",
    "X_base_fix = np.hstack([proba_fix_xgb, proba_fix_dc, proba_fix_tf])\n",
    "X_meta_fix = np.hstack([X_base_fix, pD_fix_special.reshape(-1, 1)])\n",
    "print(f\"Meta feature shape: {X_meta_fix.shape}\")\n",
    "\n",
    "###################################################\n",
    "#  Logistic\n",
    "###################################################\n",
    "\n",
    "p_draw_fix = stage1_all.predict_proba(X_meta_fix)[:, 1]\n",
    "p_away_fix = stage2_all.predict_proba(X_meta_fix)[:, 1]\n",
    "\n",
    "proba_fix_base = make_proba_two_stage_alpha(p_draw_fix, p_away_fix, alpha_final)\n",
    "\n",
    "###################################################\n",
    "# Isotonic Calibration\n",
    "###################################################\n",
    "\n",
    "if iso_calibrators is not None:\n",
    "    proba_fix_cal = proba_fix_base.copy()\n",
    "    for k, iso in enumerate(iso_calibrators):\n",
    "        if iso is not None:\n",
    "            proba_fix_cal[:, k] = iso.predict(proba_fix_base[:, k])\n",
    "    proba_fix_cal = np.clip(proba_fix_cal, 1e-7, 1)\n",
    "    proba_fix_cal /= proba_fix_cal.sum(axis=1, keepdims=True)\n",
    "    print(\" using Isotonic calibration\")\n",
    "else:\n",
    "    proba_fix_cal = proba_fix_base\n",
    "    print(\"!!! Isotonic calibration is not applied\")\n",
    "\n",
    "###################################################\n",
    "# \n",
    "###################################################\n",
    "\n",
    "y_pred_fix = predict_with_threshold_v2(proba_fix_cal, tau=tau_final, draw_bias=0.0)\n",
    "idx2label = {0: \"H\", 1: \"D\", 2: \"A\"}\n",
    "pred_label = [idx2label[int(i)] for i in y_pred_fix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3fcf4678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== FINAL PREDICTIONS (tau=0.269, alpha=0.931) ====\n",
      "Predicted distribution: H=6, D=1, A=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>pH</th>\n",
       "      <th>pD</th>\n",
       "      <th>pA</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>Brentford</td>\n",
       "      <td>0.512590</td>\n",
       "      <td>0.215082</td>\n",
       "      <td>0.272328</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Brighton</td>\n",
       "      <td>Everton</td>\n",
       "      <td>0.418683</td>\n",
       "      <td>0.214227</td>\n",
       "      <td>0.367090</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>0.544295</td>\n",
       "      <td>0.212034</td>\n",
       "      <td>0.243672</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>0.316292</td>\n",
       "      <td>0.212552</td>\n",
       "      <td>0.471157</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>0.450454</td>\n",
       "      <td>0.230483</td>\n",
       "      <td>0.319063</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Man United</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>0.471021</td>\n",
       "      <td>0.229174</td>\n",
       "      <td>0.299805</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Nottingham Forest</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>0.369329</td>\n",
       "      <td>0.215982</td>\n",
       "      <td>0.414688</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Sunderland</td>\n",
       "      <td>Burnley</td>\n",
       "      <td>0.544295</td>\n",
       "      <td>0.212034</td>\n",
       "      <td>0.243672</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>Man City</td>\n",
       "      <td>0.353012</td>\n",
       "      <td>0.221570</td>\n",
       "      <td>0.425418</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>Wolves</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>0.385593</td>\n",
       "      <td>0.210412</td>\n",
       "      <td>0.403994</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date           HomeTeam        AwayTeam        pH        pD  \\\n",
       "0   2026-01-31        Aston Villa       Brentford  0.512590  0.215082   \n",
       "1   2026-01-31           Brighton         Everton  0.418683  0.214227   \n",
       "2   2026-01-31            Chelsea        West Ham  0.544295  0.212034   \n",
       "3   2026-01-31              Leeds         Arsenal  0.316292  0.212552   \n",
       "4   2026-01-31          Liverpool       Newcastle  0.450454  0.230483   \n",
       "5   2026-01-31         Man United          Fulham  0.471021  0.229174   \n",
       "6   2026-01-31  Nottingham Forest  Crystal Palace  0.369329  0.215982   \n",
       "7   2026-01-31         Sunderland         Burnley  0.544295  0.212034   \n",
       "8   2026-01-31          Tottenham        Man City  0.353012  0.221570   \n",
       "9   2026-01-31             Wolves     Bournemouth  0.385593  0.210412   \n",
       "\n",
       "         pA Pred  \n",
       "0  0.272328    H  \n",
       "1  0.367090    H  \n",
       "2  0.243672    H  \n",
       "3  0.471157    A  \n",
       "4  0.319063    H  \n",
       "5  0.299805    H  \n",
       "6  0.414688    A  \n",
       "7  0.243672    H  \n",
       "8  0.425418    A  \n",
       "9  0.403994    D  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result has been save to epl_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ====== FIX: 用 _orig_idx 对齐概率，而不是按行号 ======\n",
    "\n",
    "# 1️⃣ 给 fixtures 一个永久 id（只基于原始顺序）\n",
    "fixtures = fixtures.copy()\n",
    "fixtures[\"_orig_idx\"] = fixtures.index\n",
    "\n",
    "# 2️⃣ 用生成 proba 时对应的 index 构建概率表\n",
    "df_proba = pd.DataFrame({\n",
    "    \"_orig_idx\": fixtures_fe.index,   # ⚠️ 关键：和 proba_fix_cal 一一对应\n",
    "    \"pH\": proba_fix_cal[:, 0],\n",
    "    \"pD\": proba_fix_cal[:, 1],\n",
    "    \"pA\": proba_fix_cal[:, 2],\n",
    "    \"Pred\": pred_label\n",
    "})\n",
    "\n",
    "# 3️⃣ merge 回 fixtures（不会错位）\n",
    "fixtures_fix = fixtures.merge(df_proba, on=\"_orig_idx\", how=\"left\")\n",
    "\n",
    "# 4️⃣ 构造最终 result（从 merge 后的 fixtures_fix 来）\n",
    "result = fixtures_fix[['Date', 'HomeTeam', 'AwayTeam', 'pH', 'pD', 'pA', 'Pred']].copy()\n",
    "\n",
    "# 日期格式（保留你原来的逻辑）\n",
    "result['Date'] = pd.to_datetime(result['Date']).dt.strftime('%Y-%m-%d')\n",
    "result['Date'] = \" \" + result['Date'].astype(str)\n",
    "\n",
    "\n",
    "print(f\"\\n==== FINAL PREDICTIONS (tau={tau_final:.3f}, alpha={alpha_final:.3f}) ====\")\n",
    "print(f\"Predicted distribution: H={sum(y_pred_fix==0)}, D={sum(y_pred_fix==1)}, A={sum(y_pred_fix==2)}\")\n",
    "\n",
    "# result\n",
    "display(result[[\"Date\", \"HomeTeam\", \"AwayTeam\", \"pH\", \"pD\", \"pA\", \"Pred\"]])\n",
    "\n",
    "#  CSV\n",
    "result.to_csv(\"epl_predictions.csv\", index=False)\n",
    "print(\"result has been save to epl_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b535443-49d1-4cee-b45a-7a7c5c4d5e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
